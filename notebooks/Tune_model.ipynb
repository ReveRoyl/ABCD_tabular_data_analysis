{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e026648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Global env + basic seeds set, seed = 52\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[Seed fixed] random/numpy/torch all set to 52\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Tune_model.ipynb\n",
    "import os\n",
    "\n",
    "seed = 52\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "print(\">>> Global env + basic seeds set, seed =\", seed)\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "from model_code import set_deterministic, _total_evr_from_recon, _total_r2_from_recon, QuestionnaireDataset, decorrelation_loss, batch_swap_noise\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "device = set_deterministic(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7564530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = Path.cwd()\n",
    "data_path = code_dir.parent / 'data'\n",
    "data_file = data_path / 'cbcl_data_remove_low_frequency.csv'\n",
    "if not data_file.exists():\n",
    "    raise FileNotFoundError(f'Could not find {data_file}')\n",
    "\n",
    "qns = pd.read_csv(data_file, encoding='utf-8')\n",
    "X = qns.iloc[:, 1:].values\n",
    "\n",
    "X_train_val, X_test = train_test_split(X, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "encoding_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe52d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEModel(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, h1, h2):\n",
    "        super(AEModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, h1),\n",
    "            # nn.LeakyReLU(0.01),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h1, h2),\n",
    "            # nn.LeakyReLU(0.01),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(h2, h3),\n",
    "            # nn.LeakyReLU(0.01),\n",
    "            nn.Linear(h2, latent_dim),\n",
    "\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, h2),\n",
    "            # nn.LeakyReLU(0.01),\n",
    "            nn.GELU(),\n",
    "            # nn.Linear(h3, h2),\n",
    "            # nn.LeakyReLU(0.01),\n",
    "            nn.Linear(h2, h1),\n",
    "            # nn.LeakyReLU(0.01),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h1, input_dim),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "class AE:\n",
    "    \"\"\"\n",
    "    简化版示例：Autoencoder 的外层封装\n",
    "    __init__(X_train, X_val, encoding_dim, h1, h2, h3)\n",
    "    train(max_epochs=..., patience=..., show_plot=...)\n",
    "    evaluate_on_data(X) -> (latent, rec_errors, evr, total_evr, recon)\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, X_val, encoding_dim, h1=0, h2=0, *,  lr, seed, lambda_decorr=None, weight_decay=None):\n",
    "        train_ds = QuestionnaireDataset(X_train)\n",
    "        val_ds = QuestionnaireDataset(X_val)\n",
    "        g_loader = torch.Generator().manual_seed(seed)\n",
    "        self.lambda_decorr = lambda_decorr\n",
    "        self.train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, generator=g_loader)\n",
    "        self.val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "        # self.g_noise = torch.Generator(device=device).manual_seed(seed + 1)\n",
    "        self.g_noise = torch.Generator().manual_seed(seed + 1)  # CPU generator 足够\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "        self.model = AEModel(input_dim, encoding_dim, h1, h2).to(device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode=\"min\", factor=0.1, patience=5\n",
    "        )\n",
    "\n",
    "    def train(self, max_epochs=1000, patience=30, show_plot=False):\n",
    "        best_val_loss = float(\"inf\")\n",
    "        epochs_no_improve = 0\n",
    "        train_losses, val_losses = [], []\n",
    "        for epoch in range(max_epochs):\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            for batch_x, _ in self.train_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                x_noisy = batch_swap_noise(batch_x, swap_prob=0.1, generator=self.g_noise)\n",
    "                # x_noisy = batch_x\n",
    "                self.optimizer.zero_grad()\n",
    "                reconstructed = self.model(x_noisy)\n",
    "                rec_loss = self.criterion(reconstructed, batch_x)\n",
    "                latent = self.model.encoder(batch_x)\n",
    "                dec_loss = self.lambda_decorr * decorrelation_loss(latent)\n",
    "                loss = rec_loss + dec_loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            train_avg = total_train_loss / len(self.train_loader.dataset)\n",
    "            train_losses.append(train_avg)\n",
    "\n",
    "            # 验证\n",
    "            self.model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, _ in self.val_loader:\n",
    "                    batch_x = batch_x.to(device)\n",
    "                    reconstructed = self.model(batch_x)\n",
    "                    rec_loss = self.criterion(reconstructed, batch_x)\n",
    "                    latent = self.model.encoder(batch_x)\n",
    "                    dec_loss = self.lambda_decorr * decorrelation_loss(latent)\n",
    "                    total_val_loss += (rec_loss + dec_loss).item() * batch_x.size(0)\n",
    "\n",
    "            val_avg = total_val_loss / len(self.val_loader.dataset)\n",
    "            val_losses.append(val_avg)\n",
    "            self.scheduler.step(val_avg)\n",
    "\n",
    "            if val_avg < best_val_loss:\n",
    "                best_val_loss = val_avg\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "        if show_plot:\n",
    "            plt.plot(train_losses, label=\"Train Loss\")\n",
    "            plt.plot(val_losses, label=\"Validation Loss\")\n",
    "            plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"AE Loss Curves\")\n",
    "            plt.legend(); plt.show()\n",
    "\n",
    "    def evaluate_on_data(self, X):\n",
    "        self.model.eval()\n",
    "        X_np = np.asarray(X)\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.tensor(X_np, dtype=torch.float32).to(device)\n",
    "            recon = self.model(X_t)\n",
    "            latent = self.model.encoder(X_t).cpu().numpy()\n",
    "            latent_vars = np.var(latent, axis=0, ddof=1)\n",
    "            total_var = np.var(X_np, axis=0, ddof=1).sum()\n",
    "            evr = latent_vars / total_var\n",
    "            recon_np = recon.cpu().numpy()\n",
    "            rec_errors = np.mean((X_np - recon_np)**2, axis=1)\n",
    "            total_evr = _total_evr_from_recon(X_np, recon_np, ddof=1)\n",
    "            total_r2 = _total_r2_from_recon(X_np, recon_np)\n",
    "        return latent, rec_errors, evr, total_evr, recon_np, total_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b0186d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed fixed] random/numpy/torch all set to 52\n",
      "Using device: cuda\n",
      "Early stopping at epoch 95\n",
      "Evaluating on test set: EVR test = 0.3698\n",
      "AE -> MSE=0.608519, Total EVR=0.3917\n"
     ]
    }
   ],
   "source": [
    "set_deterministic(seed)\n",
    "\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.2, random_state=seed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "autoencoder = AE(\n",
    "    X_train,\n",
    "    X_val,\n",
    "    encoding_dim=encoding_dim,\n",
    "    h1=192,\n",
    "    h2=64,\n",
    "    seed=seed,\n",
    "    lr=0.003,\n",
    "    lambda_decorr=1,\n",
    "    weight_decay=8e-05,\n",
    ")\n",
    "\n",
    "autoencoder.train(show_plot=False)\n",
    "\n",
    "latent_factors, _, _, evr_test, reconstructed, _ = autoencoder.evaluate_on_data(X_test)\n",
    "print(f\"Evaluating on test set: EVR test = {evr_test:.4f}\")\n",
    "\n",
    "latent_factors, _, _, evr_train, reconstructed, _ = autoencoder.evaluate_on_data(X_train)\n",
    "mse_ae = float(np.mean((X_train - reconstructed) ** 2))\n",
    "\n",
    "print(f'AE -> MSE={mse_ae:.6f}, Total EVR={evr_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9ed80da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set: EVR test = 0.369844\n",
      "Mean reconstruction error on test set: MSE test = 12.645459\n",
      "Total R2 on test set: R2 test = 0.358717\n",
      "Evaluating on total set: EVR total = 0.380717\n",
      "Mean reconstruction error on total set: MSE total = 12.560674\n",
      "Total R2 on total set: R2 total = 0.370054\n",
      "Evaluating on train set: EVR train = 0.391696\n",
      "Mean reconstruction error on train set: MSE train = 0.608519\n",
      "Total R2 on train set: R2 train = 0.391481\n"
     ]
    }
   ],
   "source": [
    "latent_factors, error_test, _, evr_test, reconstructed, r2_test = autoencoder.evaluate_on_data(X_test)\n",
    "print(f\"Evaluating on test set: EVR test = {evr_test:.6f}\")\n",
    "print(f\"Mean reconstruction error on test set: MSE test = {np.mean(error_test):.6f}\")\n",
    "print(f\"Total R2 on test set: R2 test = {r2_test:.6f}\")\n",
    "\n",
    "latent_factors, error_val, _, evr_val, reconstructed, r2_val = autoencoder.evaluate_on_data(scaler.transform(X_val))\n",
    "print(f\"Evaluating on total set: EVR total = {evr_val:.6f}\")\n",
    "print(\"Mean reconstruction error on total set:\" f\" MSE total = {np.mean(error_val):.6f}\")\n",
    "print(f\"Total R2 on total set: R2 total = {r2_val:.6f}\")\n",
    "\n",
    "latent_factors, error_train, _, evr_train, reconstructed, r2_train = autoencoder.evaluate_on_data(X_train)\n",
    "print(f\"Evaluating on train set: EVR train = {evr_train:.6f}\")\n",
    "print(f\"Mean reconstruction error on train set: MSE train = {np.mean(error_train):.6f}\")\n",
    "print(f\"Total R2 on train set: R2 train = {r2_train:.6f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e4ff245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed fixed] random/numpy/torch all set to 52\n",
      "Using device: cuda\n",
      "Early stopping at epoch 82\n",
      "Evaluating on test set: EVR test = 0.5343\n",
      "AE -> MSE=0.581805, Total EVR=0.4184\n"
     ]
    }
   ],
   "source": [
    "set_deterministic(seed)\n",
    "\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.2, random_state=seed)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "autoencoder = AE(\n",
    "    X_train,\n",
    "    X_val,\n",
    "    encoding_dim=encoding_dim,\n",
    "    h1=192,\n",
    "    h2=64,\n",
    "    seed=seed,\n",
    "    lr=0.003,\n",
    "    lambda_decorr=0.005,\n",
    "    weight_decay=8e-05,\n",
    ")\n",
    "\n",
    "autoencoder.train(show_plot=False)\n",
    "\n",
    "latent_factors, _, _, evr_test, reconstructed, _ = autoencoder.evaluate_on_data(X_test)\n",
    "print(f\"Evaluating on test set: EVR test = {evr_test:.4f}\")\n",
    "\n",
    "latent_factors, _, _, evr_train, reconstructed, _ = autoencoder.evaluate_on_data(X_train)\n",
    "mse_ae = float(np.mean((X_train - reconstructed) ** 2))\n",
    "\n",
    "print(f'AE -> MSE={mse_ae:.6f}, Total EVR={evr_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "012ecd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set: EVR test = 0.534276\n",
      "Mean reconstruction error on test set: MSE test = 825.946489\n",
      "Total R2 on test set: R2 test = 0.523288\n",
      "Evaluating on total set: EVR total = 0.489109\n",
      "Mean reconstruction error on total set: MSE total = 10.468645\n",
      "Total R2 on total set: R2 total = 0.474974\n",
      "Evaluating on train set: EVR train = 0.418368\n",
      "Mean reconstruction error on train set: MSE train = 0.581805\n",
      "Total R2 on train set: R2 train = 0.418195\n"
     ]
    }
   ],
   "source": [
    "latent_factors, error_test, _, evr_test, reconstructed, r2_test = autoencoder.evaluate_on_data(X_test)\n",
    "print(f\"Evaluating on test set: EVR test = {evr_test:.6f}\")\n",
    "print(f\"Mean reconstruction error on test set: MSE test = {np.mean(error_test):.6f}\")\n",
    "print(f\"Total R2 on test set: R2 test = {r2_test:.6f}\")\n",
    "\n",
    "latent_factors, error_val, _, evr_val, reconstructed, r2_val = autoencoder.evaluate_on_data(scaler.transform(X_val))\n",
    "print(f\"Evaluating on total set: EVR total = {evr_val:.6f}\")\n",
    "print(\"Mean reconstruction error on total set:\" f\" MSE total = {np.mean(error_val):.6f}\")\n",
    "print(f\"Total R2 on total set: R2 total = {r2_val:.6f}\")\n",
    "\n",
    "latent_factors, error_train, _, evr_train, reconstructed, r2_train = autoencoder.evaluate_on_data(X_train)\n",
    "print(f\"Evaluating on train set: EVR train = {evr_train:.6f}\")\n",
    "print(f\"Mean reconstruction error on train set: MSE train = {np.mean(error_train):.6f}\")\n",
    "print(f\"Total R2 on train set: R2 train = {r2_train:.6f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ======== 超参范围（全局） ========\n",
    "H1_MIN, H1_MAX, H1_STEP = 64, 256, 32\n",
    "H2_MIN, H2_MAX, H2_STEP = 32, 128, 16\n",
    "LR_MIN, LR_MAX = 1e-4, 5e-3\n",
    "LDEC_MIN, LDEC_MAX = 0.0, 1.0\n",
    "WD_MIN, WD_MAX = 1e-6, 1e-3\n",
    "\n",
    "encoding_dim = 5\n",
    "outer_kf = KFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "\n",
    "# Optuna samplers\n",
    "tpe_sampler = optuna.samplers.TPESampler(seed=seed)\n",
    "cma_sampler = optuna.samplers.CmaEsSampler(\n",
    "    seed=seed,\n",
    "    sigma0=0.5,\n",
    "    warn_independent_sampling=False,\n",
    ")\n",
    "\n",
    "all_results = []\n",
    "fold_metadata = []\n",
    "outer_fold_id = 1\n",
    "\n",
    "# 可选：结果保存目录\n",
    "nested_root = code_dir / \"output\" / \"nested_cv_ae_optuna\"\n",
    "nested_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _clip_int_range(center, lo, hi, step):\n",
    "    \"\"\"小工具：给定中心值，在 [lo, hi] 范围里取一个较窄区间\"\"\"\n",
    "    low = max(lo, center - step)\n",
    "    high = min(hi, center + step)\n",
    "    # 保险起见，保证 low < high\n",
    "    if low >= high:\n",
    "        low = lo\n",
    "        high = hi\n",
    "    return low, high\n",
    "\n",
    "for train_val_idx, test_idx in outer_kf.split(X):\n",
    "\n",
    "    print(f\"\\n================ Outer Fold {outer_fold_id} ================\")\n",
    "\n",
    "    # ---------- 外层 Train/Val/Test 划分 ----------\n",
    "    X_train_val_raw = X[train_val_idx]\n",
    "    X_test_raw = X[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_val = scaler.fit_transform(X_train_val_raw)\n",
    "    X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "    # ---------- inner objective（共用） ----------\n",
    "    def make_objective(X_outer_train_val, search_space_mode=\"wide\", base_params=None):\n",
    "\n",
    "        def objective(trial):\n",
    "            # ========== 1) 定义搜索空间 ==========\n",
    "            if search_space_mode == \"wide\":\n",
    "                # Stage 1：TPE 使用的“大范围”\n",
    "                h1 = trial.suggest_int(\"h1\", H1_MIN, H1_MAX, step=H1_STEP)\n",
    "                h2 = trial.suggest_int(\"h2\", H2_MIN, H2_MAX, step=H2_STEP)\n",
    "                lr = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
    "                lambda_decorr = trial.suggest_float(\"lambda_decorr\", LDEC_MIN, LDEC_MAX)\n",
    "                weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
    "\n",
    "            elif search_space_mode == \"narrow\":\n",
    "                # Stage 2：CMA-ES 使用的“缩小范围”\n",
    "                assert base_params is not None, \"base_params 不能为空（narrow 模式）\"\n",
    "\n",
    "                h1_center = int(base_params[\"h1\"])\n",
    "                h2_center = int(base_params[\"h2\"])\n",
    "                lr_center = float(base_params[\"lr\"])\n",
    "                ldec_center = float(base_params[\"lambda_decorr\"])\n",
    "                wd_center = float(base_params[\"weight_decay\"])\n",
    "\n",
    "                # around h1, h2（±一个 step）\n",
    "                h1_low, h1_high = _clip_int_range(h1_center, H1_MIN, H1_MAX, H1_STEP)\n",
    "                h2_low, h2_high = _clip_int_range(h2_center, H2_MIN, H2_MAX, H2_STEP)\n",
    "\n",
    "                h1 = trial.suggest_int(\"h1\", h1_low, h1_high, step=H1_STEP)\n",
    "                h2 = trial.suggest_int(\"h2\", h2_low, h2_high, step=H2_STEP)\n",
    "\n",
    "                # lr：以 center 为中点、对数尺度缩窄一圈\n",
    "                lr_low = max(LR_MIN, lr_center / 3.0)\n",
    "                lr_high = min(LR_MAX, lr_center * 3.0)\n",
    "                lr = trial.suggest_float(\"lr\", lr_low, lr_high, log=True)\n",
    "\n",
    "                # lambda_decorr：在 [center-0.3, center+0.3] 之间截断\n",
    "                ldec_low = max(LDEC_MIN, ldec_center - 0.3)\n",
    "                ldec_high = min(LDEC_MAX, ldec_center + 0.3)\n",
    "                if ldec_low >= ldec_high:\n",
    "                    ldec_low, ldec_high = LDEC_MIN, LDEC_MAX\n",
    "                lambda_decorr = trial.suggest_float(\"lambda_decorr\", ldec_low, ldec_high)\n",
    "\n",
    "                # weight_decay：同样 log 缩窄\n",
    "                wd_low = max(WD_MIN, wd_center / 5.0)\n",
    "                wd_high = min(WD_MAX, wd_center * 5.0)\n",
    "                weight_decay = trial.suggest_float(\"weight_decay\", wd_low, wd_high, log=True)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown search_space_mode: {search_space_mode}\")\n",
    "\n",
    "            # ========== 2) inner KFold ==========\n",
    "            inner_kf = KFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "            ev_totals = []\n",
    "\n",
    "            inner_fold_id = 1\n",
    "            for inner_train_idx, inner_val_idx in inner_kf.split(X_outer_train_val):\n",
    "                print(f\"-------- Inner Fold {inner_fold_id} ({search_space_mode}) --------\")\n",
    "\n",
    "                X_inner_train = X_outer_train_val[inner_train_idx]\n",
    "                X_inner_val = X_outer_train_val[inner_val_idx]\n",
    "\n",
    "                set_deterministic(seed + inner_fold_id)\n",
    "\n",
    "                ae = AE(\n",
    "                    X_inner_train,\n",
    "                    X_inner_val,\n",
    "                    encoding_dim=encoding_dim,\n",
    "                    h1=h1,\n",
    "                    h2=h2,\n",
    "                    lr=lr,\n",
    "                    seed=seed + inner_fold_id,\n",
    "                    lambda_decorr=lambda_decorr,\n",
    "                    weight_decay=weight_decay,\n",
    "                )\n",
    "\n",
    "                print(f\"[Inner Fold {inner_fold_id}] Training AE...\")\n",
    "                ae.train(show_plot=False)\n",
    "\n",
    "                _, _, _, ev_total_val, _, _ = ae.evaluate_on_data(X_inner_val)\n",
    "                ev_totals.append(ev_total_val)\n",
    "                print(f\"[Inner Fold {inner_fold_id}] EV_total = {ev_total_val:.4f}\")\n",
    "\n",
    "                inner_fold_id += 1\n",
    "\n",
    "            mean_ev = float(np.mean(ev_totals))\n",
    "            print(f\"[Inner Summary | mode={search_space_mode}] Mean EV_total = {mean_ev:.4f}\")\n",
    "            return mean_ev\n",
    "\n",
    "        return objective\n",
    "\n",
    "    # ---------- Stage 1：TPE 粗搜索 ----------\n",
    "    objective_wide = make_objective(X_train_val, search_space_mode=\"wide\")\n",
    "\n",
    "    study_tpe = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=tpe_sampler,\n",
    "        study_name=f\"ae_outer{outer_fold_id}_tpe\",\n",
    "    )\n",
    "    study_tpe.optimize(objective_wide, n_trials=20)\n",
    "\n",
    "    best_params_tpe = study_tpe.best_params\n",
    "    best_value_tpe = study_tpe.best_value\n",
    "    print(f\"[Outer Fold {outer_fold_id}] TPE Best Params: {best_params_tpe}\")\n",
    "    print(f\"[Outer Fold {outer_fold_id}] TPE Best EV_total = {best_value_tpe:.4f}\")\n",
    "\n",
    "    # ---------- Stage 2：CMA-ES 精细搜索 ----------\n",
    "    objective_narrow = make_objective(\n",
    "        X_train_val,\n",
    "        search_space_mode=\"narrow\",\n",
    "        base_params=best_params_tpe,\n",
    "    )\n",
    "\n",
    "    study_cma = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=cma_sampler,\n",
    "        study_name=f\"ae_outer{outer_fold_id}_cma\",\n",
    "    )\n",
    "    study_cma.optimize(objective_narrow, n_trials=30)\n",
    "\n",
    "    best_params_cma = study_cma.best_params\n",
    "    best_value_cma = study_cma.best_value\n",
    "    print(f\"[Outer Fold {outer_fold_id}] CMA-ES Best Params: {best_params_cma}\")\n",
    "    print(f\"[Outer Fold {outer_fold_id}] CMA-ES Best EV_total = {best_value_cma:.4f}\")\n",
    "\n",
    "    # 你也可以选 TPE 和 CMA-ES 里更大的那个，这里先用 CMA-ES 的结果：\n",
    "    final_best_params = best_params_cma\n",
    "    final_best_params[\"stage1_best_ev\"] = best_value_tpe\n",
    "    final_best_params[\"stage2_best_ev\"] = best_value_cma\n",
    "\n",
    "    best_h1 = final_best_params[\"h1\"]\n",
    "    best_h2 = final_best_params[\"h2\"]\n",
    "    best_lr = final_best_params[\"lr\"]\n",
    "    best_lambda_decorr = final_best_params[\"lambda_decorr\"]\n",
    "    best_weight_decay = final_best_params[\"weight_decay\"]\n",
    "\n",
    "    # ---------- outer fold 上用最佳超参重新训练 ----------\n",
    "    X_train, X_val = train_test_split(\n",
    "        X_train_val,\n",
    "        test_size=0.16,   # 0.64 / 0.16 / 0.20 结构\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    autoencoder = AE(\n",
    "        X_train,\n",
    "        X_val,\n",
    "        encoding_dim=encoding_dim,\n",
    "        h1=best_h1,\n",
    "        h2=best_h2,\n",
    "        lr=best_lr,\n",
    "        seed=seed,\n",
    "        lambda_decorr=best_lambda_decorr,\n",
    "        weight_decay=best_weight_decay,\n",
    "    )\n",
    "\n",
    "    print(f\"[Outer Fold {outer_fold_id}] Training Final AE (with best params)...\")\n",
    "    autoencoder.train(show_plot=False)\n",
    "\n",
    "    # ---------- 测试集评估 ----------\n",
    "    latent_factors, rec_errors, evr, ev_total, reconstructed, r2 = \\\n",
    "        autoencoder.evaluate_on_data(X_test)\n",
    "\n",
    "    rec_mean = rec_errors.mean()\n",
    "    print(\n",
    "        f\"[Outer Fold {outer_fold_id}] Final Test: \"\n",
    "        f\"EV_total={ev_total:.4f}, RecErr={rec_mean:.4f}, R²={r2:.4f}\"\n",
    "    )\n",
    "\n",
    "    all_results.append({\n",
    "        \"fold\": outer_fold_id,\n",
    "        \"latent\": latent_factors,\n",
    "        \"ev_total\": ev_total,\n",
    "        \"reconstructed\": reconstructed,\n",
    "        \"r2\": r2,\n",
    "        \"rec_error\": rec_errors,\n",
    "    })\n",
    "\n",
    "    fold_metadata.append({\n",
    "        \"fold\": outer_fold_id,\n",
    "        \"ev_total\": float(ev_total),\n",
    "        \"rec_error_mean\": float(rec_mean),\n",
    "        \"r2\": float(r2),\n",
    "        \"best_h1\": int(best_h1),\n",
    "        \"best_h2\": int(best_h2),\n",
    "        \"best_lr\": float(best_lr),\n",
    "        \"best_lambda_decorr\": float(best_lambda_decorr),\n",
    "        \"best_weight_decay\": float(best_weight_decay),\n",
    "        \"tpe_best_ev\": float(best_value_tpe),\n",
    "        \"cma_best_ev\": float(best_value_cma),\n",
    "    })\n",
    "\n",
    "    # 顺手把每个 outer fold 对应的 study 的 trials 存一下（可选）\n",
    "    study_tpe.trials_dataframe().to_csv(\n",
    "        nested_root / f\"outer{outer_fold_id}_tpe_trials.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    study_cma.trials_dataframe().to_csv(\n",
    "        nested_root / f\"outer{outer_fold_id}_cma_trials.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    outer_fold_id += 1\n",
    "\n",
    "# ---------- 最终汇总 ----------\n",
    "ev_list = [r[\"ev_total\"] for r in all_results]\n",
    "r2_list = [r[\"r2\"] for r in all_results]\n",
    "rec_list = [r[\"rec_error\"].mean() for r in all_results]\n",
    "\n",
    "print(\"\\n================ Nested CV Summary ================\")\n",
    "print(f\"EV_total mean={np.mean(ev_list):.4f}, std={np.std(ev_list):.4f}\")\n",
    "print(f\"RecErr mean={np.mean(rec_list):.4f}, std={np.std(rec_list):.4f}\")\n",
    "print(f\"R² mean={np.mean(r2_list):.4f}, std={np.std(r2_list):.4f}\")\n",
    "\n",
    "# ---------- 保存一个总结果（你问“该去哪儿”） ----------\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "summary = {\n",
    "    \"seed\": int(seed),\n",
    "    \"encoding_dim\": int(encoding_dim),\n",
    "    \"outer_folds\": fold_metadata,\n",
    "    \"summary_metrics\": {\n",
    "        \"ev_total_mean\": float(np.mean(ev_list)),\n",
    "        \"ev_total_std\": float(np.std(ev_list)),\n",
    "        \"rec_err_mean\": float(np.mean(rec_list)),\n",
    "        \"rec_err_std\": float(np.std(rec_list)),\n",
    "        \"r2_mean\": float(np.mean(r2_list)),\n",
    "        \"r2_std\": float(np.std(r2_list)),\n",
    "    },\n",
    "}\n",
    "\n",
    "# JSON：详细信息\n",
    "with open(nested_root / \"nested_cv_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# CSV：每个 outer fold 一行，便于以后快速看表\n",
    "pd.DataFrame(fold_metadata).to_csv(\n",
    "    nested_root / \"nested_cv_folds.csv\", index=False\n",
    ")\n",
    "\n",
    "print(f\"Nested CV summary saved to: {nested_root}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa2dae",
   "metadata": {},
   "source": [
    "Selecting from 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294496bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('g:/ABCD/script/trail/notebooks/output/nested_cv_ae_optuna')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_root = code_dir / \"output\" / \"nested_cv_ae_optuna\"\n",
    "nested_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3eff6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_1\n",
      "fold_2\n",
      "fold_3\n",
      "fold_4\n",
      "fold_5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>ev_total</th>\n",
       "      <th>rec_error_mean</th>\n",
       "      <th>r2</th>\n",
       "      <th>best_h1</th>\n",
       "      <th>best_h2</th>\n",
       "      <th>best_lr</th>\n",
       "      <th>best_lambda_decorr</th>\n",
       "      <th>best_weight_decay</th>\n",
       "      <th>tpe_best_ev</th>\n",
       "      <th>cma_best_ev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.382993</td>\n",
       "      <td>0.640465</td>\n",
       "      <td>0.382556</td>\n",
       "      <td>224</td>\n",
       "      <td>96</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.092998</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.353428</td>\n",
       "      <td>0.355562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.362788</td>\n",
       "      <td>0.624998</td>\n",
       "      <td>0.362262</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.358757</td>\n",
       "      <td>0.355001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.339258</td>\n",
       "      <td>0.654328</td>\n",
       "      <td>0.338698</td>\n",
       "      <td>192</td>\n",
       "      <td>80</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>0.929482</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.350982</td>\n",
       "      <td>0.352103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.359715</td>\n",
       "      <td>0.692212</td>\n",
       "      <td>0.359167</td>\n",
       "      <td>192</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.356611</td>\n",
       "      <td>0.361090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.360503</td>\n",
       "      <td>0.598198</td>\n",
       "      <td>0.359983</td>\n",
       "      <td>96</td>\n",
       "      <td>48</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.353440</td>\n",
       "      <td>0.357725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold  ev_total  rec_error_mean        r2  best_h1  best_h2   best_lr  \\\n",
       "0     1  0.382993        0.640465  0.382556      224       96  0.002195   \n",
       "0     2  0.362788        0.624998  0.362262      128       64  0.003469   \n",
       "0     3  0.339258        0.654328  0.338698      192       80  0.003245   \n",
       "0     4  0.359715        0.692212  0.359167      192       64  0.002361   \n",
       "0     5  0.360503        0.598198  0.359983       96       48  0.003768   \n",
       "\n",
       "   best_lambda_decorr  best_weight_decay  tpe_best_ev  cma_best_ev  \n",
       "0            0.092998           0.000124     0.353428     0.355562  \n",
       "0            0.006983           0.000086     0.358757     0.355001  \n",
       "0            0.929482           0.000035     0.350982     0.352103  \n",
       "0            0.003367           0.000150     0.356611     0.361090  \n",
       "0            0.004493           0.000088     0.353440     0.357725  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_result = pd.DataFrame()\n",
    "for n in range(1, 6):\n",
    "    fold_ = pd.read_csv(nested_root / f\"fold_{n}\" / \"nested_cv_folds.csv\")\n",
    "    fold_result = pd.concat([fold_result, fold_], axis=0)\n",
    "    print(f\"fold_{n}\")\n",
    "fold_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
