{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from models import Autoencoder\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from utils import translate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = Path(os.getcwd())\n",
    "data_path = code_dir.parent / \"data\"\n",
    "assert os.path.exists(\n",
    "    data_path\n",
    "), \"Data directory not found. Make sure you're running this code from the root directory of the project.\"\n",
    "\n",
    "with open(data_path / \"cbcl_data_remove_unrelated.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qns = pd.read_csv(f)\n",
    "\n",
    "X = qns.iloc[:, 2:].values\n",
    "\n",
    "# Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train_raw, X_temp = train_test_split(X, test_size=0.4)\n",
    "X_val_raw, X_test_raw = train_test_split(X_temp, test_size=0.5)\n",
    "\n",
    "\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_val = scaler.transform(X_val_raw)\n",
    "X_test = scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(X_train, X_val, encoding_dim=5, \n",
    "                          layer1_neurons=69, layer2_neurons=58, layer3_neurons=53)\n",
    "autoencoder.train()\n",
    "(\n",
    "    latent_factors,\n",
    "    reconstruction_errors_test,\n",
    "    explained_variance_ratios,\n",
    "    explained_variance_ratio_test,\n",
    ") = autoencoder.evaluate_on_data(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "autoencoder.export_to_onnx(X_train, onnx_path = \"../output/autoencoder_real_input.onnx\")  # export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_factors = latent_factors  # convert to NumPy\n",
    "X_scaled = np.array(X_scaled)  # convert to NumPy\n",
    "\n",
    "# calculate loadings\n",
    "loadings = []\n",
    "for i in range(X_scaled.shape[1]):  # transversal all features\n",
    "    reg = LinearRegression().fit(latent_factors, X_scaled[:, i])  # regression\n",
    "    loadings.append(reg.coef_)  # store the coefficients\n",
    "\n",
    "loading_matrix = np.array(\n",
    "    loadings\n",
    ").T  #  transpose to (latent factors, original features)\n",
    "\n",
    "print(\"Loading matrix shape:\", loading_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the possible dimensions of the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataFrame to store the results\n",
    "dim_df = pd.DataFrame(\n",
    "    columns=[\"dim\"] + [f\"Factor_{i+1}\" for i in range(10)] + [\"Total variance ratio\"]\n",
    ")\n",
    "\n",
    "for latent_dim in range(4, 6):\n",
    "    print(\"dim:\", latent_dim)\n",
    "\n",
    "    # Initialize and train the autoencoder\n",
    "    autoencoder = Autoencoder(X_train, X_val, encoding_dim=latent_dim)\n",
    "    autoencoder.train()\n",
    "\n",
    "    # Evaluate the autoencoder on the scaled data\n",
    "    (\n",
    "        latent_factors,\n",
    "        reconstruction_errors,\n",
    "        explained_variance_ratios,\n",
    "        explained_variance_ratio_total,\n",
    "    ) = autoencoder.evaluate_on_data(X_scaled)\n",
    "\n",
    "    # Create a temporary DataFrame to store the results for the current latent dimension\n",
    "    temp_df = pd.DataFrame(\n",
    "        {\n",
    "            \"dim\": [latent_dim],  # Current dimension\n",
    "            **{\n",
    "                f\"Factor_{i+1}\": ratio\n",
    "                for i, ratio in enumerate(explained_variance_ratios)\n",
    "            },\n",
    "            \"Total variance ratio\": [explained_variance_ratio_total],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Fill missing factor columns with NaN to maintain consistent column names\n",
    "    for col in dim_df.columns:\n",
    "        if col not in temp_df.columns:\n",
    "            temp_df[col] = None  # Fill with NaN\n",
    "\n",
    "    # Sort columns to match the order in dim_df\n",
    "    temp_df = temp_df[dim_df.columns]\n",
    "\n",
    "    # Concatenate the temporary DataFrame with the main DataFrame\n",
    "    dim_df = pd.concat([dim_df, temp_df], ignore_index=True)\n",
    "\n",
    "print(dim_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = qns.iloc[:, 2:]  # the cbcl data\n",
    "\n",
    "rank = 5  # number of components to extract\n",
    "\n",
    "# NMF dicomposition\n",
    "nmf_model = NMF(\n",
    "    n_components=rank, init=\"random\", solver=\"mu\", max_iter=1000, random_state=42\n",
    ")\n",
    "W = nmf_model.fit_transform(data_cleaned)  # 基矩阵 W basis matrix\n",
    "H = nmf_model.components_  # 系数矩阵 H coefficients matrix\n",
    "\n",
    "# reconstruction error\n",
    "reconstruction_error = nmf_model.reconstruction_err_\n",
    "print(f\"Reconstruction error: {reconstruction_error}\")\n",
    "\n",
    "# reconstruct the matrix\n",
    "X_reconstructed = np.dot(W, H)\n",
    "\n",
    "# calculate Frobenius norm\n",
    "X_norm = np.linalg.norm(data_cleaned, ord=\"fro\")\n",
    "reconstruction_error_frobenius = np.linalg.norm(\n",
    "    data_cleaned - X_reconstructed, ord=\"fro\"\n",
    ")\n",
    "\n",
    "# calculate relative error\n",
    "relative_error = reconstruction_error_frobenius / X_norm\n",
    "print(f\"Relative Error: {relative_error}\")\n",
    "\n",
    "# calculate explained variance\n",
    "data_cleaned_np = data_cleaned.to_numpy()\n",
    "\n",
    "# calculate the total variance of the original data matrix\n",
    "total_variance = np.sum((data_cleaned_np - np.mean(data_cleaned_np)) ** 2)\n",
    "\n",
    "# calculate the variance of the reconstructed matrix\n",
    "reconstructed_variance = np.sum((X_reconstructed - np.mean(X_reconstructed)) ** 2)\n",
    "\n",
    "# variance_explained\n",
    "variance_explained = reconstructed_variance / total_variance\n",
    "print(f\"Variance explained by the NMF model: {variance_explained * 100:.2f}%\")\n",
    "\n",
    "# add ID to W\n",
    "W_with_id = pd.concat([pd.Series(qns.iloc[:, 1], name=\"ID\"), pd.DataFrame(W)], axis=1)\n",
    "\n",
    "# 转置 H\n",
    "H_transposed = H.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results from autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 latent_df 转换为 numpy 数组（如果它是 Pandas DataFrame）\n",
    "latent_factors = (\n",
    "    latent_factors.values\n",
    "    if isinstance(latent_factors, pd.DataFrame)\n",
    "    else latent_factors\n",
    ")\n",
    "original_features = X if isinstance(X, np.ndarray) else X.values\n",
    "\n",
    "# 存储每个原始特征的回归系数\n",
    "n_original_features = original_features.shape[1]\n",
    "n_latent_factors = latent_factors.shape[1]\n",
    "\n",
    "loadings = []\n",
    "\n",
    "# 对每个原始特征进行回归，使用 latent_factors 作为输入特征\n",
    "for i in range(n_original_features):\n",
    "    y = original_features[:, i]  # 当前原始特征\n",
    "    reg = LinearRegression().fit(latent_factors, y)\n",
    "    loadings.append(reg.coef_)\n",
    "\n",
    "# 将结果转换为 DataFrame，便于查看\n",
    "loadings_df = pd.DataFrame(\n",
    "    loadings, columns=[f\"Latent_{j+1}\" for j in range(n_latent_factors)]\n",
    ")\n",
    "loadings_df.index = [f\"Feature_{i+1}\" for i in range(n_original_features)]\n",
    "\n",
    "# 输出每个潜在因子对原始特征的贡献（类似于 PCA 的负载）the loading matrix of the autoencoder\n",
    "print(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_df.index = qns.iloc[:, 2:].columns\n",
    "df = pd.DataFrame()\n",
    "for i in range(latent_dim):\n",
    "    df0 = (\n",
    "        loadings_df[\"Latent_{0}\".format(i + 1)]\n",
    "        .reindex(\n",
    "            loadings_df[\"Latent_{0}\".format(i + 1)]\n",
    "            .abs()\n",
    "            .sort_values(ascending=False)\n",
    "            .index\n",
    "        )\n",
    "        .to_frame(name=\"Latent_{0}\".format(i + 1))\n",
    "    )\n",
    "    df0 = df0.reset_index().rename(columns={\"index\": \"Row_Name\"})\n",
    "    df = pd.concat([df, df0], axis=1)\n",
    "df_even_columns = df.iloc[\n",
    "    :, ::2\n",
    "]  # # Select all rows and every second column from the DataFrame\n",
    "\n",
    "details_autoencoder = translate_text(df_even_columns, 5, \"en\")\n",
    "details_autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results from NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_csv(\n",
    "    r\"G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\NMF_H.csv\"\n",
    ").iloc[:, 1:]\n",
    "label.index = qns.iloc[:, 2:].columns\n",
    "df_NMF = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    df0 = (\n",
    "        label[\"V{0}\".format(i + 1)]\n",
    "        .reindex(label[\"V{0}\".format(i + 1)].abs().sort_values(ascending=False).index)\n",
    "        .to_frame(name=\"V{0}\".format(i + 1))\n",
    "    )\n",
    "    df0 = df0.reset_index().rename(columns={\"index\": \"Row_Name{0}\".format(i + 1)})\n",
    "    df_NMF = pd.concat([df_NMF, df0], axis=1)\n",
    "df_NMF_even_columns = df_NMF.iloc[:, ::2]  # 选择所有行和每隔两列的列\n",
    "\n",
    "details = translate_text(df_NMF_even_columns, 5, \"en\")\n",
    "details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer(n_factors=5, rotation=\"varimax\")\n",
    "fa.fit(X_scaled)\n",
    "# Check Eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "ev\n",
    "factor_loadings = fa.loadings_\n",
    "# 计算每个因子的贡献方差和方差解释率\n",
    "variance_explained = fa.get_factor_variance()\n",
    "print(f\"varience explained: {variance_explained[2][-1]:.2%}\")\n",
    "\n",
    "# 计算所有个体的因子得分\n",
    "factor_scores = fa.transform(X_scaled)\n",
    "factor_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the reconstuction error of the autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the datasets\n",
    "datasets = {\"Train\": X_train, \"Validation\": X_val, \"Test\": X_test}\n",
    "\n",
    "# Plot the reconstruction errors\n",
    "autoencoder.plot_reconstruction_errors(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression model from questionnaire items to factors generated with NMF (from Toby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LassoAnalysis\n",
    "\n",
    "qns = X\n",
    "scores = pd.read_csv(\n",
    "    r\"G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\NMF_W.csv\"\n",
    ").iloc[:, 2:]\n",
    "\n",
    "# Initialize the LassoAnalysis class\n",
    "lasso_analysis = LassoAnalysis(qns, scores)\n",
    "\n",
    "# Perform the analysis\n",
    "lasso_analysis.perform_analysis()\n",
    "\n",
    "# Plot R² values\n",
    "lasso_analysis.plot_r2_values()\n",
    "\n",
    "# Plot factor predictions\n",
    "lasso_analysis.plot_factor_predictions(alpha=0.125)\n",
    "\n",
    "# Plot heatmap of coefficients\n",
    "lasso_analysis.plot_heatmap(alpha=0.125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be continued: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From neuroimaging data to latent factors (to be continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and drop the first column and the subject id\n",
    "data = pd.read_csv(r\"G:/ABCD/data/mri_y_rsfmr_cor_gp_gp.csv\")\n",
    "# only keep eventname = baseline_year_1_arm_1\n",
    "data = (\n",
    "    data[data[\"eventname\"] == \"baseline_year_1_arm_1\"]\n",
    "    .drop(columns=[\"eventname\"])\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "labels = pd.read_csv(\n",
    "    r\"G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\EFA.csv\"\n",
    ").iloc[:, 1:]\n",
    "# labels = pd.read_csv(r'G:/ABCD/data/mh_p_cbcl.csv')\n",
    "# # only keep the  column.('cbcl_scr_syn_anxdep_t')\n",
    "# labels = labels[labels['eventname'] == 'baseline_year_1_arm_1'].drop(columns=['eventname'])\n",
    "# labels = labels[['src_subject_id', 'cbcl_scr_syn_anxdep_t']].dropna()\n",
    "\n",
    "# only keep the subjects that have labels\n",
    "data = data[data[\"src_subject_id\"].isin(labels[\"src_subject_id\"])]\n",
    "data = data.drop(columns=[\"src_subject_id\"])\n",
    "# reshaped_array = scores.iloc[:,1].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"G:/ABCD/data/mri_y_rsfmr_cor_gp_gp.csv\")\n",
    "data = (\n",
    "    data[data[\"eventname\"] == \"baseline_year_1_arm_1\"]\n",
    "    .drop(columns=[\"eventname\"])\n",
    "    .dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "\n",
    "\n",
    "# ========== 2. 交叉验证与预测 ==========\n",
    "def cross_validate_prediction(\n",
    "    X, y, cv_type=\"loocv\", k_folds=10, feature_selection=False\n",
    "):\n",
    "    if cv_type == \"loocv\":\n",
    "        cv = LeaveOneOut()\n",
    "    elif cv_type == \"kfold\":\n",
    "        cv = KFold(n_splits=k_folds, shuffle=True, random_state=9873)\n",
    "    else:\n",
    "        raise ValueError(\"cv_type 必须是 'loocv' 或 'kfold'\")\n",
    "\n",
    "    y_actual, y_pred = [], []\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        if feature_selection:\n",
    "            feature_corr = np.array(\n",
    "                [\n",
    "                    (\n",
    "                        stats.spearmanr(X_train[:, i], y_train)[0]\n",
    "                        if len(np.unique(X_train[:, i])) > 1\n",
    "                        else 0\n",
    "                    )\n",
    "                    for i in range(X_train.shape[1])\n",
    "                ]\n",
    "            )\n",
    "            selected_features = np.where(np.abs(feature_corr) > 0.1)[0]\n",
    "            if len(selected_features) == 0:\n",
    "                selected_features = np.arange(X_train.shape[1])\n",
    "            X_train, X_test = (\n",
    "                X_train[:, selected_features],\n",
    "                X_test[:, selected_features],\n",
    "            )\n",
    "\n",
    "        # model = LinearRegression()  # 或 Lasso(alpha=0.01)\n",
    "        model = Lasso(alpha=0.01)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred.extend(model.predict(X_test))\n",
    "        y_actual.extend(y_test)\n",
    "\n",
    "    return np.array(y_actual), np.array(y_pred)\n",
    "\n",
    "\n",
    "# ========== 3. 计算评估指标 ==========\n",
    "def evaluate_performance(y_actual, y_pred):\n",
    "    spearman_corr, spearman_p = stats.spearmanr(y_actual, y_pred)\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "\n",
    "    if np.isnan(spearman_corr):\n",
    "        spearman_corr, spearman_p = 0, 1\n",
    "\n",
    "    print(f\"Spearman 相关性: {spearman_corr:.4f} (p={spearman_p:.5f}), MSE: {mse:.4f}\")\n",
    "\n",
    "\n",
    "# ========== 4. 画图（预测 vs 真实值） ==========\n",
    "def plot_results(y_actual, y_pred):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_actual, y_pred, alpha=0.7, label=\"预测 vs. 真实值\")\n",
    "    plt.plot(\n",
    "        [min(y_actual), max(y_actual)],\n",
    "        [min(y_actual), max(y_actual)],\n",
    "        \"r--\",\n",
    "        label=\"理想情况\",\n",
    "    )\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(y_actual.reshape(-1, 1), y_pred)\n",
    "    y_fit = model.predict(y_actual.reshape(-1, 1))\n",
    "    plt.plot(\n",
    "        y_actual,\n",
    "        y_fit,\n",
    "        \"b-\",\n",
    "        label=f\"拟合线 (R²={model.score(y_actual.reshape(-1, 1), y_pred):.3f})\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"真实值 y\")\n",
    "    plt.ylabel(\"预测值 y_pred\")\n",
    "    plt.title(\"交叉验证预测结果\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ========== 运行代码 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 读取数据\n",
    "    X = np.array(data1)  # 转换为 numpy 数组，确保索引不影响\n",
    "    y = latent_factors.iloc[:, 0].values  # 显式转换为 numpy 数组\n",
    "    # y = factor_scores[:,0]\n",
    "    # X = data1.loc[:, :].values  # 确保 data1 按列取所有值\n",
    "    # y = latent_df.iloc[:, 0].reindex(data1.index).values  # 让 y 的索引与 data1 对齐\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # 2. 进行交叉验证（LOOCV 或 KFold），选择是否启用特征选择\n",
    "    cv_type = \"loocv\"  # 可改成 \"kfold\"\n",
    "    feature_selection = True  # 是否开启特征选择\n",
    "\n",
    "    y_actual, y_pred = cross_validate_prediction(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv_type=cv_type,\n",
    "        k_folds=10,\n",
    "        feature_selection=feature_selection,\n",
    "    )\n",
    "\n",
    "    # 3. 计算 Spearman 相关性 & 误差\n",
    "    evaluate_performance(y_actual, y_pred)\n",
    "\n",
    "    # 4. 画图\n",
    "    plot_results(y_actual, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 运行代码 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 读取数据\n",
    "    X = np.array(data1)  # 转换为 numpy 数组，确保索引不影响\n",
    "    # y = latent_df.iloc[:, 0].values  # 显式转换为 numpy 数组\n",
    "    y = factor_scores[:, 0]\n",
    "    # X = data1.loc[:, :].values  # 确保 data1 按列取所有值\n",
    "    # y = latent_df.iloc[:, 0].reindex(data1.index).values  # 让 y 的索引与 data1 对齐\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42\n",
    "    )\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # 2. 进行交叉验证（LOOCV 或 KFold），选择是否启用特征选择\n",
    "    cv_type = \"loocv\"  # 可改成 \"kfold\"\n",
    "    feature_selection = True  # 是否开启特征选择\n",
    "\n",
    "    y_actual, y_pred = cross_validate_prediction(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv_type=cv_type,\n",
    "        k_folds=10,\n",
    "        feature_selection=feature_selection,\n",
    "    )\n",
    "\n",
    "    # 3. 计算 Spearman 相关性 & 误差\n",
    "    evaluate_performance(y_actual, y_pred)\n",
    "\n",
    "    # 4. 画图\n",
    "    plot_results(y_actual, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting fmri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cx_Oracle\n",
    "\n",
    "# 手动指定 Instant Client 目录\n",
    "os.environ[\"PATH\"] = \"C:\\\\oracle\\\\instantclient_23_7;\" + os.environ[\"PATH\"]\n",
    "\n",
    "# 测试 cx_Oracle 是否正确加载\n",
    "print(cx_Oracle.clientversion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接数据库\n",
    "dsn = cx_Oracle.makedsn(\n",
    "    \"mindarvpc.cqahbwk3l1mb.us-east-1.rds.amazonaws.com\", 1521, service_name=\"ORCL\"\n",
    ")\n",
    "conn = cx_Oracle.connect(user=\"k21116947_1236370\", password=\"\", dsn=dsn)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 查询 ENDPOINT 列的前 10 条记录\n",
    "query = \"\"\"\n",
    "SELECT ENDPOINT\n",
    "FROM S3_LINKS\n",
    "WHERE ENDPOINT LIKE '%baseline%' AND ENDPOINT LIKE '%rsfMRI%' AND ENDPOINT LIKE '%NDARINV005V6D2C%' AND ENDPOINT LIKE '%MPROC%' AND ENDPOINT LIKE '%release5%'\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "\n",
    "s3_samples = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"S3_LINKS 表的 ENDPOINT 列数据示例：\")\n",
    "for url in s3_samples:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn = cx_Oracle.makedsn(\n",
    "    \"mindarvpc.cqahbwk3l1mb.us-east-1.rds.amazonaws.com\", 1521, service_name=\"ORCL\"\n",
    ")\n",
    "conn = cx_Oracle.connect(user=\"k21116947_1236370\", password=\"\", dsn=dsn)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ENDPOINT\n",
    "FROM S3_LINKS\n",
    "WHERE ENDPOINT LIKE '%baseline%' AND ENDPOINT LIKE '%rsfMRI%' AND ENDPOINT LIKE '%NDARINV005V6D2C%' AND ENDPOINT LIKE '%MPROC%' \n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "\n",
    "s3_samples = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# for url in s3_samples:\n",
    "#    print(url)\n",
    "\n",
    "np.savetxt(\"output/s3_links.txt\", s3_samples, fmt=\"%s\")\n",
    "\n",
    "\n",
    "!downloadcmd -dp 1236370 -t output/s3_links.txt -d ./"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
