{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from models import Autoencoder\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import cx_Oracle\n",
    "from utils import translate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = Path(os.getcwd())\n",
    "data_path = code_dir.parent / \"data\"\n",
    "assert os.path.exists(\n",
    "    data_path\n",
    "), \"Data directory not found. Make sure you're running this code from the root directory of the project.\"\n",
    "\n",
    "with open(data_path / \"cbcl_data_remove_unrelated.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qns = pd.read_csv(f)\n",
    "\n",
    "X = qns.iloc[:, 2:].values\n",
    "\n",
    "# Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train_raw, X_temp = train_test_split(X, test_size=0.4)\n",
    "X_val_raw, X_test_raw = train_test_split(X_temp, test_size=0.5)\n",
    "\n",
    "\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_val = scaler.transform(X_val_raw)\n",
    "X_test = scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(X_train, X_val, encoding_dim=5, \n",
    "                          layer1_neurons=69, layer2_neurons=58, layer3_neurons=53)\n",
    "autoencoder.train()\n",
    "(\n",
    "    latent_factors,\n",
    "    reconstruction_errors_test,\n",
    "    explained_variance_ratios,\n",
    "    explained_variance_ratio_test,\n",
    ") = autoencoder.evaluate_on_data(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "autoencoder.export_to_onnx(X_train, onnx_path = \"../output/autoencoder_real_input.onnx\")  # export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_factors = latent_factors  # convert to NumPy\n",
    "X_scaled = np.array(X_scaled)  # convert to NumPy\n",
    "\n",
    "# calculate loadings\n",
    "loadings = []\n",
    "for i in range(X_scaled.shape[1]):  # transversal all features\n",
    "    reg = LinearRegression().fit(latent_factors, X_scaled[:, i])  # regression\n",
    "    loadings.append(reg.coef_)  # store the coefficients\n",
    "\n",
    "loading_matrix = np.array(\n",
    "    loadings\n",
    ").T  #  transpose to (latent factors, original features)\n",
    "\n",
    "print(\"Loading matrix shape:\", loading_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the reconstuction error of the autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datasets\n",
    "datasets = {\"Train\": X_train, \"Validation\": X_val, \"Test\": X_test}\n",
    "\n",
    "# Plot the reconstruction errors\n",
    "autoencoder.plot_reconstruction_errors(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the possible dimensions of the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataFrame to store the results\n",
    "dim_df = pd.DataFrame(\n",
    "    columns=[\"dim\"] + [f\"Factor_{i+1}\" for i in range(10)] + [\"Total variance ratio\"]\n",
    ")\n",
    "\n",
    "for latent_dim in range(4, 6):\n",
    "    print(\"dim:\", latent_dim)\n",
    "\n",
    "    # Initialize and train the autoencoder\n",
    "    autoencoder = Autoencoder(X_train, X_val, encoding_dim=latent_dim)\n",
    "    autoencoder.train()\n",
    "\n",
    "    # Evaluate the autoencoder on the scaled data\n",
    "    (\n",
    "        latent_factors,\n",
    "        reconstruction_errors,\n",
    "        explained_variance_ratios,\n",
    "        explained_variance_ratio_total,\n",
    "    ) = autoencoder.evaluate_on_data(X_scaled)\n",
    "\n",
    "    # Create a temporary DataFrame to store the results for the current latent dimension\n",
    "    temp_df = pd.DataFrame(\n",
    "        {\n",
    "            \"dim\": [latent_dim],  # Current dimension\n",
    "            **{\n",
    "                f\"Factor_{i+1}\": ratio\n",
    "                for i, ratio in enumerate(explained_variance_ratios)\n",
    "            },\n",
    "            \"Total variance ratio\": [explained_variance_ratio_total],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Fill missing factor columns with NaN to maintain consistent column names\n",
    "    for col in dim_df.columns:\n",
    "        if col not in temp_df.columns:\n",
    "            temp_df[col] = None  # Fill with NaN\n",
    "\n",
    "    # Sort columns to match the order in dim_df\n",
    "    temp_df = temp_df[dim_df.columns]\n",
    "\n",
    "    # Concatenate the temporary DataFrame with the main DataFrame\n",
    "    dim_df = pd.concat([dim_df, temp_df], ignore_index=True)\n",
    "\n",
    "print(dim_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other approaches to extract factors: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = qns.iloc[:, 2:]  # the cbcl data\n",
    "\n",
    "rank = 5  # number of components to extract\n",
    "\n",
    "# NMF dicomposition\n",
    "nmf_model = NMF(\n",
    "    n_components=rank, init=\"random\", solver=\"mu\", max_iter=1000, random_state=42\n",
    ")\n",
    "W = nmf_model.fit_transform(data_cleaned)  # 基矩阵 W basis matrix\n",
    "H = nmf_model.components_  # 系数矩阵 H coefficients matrix\n",
    "\n",
    "# reconstruction error\n",
    "reconstruction_error = nmf_model.reconstruction_err_\n",
    "print(f\"Reconstruction error: {reconstruction_error}\")\n",
    "\n",
    "# reconstruct the matrix\n",
    "X_reconstructed = np.dot(W, H)\n",
    "\n",
    "# calculate Frobenius norm\n",
    "X_norm = np.linalg.norm(data_cleaned, ord=\"fro\")\n",
    "reconstruction_error_frobenius = np.linalg.norm(\n",
    "    data_cleaned - X_reconstructed, ord=\"fro\"\n",
    ")\n",
    "\n",
    "# calculate relative error\n",
    "relative_error = reconstruction_error_frobenius / X_norm\n",
    "print(f\"Relative Error: {relative_error}\")\n",
    "\n",
    "# calculate explained variance\n",
    "data_cleaned_np = data_cleaned.to_numpy()\n",
    "\n",
    "# calculate the total variance of the original data matrix\n",
    "total_variance = np.sum((data_cleaned_np - np.mean(data_cleaned_np)) ** 2)\n",
    "\n",
    "# calculate the variance of the reconstructed matrix\n",
    "reconstructed_variance = np.sum((X_reconstructed - np.mean(X_reconstructed)) ** 2)\n",
    "\n",
    "# variance_explained\n",
    "variance_explained = reconstructed_variance / total_variance\n",
    "print(f\"Variance explained by the NMF model: {variance_explained * 100:.2f}%\")\n",
    "\n",
    "# add ID to W\n",
    "W_with_id = pd.concat([pd.Series(qns.iloc[:, 1], name=\"ID\"), pd.DataFrame(W)], axis=1)\n",
    "\n",
    "# 转置 H\n",
    "H_transposed = H.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer(n_factors=5, rotation=\"varimax\")\n",
    "fa.fit(X_scaled)\n",
    "# Check Eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "ev\n",
    "factor_loadings = fa.loadings_\n",
    "# 计算每个因子的贡献方差和方差解释率\n",
    "variance_explained = fa.get_factor_variance()\n",
    "print(f\"varience explained: {variance_explained[2][-1]:.2%}\")\n",
    "\n",
    "# 计算所有个体的因子得分\n",
    "factor_scores = fa.transform(X_scaled)\n",
    "factor_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results from autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 latent_df 转换为 numpy 数组（如果它是 Pandas DataFrame）\n",
    "latent_factors = (\n",
    "    latent_factors.values\n",
    "    if isinstance(latent_factors, pd.DataFrame)\n",
    "    else latent_factors\n",
    ")\n",
    "original_features = X if isinstance(X, np.ndarray) else X.values\n",
    "\n",
    "# 存储每个原始特征的回归系数\n",
    "n_original_features = original_features.shape[1]\n",
    "n_latent_factors = latent_factors.shape[1]\n",
    "\n",
    "loadings = []\n",
    "\n",
    "# 对每个原始特征进行回归，使用 latent_factors 作为输入特征\n",
    "for i in range(n_original_features):\n",
    "    y = original_features[:, i]  # 当前原始特征\n",
    "    reg = LinearRegression().fit(latent_factors, y)\n",
    "    loadings.append(reg.coef_)\n",
    "\n",
    "# 将结果转换为 DataFrame，便于查看\n",
    "loadings_df = pd.DataFrame(\n",
    "    loadings, columns=[f\"Latent_{j+1}\" for j in range(n_latent_factors)]\n",
    ")\n",
    "loadings_df.index = [f\"Feature_{i+1}\" for i in range(n_original_features)]\n",
    "\n",
    "# 输出每个潜在因子对原始特征的贡献（类似于 PCA 的负载）the loading matrix of the autoencoder\n",
    "print(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_df.index = qns.iloc[:, 2:].columns\n",
    "df = pd.DataFrame()\n",
    "for i in range(latent_dim):\n",
    "    df0 = (\n",
    "        loadings_df[\"Latent_{0}\".format(i + 1)]\n",
    "        .reindex(\n",
    "            loadings_df[\"Latent_{0}\".format(i + 1)]\n",
    "            .abs()\n",
    "            .sort_values(ascending=False)\n",
    "            .index\n",
    "        )\n",
    "        .to_frame(name=\"Latent_{0}\".format(i + 1))\n",
    "    )\n",
    "    df0 = df0.reset_index().rename(columns={\"index\": \"Row_Name\"})\n",
    "    df = pd.concat([df, df0], axis=1)\n",
    "df_even_columns = df.iloc[\n",
    "    :, ::2\n",
    "]  # # Select all rows and every second column from the DataFrame\n",
    "\n",
    "details_autoencoder = translate_text(df_even_columns, 5, \"en\")\n",
    "details_autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results from NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_csv(\n",
    "    r\"G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\NMF_H.csv\"\n",
    ").iloc[:, 1:]\n",
    "label.index = qns.iloc[:, 2:].columns\n",
    "df_NMF = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    df0 = (\n",
    "        label[\"V{0}\".format(i + 1)]\n",
    "        .reindex(label[\"V{0}\".format(i + 1)].abs().sort_values(ascending=False).index)\n",
    "        .to_frame(name=\"V{0}\".format(i + 1))\n",
    "    )\n",
    "    df0 = df0.reset_index().rename(columns={\"index\": \"Row_Name{0}\".format(i + 1)})\n",
    "    df_NMF = pd.concat([df_NMF, df0], axis=1)\n",
    "df_NMF_even_columns = df_NMF.iloc[:, ::2]  # 选择所有行和每隔两列的列\n",
    "\n",
    "details = translate_text(df_NMF_even_columns, 5, \"en\")\n",
    "details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression model from questionnaire items to factors generated with NMF (from Toby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LassoAnalysis\n",
    "\n",
    "qns = X\n",
    "scores = pd.read_csv(\n",
    "    r\"G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\NMF_W.csv\"\n",
    ").iloc[:, 2:]\n",
    "\n",
    "# Initialize the LassoAnalysis class\n",
    "lasso_analysis = LassoAnalysis(qns, scores)\n",
    "\n",
    "# Perform the analysis\n",
    "lasso_analysis.perform_analysis()\n",
    "\n",
    "# Plot R² values\n",
    "lasso_analysis.plot_r2_values()\n",
    "\n",
    "# Plot factor predictions\n",
    "lasso_analysis.plot_factor_predictions(alpha=0.125)\n",
    "\n",
    "# Plot heatmap of coefficients\n",
    "lasso_analysis.plot_heatmap(alpha=0.125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be continued: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### complete pipeline for getting fmri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path of Oracle Instant Client\n",
    "os.environ[\"PATH\"] = \"C:\\\\oracle\\\\instantclient_23_7;\" + os.environ[\"PATH\"]\n",
    "print(cx_Oracle.clientversion())\n",
    "\n",
    "dsn = cx_Oracle.makedsn(\n",
    "    \"mindarvpc.cqahbwk3l1mb.us-east-1.rds.amazonaws.com\", 1521, service_name=\"ORCL\"\n",
    ")\n",
    "conn = cx_Oracle.connect(user=\"k21116947_1236370\", password=\"<input>\", dsn=dsn)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ENDPOINT\n",
    "FROM S3_LINKS\n",
    "WHERE ENDPOINT LIKE '%baseline%' AND ENDPOINT LIKE '%rsfMRI%' AND ENDPOINT LIKE '%NDARINV005V6D2C%' AND ENDPOINT LIKE '%MPROC%' \n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "\n",
    "s3_samples = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# for url in s3_samples:\n",
    "#    print(url)\n",
    "\n",
    "np.savetxt(\"data/s3_links.txt\", s3_samples, fmt=\"%s\")\n",
    "\n",
    "\n",
    "!downloadcmd -dp 1236370 -t data/s3_links.txt -d ./data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
