{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"G:\\ABCD\\script/trail/trail_tsne_RF\"\n",
    "# load data and drop the first column and the subject id\n",
    "data = pd.read_csv(data_path + \"/merged.csv\").drop(columns=[\"Unnamed: 0\", \"src_subject_id\"])\n",
    "label_columns = data.columns[data.columns.str.startswith(\"cbcl\")].tolist()\n",
    "\n",
    "data = data[label_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFA from previous research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed columns with low frequency: ['cbcl_q02_p', 'cbcl_q73_p', 'cbcl_q99_p', 'cbcl_q101_p', 'cbcl_q105_p']\n",
      "Highly correlated pairs (r > 0.75): [('cbcl_q08_p', 'cbcl_q10_p'), ('cbcl_q08_p', 'cbcl_q78_p'), ('cbcl_q20_p', 'cbcl_q21_p'), ('cbcl_q21_p', 'cbcl_q106_p'), ('cbcl_q22_p', 'cbcl_q28_p'), ('cbcl_q23_p', 'cbcl_q28_p'), ('cbcl_q25_p', 'cbcl_q48_p'), ('cbcl_q53_p', 'cbcl_q55_p'), ('cbcl_q56c_p', 'cbcl_q56f_p'), ('cbcl_q57_p', 'cbcl_q97_p'), ('cbcl_q81_p', 'cbcl_q82_p')]\n"
     ]
    }
   ],
   "source": [
    "# delete columns with low frequency (more than 99.5% of the values are 0)\n",
    "low_frequency_columns = data.columns[data.apply(lambda col: (col == 0).mean() > 0.995)]\n",
    "data_cleaned = data.drop(columns=low_frequency_columns)\n",
    "print(f\"Removed columns with low frequency: {low_frequency_columns.tolist()}\")\n",
    "\n",
    "#load corerlation matrix from polychoric correlation matrix\n",
    "\n",
    "correlation_matrix = pd.read_csv(data_path + \"/factor analysis/polychoric_correlation_matrix.csv\", index_col=0)\n",
    "\n",
    "# mark highly correlated pairs (r > 0.75)\n",
    "high_corr_pairs = (correlation_matrix.abs() > 0.75).where(lambda x: np.triu(x, 1)).stack().index.tolist()\n",
    "print(f\"Highly correlated pairs (r > 0.75): {high_corr_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "连通分量数量: 8\n",
      "每个连通分量的列名: [('cbcl_q08_p', 'cbcl_q10_p', 'cbcl_q78_p'), ('cbcl_q106_p', 'cbcl_q20_p', 'cbcl_q21_p'), ('cbcl_q22_p', 'cbcl_q23_p', 'cbcl_q28_p'), ('cbcl_q25_p', 'cbcl_q48_p'), ('cbcl_q53_p', 'cbcl_q55_p'), ('cbcl_q56c_p', 'cbcl_q56f_p'), ('cbcl_q57_p', 'cbcl_q97_p'), ('cbcl_q81_p', 'cbcl_q82_p')]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_connected_groups(pairs):\n",
    "    # 建立图结构\n",
    "    graph = defaultdict(set)\n",
    "    for col1, col2 in pairs:\n",
    "        graph[col1].add(col2)\n",
    "        graph[col2].add(col1)\n",
    "    \n",
    "    # 深度优先搜索（DFS）找到所有连通分量\n",
    "    visited = set()\n",
    "    connected_groups = []\n",
    "\n",
    "    def dfs(node, group):\n",
    "        visited.add(node)\n",
    "        group.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                dfs(neighbor, group)\n",
    "\n",
    "    # 遍历所有节点，找到每个连通分量\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            group = set()\n",
    "            dfs(node, group)\n",
    "            connected_groups.append(tuple(sorted(group)))\n",
    "\n",
    "    return connected_groups\n",
    "\n",
    "# 使用函数\n",
    "result = find_connected_groups(high_corr_pairs)\n",
    "print(\"number of connected groups:\", len(result))\n",
    "print(\"columns for each connected group:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe to store the final data\n",
    "data_final = data_cleaned.copy()\n",
    "for group in result:\n",
    "    # calculate the average of the columns in the group\n",
    "    data_final[f\"avg_{'_'.join(group)}\"] = data_cleaned[list(group)].mean(axis=1).round().astype(int)\n",
    "    # delete the original columns\n",
    "    data_final.drop(columns=list(group), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data_final\n",
    "\n",
    "data_final.to_csv(data_path + \"/factor analysis/data_final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data_final\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_final)\n",
    "\n",
    "#make data_scaled to dataframe\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bartlett's Test Chi-square: 287595.4009552738, p-value: 0.0\n",
      "KMO Test Score: 0.9628842767009992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\PhD\\Lib\\site-packages\\factor_analyzer\\utils.py:244: UserWarning: The inverse of the variance-covariance matrix was calculated using the Moore-Penrose generalized matrix inversion, due to its determinant being at or very close to zero.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor Loadings DataFrame:\n",
      "                              Factor 1  Factor 2  Factor 3  Factor 4  Factor 5\n",
      "cbcl_q01_p                   0.442501  0.004921  0.059961  0.243081 -0.107934\n",
      "cbcl_q03_p                   0.573576  0.036003  0.391045 -0.077239 -0.057303\n",
      "cbcl_q04_p                   0.534178 -0.014781  0.185261  0.281677 -0.058672\n",
      "cbcl_q05_p                   0.460084  0.004756 -0.122386 -0.034585 -0.113965\n",
      "cbcl_q06_p                   0.136429  0.050066 -0.050143  0.046862  0.033646\n",
      "...                               ...       ...       ...       ...       ...\n",
      "avg_cbcl_q25_p_cbcl_q48_p    0.496157  0.178429 -0.064771  0.019241 -0.093052\n",
      "avg_cbcl_q53_p_cbcl_q55_p    0.226577 -0.004396 -0.024196  0.011634  0.069192\n",
      "avg_cbcl_q56c_p_cbcl_q56f_p  0.299530 -0.106163 -0.023035 -0.059445  0.432832\n",
      "avg_cbcl_q57_p_cbcl_q97_p    0.382261  0.414919  0.015314 -0.176730  0.027314\n",
      "avg_cbcl_q81_p_cbcl_q82_p    0.285230  0.345121 -0.018197  0.015297  0.015132\n",
      "\n",
      "[103 rows x 5 columns]\n",
      "Variance Explained:\n",
      " (array([16.50238241,  2.39097666,  2.03783769,  1.76356159,  1.33830762]), array([0.1602173 , 0.02321337, 0.01978483, 0.01712196, 0.01299328]), array([0.1602173 , 0.18343067, 0.2032155 , 0.22033746, 0.23333074]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Bartlett 和 KMO 测试\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(data_scaled)\n",
    "print(f\"Bartlett's Test Chi-square: {chi_square_value}, p-value: {p_value}\")\n",
    "kmo_all, kmo_model = calculate_kmo(data_scaled)\n",
    "print(f\"KMO Test Score: {kmo_model}\")\n",
    "\n",
    "# factor analysis\n",
    "fa = FactorAnalyzer(n_factors=5, rotation=\"geomin_ort\")\n",
    "# fa = FactorAnalyzer(n_factors=5, rotation=\"varimax\")\n",
    "# fa.fit(data_cleaned)\n",
    "fa.fit(data_scaled)\n",
    "\n",
    "# factor loadings\n",
    "factor_loadings = fa.loadings_\n",
    "# factor_loadings_df = pd.DataFrame(factor_loadings, columns=[\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\", \"Factor 6\"])\n",
    "factor_loadings_df = pd.DataFrame(factor_loadings, columns=[f\"Factor {i}\" for i in range(1, 6)])\n",
    "# factor_loadings_df = pd.DataFrame(factor_loadings, columns=[\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\"])\n",
    "# factor_loadings_df.index = data_cleaned.columns\n",
    "factor_loadings_df.index = data_final.columns\n",
    "print(\"Factor Loadings DataFrame:\\n\", factor_loadings_df)\n",
    "\n",
    "# variance explained\n",
    "variance_explained = fa.get_factor_variance()\n",
    "print(\"Variance Explained:\\n\", variance_explained)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1602173 , 0.02321337, 0.01978483, 0.01712196, 0.01299328])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_explained[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
