{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from factor_analyzer import FactorAnalyzer\n",
    "# from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"G:\\ABCD\\script/trail/trail_tsne_RF\"\n",
    "# load data and drop the first column and the subject id\n",
    "data_raw = pd.read_csv(data_path + \"/merged.csv\")\n",
    "label_columns = data_raw.columns[data_raw.columns.str.startswith(\"cbcl\")].tolist()\n",
    "\n",
    "data_raw = data_raw[[\"src_subject_id\"] + label_columns]\n",
    "\n",
    "#out put directory\n",
    "rotation = \"oblimin\"\n",
    "output_dir = \"G:/ABCD/script/trail/trail_tsne_RF/factor analysis/output/\" + rotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the subject id for data_raw\n",
    "data = data_raw.drop(\"src_subject_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_7468\\2767713118.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  abcd_y_lt = pd.read_csv(\"G:\\ABCD\\\\data\\\\abcd_y_lt.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1个相同值的数量: 7590\n",
      "2个相同值的数量: 1701\n",
      "3个相同值的数量: 54\n",
      "4个相同值的数量: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_7468\\2767713118.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = merged_data.groupby('rel_family_id').apply(lambda x: x.sample(1)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#read form abcd_y_lt.csv\n",
    "abcd_y_lt = pd.read_csv(\"G:\\ABCD\\\\data\\\\abcd_y_lt.csv\")\n",
    "twins = abcd_y_lt[[\"src_subject_id\", \"rel_family_id\"]]\n",
    "twins_unique = twins.drop_duplicates(subset='src_subject_id', keep='first')\n",
    "# 假设 data 是另一个包含 src_subject_id 和其他列的 DataFrame\n",
    "merged_data = pd.merge(twins_unique, data_raw, on=\"src_subject_id\", how=\"inner\")\n",
    "# 假设你的 DataFrame 叫做 df，且 rel_family_id 是你要分析的列\n",
    "value_counts = merged_data['rel_family_id'].value_counts()\n",
    "\n",
    "# 找出所有不同的重复次数\n",
    "unique_counts = value_counts.value_counts().index.sort_values()\n",
    "\n",
    "# 循环输出每个重复次数的数量\n",
    "for count in unique_counts:\n",
    "    count_of_duplicates = (value_counts == count).sum()\n",
    "    print(f\"{count}个相同值的数量: {count_of_duplicates}\")\n",
    "\n",
    "data = merged_data.groupby('rel_family_id').apply(lambda x: x.sample(1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[\"src_subject_id\", \"rel_family_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0409 - mae: 0.1069 - val_loss: 0.0285 - val_mae: 0.0976\n",
      "Epoch 2/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0293 - mae: 0.0998 - val_loss: 0.0272 - val_mae: 0.0928\n",
      "Epoch 3/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.0271 - mae: 0.0934 - val_loss: 0.0255 - val_mae: 0.0900\n",
      "Epoch 4/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0254 - mae: 0.0908 - val_loss: 0.0246 - val_mae: 0.0885\n",
      "Epoch 5/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.0251 - mae: 0.0904 - val_loss: 0.0242 - val_mae: 0.0881\n",
      "Epoch 6/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.0248 - mae: 0.0896 - val_loss: 0.0241 - val_mae: 0.0876\n",
      "Epoch 7/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 0.0245 - mae: 0.0889 - val_loss: 0.0240 - val_mae: 0.0871\n",
      "Epoch 8/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 0.0248 - mae: 0.0897 - val_loss: 0.0239 - val_mae: 0.0868\n",
      "Epoch 9/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 0.0244 - mae: 0.0888 - val_loss: 0.0239 - val_mae: 0.0867\n",
      "Epoch 10/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.0241 - mae: 0.0883 - val_loss: 0.0239 - val_mae: 0.0871\n",
      "Epoch 11/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 0.0243 - mae: 0.0885 - val_loss: 0.0238 - val_mae: 0.0868\n",
      "Epoch 12/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0244 - mae: 0.0889 - val_loss: 0.0238 - val_mae: 0.0868\n",
      "Epoch 13/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 0.0245 - mae: 0.0892 - val_loss: 0.0238 - val_mae: 0.0870\n",
      "Epoch 14/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.0246 - mae: 0.0892 - val_loss: 0.0238 - val_mae: 0.0870\n",
      "Epoch 15/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 0.0246 - mae: 0.0895 - val_loss: 0.0237 - val_mae: 0.0866\n",
      "Epoch 16/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 0.0241 - mae: 0.0882 - val_loss: 0.0237 - val_mae: 0.0872\n",
      "Epoch 17/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 0.0246 - mae: 0.0895 - val_loss: 0.0237 - val_mae: 0.0863\n",
      "Epoch 18/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.0246 - mae: 0.0895 - val_loss: 0.0237 - val_mae: 0.0870\n",
      "Epoch 19/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.0242 - mae: 0.0884 - val_loss: 0.0237 - val_mae: 0.0865\n",
      "Epoch 20/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.0246 - mae: 0.0895 - val_loss: 0.0237 - val_mae: 0.0866\n",
      "Epoch 21/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 0.0242 - mae: 0.0883 - val_loss: 0.0237 - val_mae: 0.0862\n",
      "Epoch 22/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.0243 - mae: 0.0882 - val_loss: 0.0237 - val_mae: 0.0863\n",
      "Epoch 23/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 0.0243 - mae: 0.0881 - val_loss: 0.0237 - val_mae: 0.0864\n",
      "Epoch 24/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 0.0243 - mae: 0.0885 - val_loss: 0.0237 - val_mae: 0.0871\n",
      "Epoch 25/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 0.0240 - mae: 0.0882 - val_loss: 0.0237 - val_mae: 0.0862\n",
      "Epoch 26/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 0.0242 - mae: 0.0879 - val_loss: 0.0236 - val_mae: 0.0865\n",
      "Epoch 27/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.0244 - mae: 0.0887 - val_loss: 0.0236 - val_mae: 0.0863\n",
      "Epoch 28/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 0.0243 - mae: 0.0886 - val_loss: 0.0237 - val_mae: 0.0861\n",
      "Epoch 29/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.0245 - mae: 0.0888 - val_loss: 0.0237 - val_mae: 0.0861\n",
      "Epoch 30/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.0240 - mae: 0.0875 - val_loss: 0.0237 - val_mae: 0.0864\n",
      "Epoch 31/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 0.0243 - mae: 0.0882 - val_loss: 0.0236 - val_mae: 0.0858\n",
      "Epoch 32/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 0.0243 - mae: 0.0885 - val_loss: 0.0237 - val_mae: 0.0866\n",
      "Epoch 33/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.0244 - mae: 0.0884 - val_loss: 0.0236 - val_mae: 0.0855\n",
      "Epoch 34/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 0.0245 - mae: 0.0884 - val_loss: 0.0236 - val_mae: 0.0866\n",
      "Epoch 35/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.0240 - mae: 0.0873 - val_loss: 0.0236 - val_mae: 0.0859\n",
      "Epoch 36/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 0.0245 - mae: 0.0886 - val_loss: 0.0236 - val_mae: 0.0860\n",
      "Epoch 37/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 0.0245 - mae: 0.0885 - val_loss: 0.0236 - val_mae: 0.0858\n",
      "Epoch 38/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 0.0243 - mae: 0.0881 - val_loss: 0.0236 - val_mae: 0.0859\n",
      "Epoch 39/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.0242 - mae: 0.0880 - val_loss: 0.0236 - val_mae: 0.0860\n",
      "Epoch 40/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.0243 - mae: 0.0886 - val_loss: 0.0236 - val_mae: 0.0864\n",
      "Epoch 41/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.0242 - mae: 0.0881 - val_loss: 0.0236 - val_mae: 0.0859\n",
      "Epoch 42/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.0244 - mae: 0.0885 - val_loss: 0.0236 - val_mae: 0.0866\n",
      "Epoch 43/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.0243 - mae: 0.0885 - val_loss: 0.0236 - val_mae: 0.0862\n",
      "Epoch 44/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 0.0243 - mae: 0.0880 - val_loss: 0.0236 - val_mae: 0.0860\n",
      "Epoch 45/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.0245 - mae: 0.0886 - val_loss: 0.0236 - val_mae: 0.0859\n",
      "Epoch 46/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0242 - mae: 0.0878 - val_loss: 0.0237 - val_mae: 0.0865\n",
      "Epoch 47/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 0.0245 - mae: 0.0887 - val_loss: 0.0236 - val_mae: 0.0856\n",
      "Epoch 48/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.0245 - mae: 0.0887 - val_loss: 0.0236 - val_mae: 0.0859\n",
      "Epoch 49/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.0240 - mae: 0.0872 - val_loss: 0.0236 - val_mae: 0.0861\n",
      "Epoch 50/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 0.0243 - mae: 0.0881 - val_loss: 0.0236 - val_mae: 0.0858\n",
      "Epoch 51/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 0.0244 - mae: 0.0885 - val_loss: 0.0236 - val_mae: 0.0862\n",
      "Epoch 52/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.0243 - mae: 0.0883 - val_loss: 0.0236 - val_mae: 0.0861\n",
      "Epoch 53/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - loss: 0.0248 - mae: 0.0894 - val_loss: 0.0236 - val_mae: 0.0860\n",
      "Epoch 54/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 0.0245 - mae: 0.0888 - val_loss: 0.0236 - val_mae: 0.0859\n",
      "Epoch 55/100\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 0.0236 - mae: 0.0865 - val_loss: 0.0236 - val_mae: 0.0862\n",
      "Epoch 55: early stopping\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "        PC1       PC2       PC3       PC4       PC5\n",
      "0  0.265090  0.139077  0.113877  0.257824  0.067515\n",
      "1  0.258900  0.115422  0.020127  0.143958  0.128033\n",
      "2  0.638944  0.700696  0.913591  0.335130  0.184290\n",
      "3  0.586255  0.528877  0.186116  0.707368  0.616743\n",
      "4  0.376806  0.000000  0.235670  0.997396  1.624833\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Data Normalization\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# 2. Define a Simplified Autoencoder Structure\n",
    "input_dim = data.shape[1]\n",
    "encoding_dim = 5  # Capturing five main features\n",
    "\n",
    "# Input Layer\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# Encoding Layer\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Decoding Layer\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "# Construct Autoencoder Model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "# Encoder Model (to extract features)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# 3. Compile and Train the Model\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Use EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(data_scaled, data_scaled, epochs=100, batch_size=32, shuffle=True, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# 4. Extract Main Components\n",
    "encoded_data = encoder.predict(data_scaled)\n",
    "\n",
    "# Convert the extracted components to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=[f'PC{i+1}' for i in range(encoding_dim)])\n",
    "\n",
    "# View the extracted components\n",
    "print(encoded_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step\n",
      "Variance Explained: 0.3521\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 用训练好的Autoencoder对原始数据进行重构\n",
    "reconstructed_data = autoencoder.predict(data_scaled)\n",
    "\n",
    "# 计算每个特征的总方差\n",
    "total_variance = np.var(data_scaled, axis=0)\n",
    "\n",
    "# 计算重构误差 (MSE)\n",
    "reconstruction_error = mean_squared_error(data_scaled, reconstructed_data)\n",
    "\n",
    "# 计算解释的方差比例\n",
    "variance_explained_total = 1 - (reconstruction_error / np.mean(total_variance))\n",
    "\n",
    "print(f'Variance Explained: {variance_explained_total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor 1 Variance Explained: 8.85%\n",
      "Factor 2 Variance Explained: 4.27%\n",
      "Factor 3 Variance Explained: 5.52%\n",
      "Factor 4 Variance Explained: 6.78%\n",
      "Factor 5 Variance Explained: 9.80%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设你的负荷矩阵是 loading_matrix，形状为 (n_features, n_factors)\n",
    "loading_matrix = np.array(encoded_df)  # 替换为你的实际负荷矩阵\n",
    "\n",
    "# 计算每个因子的解释方差\n",
    "variance_explained = np.sum(loading_matrix ** 2, axis=0)\n",
    "\n",
    "# 将解释方差转换为百分比\n",
    "total_variance = np.sum(variance_explained)\n",
    "variance_explained_ratio = (variance_explained / total_variance) * 100\n",
    "\n",
    "# 输出每个因子的解释方差百分比\n",
    "for i, ratio in enumerate(variance_explained_ratio):\n",
    "    # print(f'Factor {i+1} Variance Explained: {ratio:.2f}%')\n",
    "    variance_explained_each = ratio*variance_explained_total\n",
    "    print(f'Factor {i+1} Variance Explained: {variance_explained_each:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 1, Average RMSE: 0.33886045973714163\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 2, Average RMSE: 0.3257940067190376\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 3, Average RMSE: 0.31681422516820434\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "J = 4, Average RMSE: 0.30991279810462624\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 5, Average RMSE: 0.3026250189044651\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "J = 6, Average RMSE: 0.29707249050530427\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "J = 7, Average RMSE: 0.2913985062655547\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "J = 8, Average RMSE: 0.2878472329276103\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 9, Average RMSE: 0.2842513377721056\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 10, Average RMSE: 0.27790517052154656\n",
      "Optimal J: 10, Minimum RMSE: 0.27790517052154656\n",
      "Epoch 1/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 0.1382\n",
      "Epoch 2/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 0.1019\n",
      "Epoch 3/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.0949\n",
      "Epoch 4/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0900 \n",
      "Epoch 5/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 0.0891\n",
      "Epoch 6/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0876\n",
      "Epoch 7/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0855 \n",
      "Epoch 8/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 0.0851\n",
      "Epoch 9/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 0.0832\n",
      "Epoch 10/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0817\n",
      "Epoch 11/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0821 \n",
      "Epoch 12/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 0.0814\n",
      "Epoch 13/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0805\n",
      "Epoch 14/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0805\n",
      "Epoch 15/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0810\n",
      "Epoch 16/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 0.0803\n",
      "Epoch 17/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0797\n",
      "Epoch 18/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0793\n",
      "Epoch 19/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0802 \n",
      "Epoch 20/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - loss: 0.0784\n",
      "Epoch 21/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0797\n",
      "Epoch 22/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.0792\n",
      "Epoch 23/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 0.0787\n",
      "Epoch 24/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0776\n",
      "Epoch 25/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0785\n",
      "Epoch 26/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0782\n",
      "Epoch 27/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0771\n",
      "Epoch 28/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0789\n",
      "Epoch 29/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0769\n",
      "Epoch 30/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 0.0764\n",
      "Epoch 31/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0772 \n",
      "Epoch 32/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0779\n",
      "Epoch 33/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0778\n",
      "Epoch 34/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0765 \n",
      "Epoch 35/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0778\n",
      "Epoch 36/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0774 \n",
      "Epoch 37/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0757\n",
      "Epoch 38/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0750\n",
      "Epoch 39/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0775\n",
      "Epoch 40/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0765\n",
      "Epoch 41/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0759\n",
      "Epoch 42/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0776\n",
      "Epoch 43/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0758\n",
      "Epoch 44/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 0.0765\n",
      "Epoch 45/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0764 \n",
      "Epoch 46/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0752\n",
      "Epoch 47/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 0.0759\n",
      "Epoch 48/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0762\n",
      "Epoch 49/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0755\n",
      "Epoch 50/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0766\n",
      "Epoch 51/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0756\n",
      "Epoch 52/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0763\n",
      "Epoch 53/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0748\n",
      "Epoch 54/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0748\n",
      "Epoch 55/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0757\n",
      "Epoch 56/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0755\n",
      "Epoch 57/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0760\n",
      "Epoch 58/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0745\n",
      "Epoch 59/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 0.0765\n",
      "Epoch 60/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0751\n",
      "Epoch 61/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0754\n",
      "Epoch 62/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.0753\n",
      "Epoch 63/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0756\n",
      "Epoch 64/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0737\n",
      "Epoch 65/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0752\n",
      "Epoch 66/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0743\n",
      "Epoch 67/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 0.0745\n",
      "Epoch 68/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0754\n",
      "Epoch 69/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0754  \n",
      "Epoch 70/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0746\n",
      "Epoch 71/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.0744\n",
      "Epoch 72/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0746\n",
      "Epoch 73/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0742  \n",
      "Epoch 74/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0746\n",
      "Epoch 75/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0752\n",
      "Epoch 76/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.0745\n",
      "Epoch 77/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0760\n",
      "Epoch 78/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0735\n",
      "Epoch 79/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.0744\n",
      "Epoch 80/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 0.0741\n",
      "Epoch 81/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0740\n",
      "Epoch 82/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0745\n",
      "Epoch 83/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0731\n",
      "Epoch 84/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0748\n",
      "Epoch 85/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0740\n",
      "Epoch 86/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0741\n",
      "Epoch 87/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 0.0749\n",
      "Epoch 88/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0736\n",
      "Epoch 89/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0738\n",
      "Epoch 90/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0745\n",
      "Epoch 91/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0744  \n",
      "Epoch 92/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0747\n",
      "Epoch 93/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0741\n",
      "Epoch 94/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0742\n",
      "Epoch 95/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0736\n",
      "Epoch 96/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 0.0741\n",
      "Epoch 97/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 0.0746\n",
      "Epoch 98/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0740\n",
      "Epoch 99/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.0729\n",
      "Epoch 100/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f437105550>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Load data from dataframe\n",
    "d = data.shape[1]  # Input dimension based on dataframe\n",
    "X = data.values  # Convert dataframe to numpy array\n",
    "\n",
    "# Define a function to build a deep autoencoder model\n",
    "def build_autoencoder(input_dim, J):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder layers\n",
    "    encoded = Dense(8 * J, activation='relu')(input_layer)\n",
    "    bottleneck = Dense(J, activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder layers\n",
    "    decoded = Dense(8 * J, activation='relu')(bottleneck)\n",
    "    output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    # Define the autoencoder model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return autoencoder\n",
    "\n",
    "# Grid search for optimal J (1 <= J <= 10)\n",
    "min_rmse = float('inf')\n",
    "optimal_J = None\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for J in range(1, 11):\n",
    "    rmses = []\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        \n",
    "        # Build and compile the model\n",
    "        autoencoder = build_autoencoder(input_dim=d, J=J)\n",
    "        autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model\n",
    "        autoencoder.fit(X_train, X_train, epochs=100, batch_size=64, verbose=0, validation_data=(X_val, X_val))\n",
    "        \n",
    "        # Predict on validation set\n",
    "        X_val_pred = autoencoder.predict(X_val)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = sqrt(mean_squared_error(X_val, X_val_pred))\n",
    "        rmses.append(rmse)\n",
    "    \n",
    "    # Average RMSE for current J\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    print(f'J = {J}, Average RMSE: {avg_rmse}')\n",
    "    \n",
    "    # Update optimal J\n",
    "    if avg_rmse < min_rmse:\n",
    "        min_rmse = avg_rmse\n",
    "        optimal_J = J\n",
    "\n",
    "print(f'Optimal J: {optimal_J}, Minimum RMSE: {min_rmse}')\n",
    "\n",
    "# Build the final model with the optimal J\n",
    "autoencoder = build_autoencoder(input_dim=d, J=optimal_J)\n",
    "autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "autoencoder.fit(X, X, epochs=100, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step\n",
      "Explained Variance Score: 0.3687\n"
     ]
    }
   ],
   "source": [
    "# 使用训练好的模型进行预测\n",
    "X_reconstructed = autoencoder.predict(X)\n",
    "\n",
    "# 使用 explained_variance_score 计算解释方差\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "explained_variance = explained_variance_score(X, X_reconstructed)\n",
    "print(f'Explained Variance Score: {explained_variance:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFA from previous research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed columns with low frequency: ['cbcl_q02_p', 'cbcl_q73_p', 'cbcl_q99_p', 'cbcl_q101_p', 'cbcl_q105_p']\n",
      "Highly correlated pairs (r > 0.75): [('cbcl_q08_p', 'cbcl_q10_p'), ('cbcl_q08_p', 'cbcl_q78_p'), ('cbcl_q16_p', 'cbcl_q97_p'), ('cbcl_q20_p', 'cbcl_q21_p'), ('cbcl_q22_p', 'cbcl_q28_p'), ('cbcl_q23_p', 'cbcl_q28_p'), ('cbcl_q25_p', 'cbcl_q48_p'), ('cbcl_q40_p', 'cbcl_q70_p'), ('cbcl_q53_p', 'cbcl_q55_p'), ('cbcl_q56c_p', 'cbcl_q56f_p'), ('cbcl_q57_p', 'cbcl_q97_p'), ('cbcl_q81_p', 'cbcl_q82_p')]\n"
     ]
    }
   ],
   "source": [
    "# delete columns with low frequency (more than 99.5% of the values are 0)\n",
    "low_frequency_columns = data.columns[data.apply(lambda col: (col == 0).mean() > 0.995)]\n",
    "data_cleaned = data.drop(columns=low_frequency_columns)\n",
    "print(f\"Removed columns with low frequency: {low_frequency_columns.tolist()}\")\n",
    "\n",
    "#load corerlation matrix from polychoric correlation matrix\n",
    "\n",
    "correlation_matrix = pd.read_csv(data_path + \"/factor analysis/output/polychoric_correlation_matrix_twins.csv\", index_col=0)\n",
    "\n",
    "# mark highly correlated pairs (r > 0.75)\n",
    "high_corr_pairs = (correlation_matrix.abs() > 0.75).where(lambda x: np.triu(x, 1)).stack().index.tolist()\n",
    "print(f\"Highly correlated pairs (r > 0.75): {high_corr_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of connected groups: 9\n",
      "columns for each connected group: [('cbcl_q08_p', 'cbcl_q10_p', 'cbcl_q78_p'), ('cbcl_q16_p', 'cbcl_q57_p', 'cbcl_q97_p'), ('cbcl_q20_p', 'cbcl_q21_p'), ('cbcl_q22_p', 'cbcl_q23_p', 'cbcl_q28_p'), ('cbcl_q25_p', 'cbcl_q48_p'), ('cbcl_q40_p', 'cbcl_q70_p'), ('cbcl_q53_p', 'cbcl_q55_p'), ('cbcl_q56c_p', 'cbcl_q56f_p'), ('cbcl_q81_p', 'cbcl_q82_p')]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_connected_groups(pairs):\n",
    "    # 建立图结构\n",
    "    graph = defaultdict(set)\n",
    "    for col1, col2 in pairs:\n",
    "        graph[col1].add(col2)\n",
    "        graph[col2].add(col1)\n",
    "    \n",
    "    # 深度优先搜索（DFS）找到所有连通分量\n",
    "    visited = set()\n",
    "    connected_groups = []\n",
    "\n",
    "    def dfs(node, group):\n",
    "        visited.add(node)\n",
    "        group.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                dfs(neighbor, group)\n",
    "\n",
    "    # 遍历所有节点，找到每个连通分量\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            group = set()\n",
    "            dfs(node, group)\n",
    "            connected_groups.append(tuple(sorted(group)))\n",
    "\n",
    "    return connected_groups\n",
    "\n",
    "# 使用函数\n",
    "result = find_connected_groups(high_corr_pairs)\n",
    "print(\"number of connected groups:\", len(result))\n",
    "print(\"columns for each connected group:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe to store the final data\n",
    "data_final = data_cleaned.copy()\n",
    "for group in result:\n",
    "    # calculate the average of the columns in the group\n",
    "    data_final[f\"avg_{'_'.join(group)}\"] = data_cleaned[list(group)].mean(axis=1).round().astype(int)\n",
    "    # delete the original columns\n",
    "    data_final.drop(columns=list(group), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #scale data_final\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# data_scaled = scaler.fit_transform(data_final)\n",
    "\n",
    "# #make data_scaled to dataframe\n",
    "# data_scaled = pd.DataFrame(data_scaled, columns=data_final.columns)\n",
    "data_scaled = data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled.drop(columns=[\"src_subject_id\", \"rel_family_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bartlett's Test Chi-square: 238146.69520761166, p-value: 0.0\n",
      "KMO Test Score: 0.9624037123398348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\PhD\\Lib\\site-packages\\factor_analyzer\\utils.py:244: UserWarning: The inverse of the variance-covariance matrix was calculated using the Moore-Penrose generalized matrix inversion, due to its determinant being at or very close to zero.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor Loadings DataFrame:\n",
      "                              Factor 1  Factor 2  Factor 3  Factor 4  Factor 5  \\\n",
      "cbcl_q01_p                   0.158746  0.054357 -0.137513  0.317554 -0.009661   \n",
      "cbcl_q03_p                   0.583481 -0.001042  0.031194  0.142819  0.032724   \n",
      "cbcl_q04_p                   0.147039 -0.034074 -0.041008  0.566630  0.037088   \n",
      "cbcl_q05_p                   0.196349 -0.046049 -0.013099  0.202941  0.009528   \n",
      "cbcl_q06_p                  -0.054184  0.004284 -0.008707 -0.017063  0.009467   \n",
      "...                               ...       ...       ...       ...       ...   \n",
      "avg_cbcl_q25_p_cbcl_q48_p    0.006006  0.040872  0.114927  0.026406  0.012949   \n",
      "avg_cbcl_q40_p_cbcl_q70_p   -0.058122  0.004397  0.236370 -0.087217  0.070249   \n",
      "avg_cbcl_q53_p_cbcl_q55_p   -0.021818  0.035933  0.024859 -0.082826 -0.033896   \n",
      "avg_cbcl_q56c_p_cbcl_q56f_p  0.005402  0.015606 -0.031792 -0.036812  0.678849   \n",
      "avg_cbcl_q81_p_cbcl_q82_p   -0.052846  0.014157  0.557623  0.051722 -0.048664   \n",
      "\n",
      "                             Factor 6  Factor 7  Factor 8  Factor 9  \\\n",
      "cbcl_q01_p                   0.038407 -0.115113  0.068428  0.092101   \n",
      "cbcl_q03_p                  -0.018788  0.002771 -0.110690  0.034025   \n",
      "cbcl_q04_p                   0.030110  0.027396 -0.037527  0.049811   \n",
      "cbcl_q05_p                   0.183087  0.035233  0.050562 -0.007544   \n",
      "cbcl_q06_p                  -0.038526 -0.028323 -0.023193  0.713100   \n",
      "...                               ...       ...       ...       ...   \n",
      "avg_cbcl_q25_p_cbcl_q48_p    0.026598  0.029737 -0.006953  0.063788   \n",
      "avg_cbcl_q40_p_cbcl_q70_p   -0.054602 -0.055378  0.222498  0.012704   \n",
      "avg_cbcl_q53_p_cbcl_q55_p   -0.056978 -0.021421 -0.081874 -0.007582   \n",
      "avg_cbcl_q56c_p_cbcl_q56f_p -0.019178  0.055137  0.029425  0.027449   \n",
      "avg_cbcl_q81_p_cbcl_q82_p    0.030524  0.007799  0.063038  0.063060   \n",
      "\n",
      "                             Factor 10  Factor 11  Factor 12  Factor 13  \\\n",
      "cbcl_q01_p                   -0.025733   0.038476   0.105795   0.230014   \n",
      "cbcl_q03_p                    0.090476   0.052660  -0.103383   0.036130   \n",
      "cbcl_q04_p                    0.119454   0.016021  -0.051581   0.028233   \n",
      "cbcl_q05_p                    0.090549   0.052506   0.084848  -0.112601   \n",
      "cbcl_q06_p                    0.023510  -0.001036  -0.002611  -0.021785   \n",
      "...                                ...        ...        ...        ...   \n",
      "avg_cbcl_q25_p_cbcl_q48_p     0.032268   0.044241   0.126943  -0.037628   \n",
      "avg_cbcl_q40_p_cbcl_q70_p     0.155627  -0.027468   0.141935   0.099658   \n",
      "avg_cbcl_q53_p_cbcl_q55_p    -0.017702   0.011805  -0.154187   0.048614   \n",
      "avg_cbcl_q56c_p_cbcl_q56f_p   0.029578  -0.022269  -0.040092  -0.012600   \n",
      "avg_cbcl_q81_p_cbcl_q82_p    -0.039128   0.039008   0.074715   0.066704   \n",
      "\n",
      "                             Factor 14  Factor 15  Factor 16  \n",
      "cbcl_q01_p                    0.263575  -0.083768   0.054505  \n",
      "cbcl_q03_p                    0.040839   0.009997   0.021058  \n",
      "cbcl_q04_p                    0.023553   0.033965   0.050972  \n",
      "cbcl_q05_p                    0.041886   0.235097  -0.264961  \n",
      "cbcl_q06_p                    0.010959  -0.030562  -0.147373  \n",
      "...                                ...        ...        ...  \n",
      "avg_cbcl_q25_p_cbcl_q48_p     0.557148   0.071217  -0.037586  \n",
      "avg_cbcl_q40_p_cbcl_q70_p    -0.046192   0.099780   0.067468  \n",
      "avg_cbcl_q53_p_cbcl_q55_p     0.140610   0.632542   0.126481  \n",
      "avg_cbcl_q56c_p_cbcl_q56f_p   0.000307   0.056562  -0.003889  \n",
      "avg_cbcl_q81_p_cbcl_q82_p    -0.055875  -0.012269   0.072156  \n",
      "\n",
      "[102 rows x 16 columns]\n",
      "Variance Explained:\n",
      " (array([3.67359709, 1.75560805, 3.21420174, 2.89985508, 2.31644997,\n",
      "       2.2742063 , 2.64637479, 1.73200009, 1.62380833, 2.00611448,\n",
      "       1.70086874, 1.81227118, 1.57741179, 1.93266923, 1.88994364,\n",
      "       1.52974663]), array([0.03601566, 0.01721184, 0.03151178, 0.02842995, 0.02271029,\n",
      "       0.02229614, 0.02594485, 0.01698039, 0.01591969, 0.01966779,\n",
      "       0.01667518, 0.01776736, 0.01546482, 0.01894774, 0.01852886,\n",
      "       0.01499752]), array([0.03601566, 0.0532275 , 0.08473928, 0.11316923, 0.13587953,\n",
      "       0.15817567, 0.18412052, 0.20110091, 0.2170206 , 0.23668839,\n",
      "       0.25336358, 0.27113094, 0.28659576, 0.3055435 , 0.32407236,\n",
      "       0.33906987]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Bartlett 和 KMO 测试\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(data_scaled)\n",
    "print(f\"Bartlett's Test Chi-square: {chi_square_value}, p-value: {p_value}\")\n",
    "kmo_all, kmo_model = calculate_kmo(data_scaled)\n",
    "print(f\"KMO Test Score: {kmo_model}\")\n",
    "\n",
    "# factor analysis\n",
    "fa = FactorAnalyzer(n_factors=16,  rotation= rotation, method = 'principal')\n",
    "# fa = FactorAnalyzer(n_factors=16, rotation=\"equamax\", method='principal')\n",
    "# fa.fit(data_cleaned)\n",
    "fa.fit(data_scaled)\n",
    "\n",
    "# factor loadings\n",
    "factor_loadings = fa.loadings_\n",
    "# factor_loadings_df = pd.DataFrame(factor_loadings, columns=[\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\", \"Factor 6\"])\n",
    "factor_loadings_df = pd.DataFrame(factor_loadings, columns=[f\"Factor {i}\" for i in range(1, 17)])\n",
    "# factor_loadings_df = pd.DataFrame(factor_loadings, columns=[\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\"])\n",
    "# factor_loadings_df.index = data_cleaned.columns\n",
    "factor_loadings_df.index = data_final.columns\n",
    "print(\"Factor Loadings DataFrame:\\n\", factor_loadings_df)\n",
    "\n",
    "# variance explained\n",
    "variance_explained = fa.get_factor_variance()\n",
    "print(\"Variance Explained:\\n\", variance_explained)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03245182, 0.01720842, 0.02686883, 0.01793263, 0.02223959,\n",
       "       0.02282064, 0.01686017, 0.0254489 , 0.01819661, 0.01726673,\n",
       "       0.02757612, 0.01682084, 0.02125505, 0.01760954, 0.01823394,\n",
       "       0.01747789])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_explained[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33626773081510103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_explained[2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor 1:\n",
      " cbcl_q95_p                               0.576580\n",
      "cbcl_q86_p                               0.562006\n",
      "cbcl_q03_p                               0.533706\n",
      "cbcl_q109_p                              0.519226\n",
      "cbcl_q87_p                               0.509411\n",
      "cbcl_q68_p                               0.483531\n",
      "cbcl_q88_p                               0.474791\n",
      "cbcl_q27_p                               0.386796\n",
      "cbcl_q14_p                               0.377646\n",
      "cbcl_q19_p                               0.361401\n",
      "cbcl_q33_p                               0.354315\n",
      "cbcl_q103_p                              0.268592\n",
      "avg_cbcl_q22_p_cbcl_q23_p_cbcl_q28_p     0.265540\n",
      "cbcl_q26_p                               0.244424\n",
      "cbcl_q104_p                              0.235022\n",
      "cbcl_q05_p                               0.181576\n",
      "cbcl_q43_p                               0.178289\n",
      "cbcl_q41_p                               0.166262\n",
      "cbcl_q89_p                               0.155802\n",
      "cbcl_q16_p                               0.149191\n",
      "cbcl_q83_p                               0.148031\n",
      "cbcl_q12_p                               0.144975\n",
      "cbcl_q01_p                               0.140634\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.136975\n",
      "cbcl_q04_p                               0.132776\n",
      "cbcl_q35_p                               0.130499\n",
      "cbcl_q07_p                               0.128027\n",
      "cbcl_q65_p                               0.118337\n",
      "avg_cbcl_q57_p_cbcl_q97_p                0.107472\n",
      "cbcl_q34_p                               0.102004\n",
      "cbcl_q94_p                               0.100273\n",
      "cbcl_q11_p                               0.100188\n",
      "Name: Factor 1, dtype: float64\n",
      "Factor 2:\n",
      " cbcl_q11_p     0.543186\n",
      "cbcl_q29_p     0.449552\n",
      "cbcl_q50_p     0.398570\n",
      "cbcl_q14_p     0.331921\n",
      "cbcl_q19_p     0.294296\n",
      "cbcl_q45_p     0.279018\n",
      "cbcl_q27_p     0.246548\n",
      "cbcl_q75_p     0.239847\n",
      "cbcl_q112_p    0.208093\n",
      "cbcl_q77_p     0.191310\n",
      "cbcl_q12_p     0.186215\n",
      "cbcl_q30_p     0.180539\n",
      "cbcl_q31_p     0.176981\n",
      "cbcl_q71_p     0.174352\n",
      "cbcl_q93_p     0.168793\n",
      "cbcl_q09_p     0.155282\n",
      "cbcl_q32_p     0.130228\n",
      "cbcl_q56d_p    0.127176\n",
      "cbcl_q01_p     0.123036\n",
      "cbcl_q46_p     0.117448\n",
      "cbcl_q64_p     0.117348\n",
      "cbcl_q56e_p    0.116584\n",
      "cbcl_q47_p     0.111428\n",
      "cbcl_q52_p     0.108013\n",
      "Name: Factor 2, dtype: float64\n",
      "Factor 3:\n",
      " avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p     0.620373\n",
      "cbcl_q61_p                               0.563955\n",
      "cbcl_q04_p                               0.559464\n",
      "cbcl_q17_p                               0.432809\n",
      "cbcl_q13_p                               0.423477\n",
      "cbcl_q80_p                               0.413497\n",
      "cbcl_q01_p                               0.370795\n",
      "cbcl_q41_p                               0.358645\n",
      "avg_cbcl_q22_p_cbcl_q23_p_cbcl_q28_p     0.348738\n",
      "cbcl_q43_p                               0.308499\n",
      "cbcl_q24_p                               0.250190\n",
      "cbcl_q26_p                               0.243927\n",
      "cbcl_q74_p                               0.239306\n",
      "cbcl_q09_p                               0.219182\n",
      "cbcl_q79_p                               0.208614\n",
      "cbcl_q05_p                               0.199410\n",
      "cbcl_q39_p                               0.169555\n",
      "cbcl_q64_p                               0.159150\n",
      "cbcl_q03_p                               0.156962\n",
      "cbcl_q35_p                               0.148305\n",
      "cbcl_q19_p                               0.137217\n",
      "cbcl_q62_p                               0.134806\n",
      "cbcl_q46_p                               0.128695\n",
      "cbcl_q11_p                               0.127419\n",
      "cbcl_q12_p                               0.109472\n",
      "cbcl_q72_p                               0.103633\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.100178\n",
      "Name: Factor 3, dtype: float64\n",
      "Factor 4:\n",
      " cbcl_q66_p                               0.478609\n",
      "cbcl_q46_p                               0.477776\n",
      "cbcl_q84_p                               0.389118\n",
      "cbcl_q85_p                               0.358588\n",
      "cbcl_q45_p                               0.328696\n",
      "cbcl_q09_p                               0.312271\n",
      "cbcl_q13_p                               0.304117\n",
      "cbcl_q80_p                               0.286350\n",
      "cbcl_q17_p                               0.236819\n",
      "cbcl_q50_p                               0.216307\n",
      "cbcl_q44_p                               0.195732\n",
      "cbcl_q89_p                               0.179435\n",
      "cbcl_q111_p                              0.175640\n",
      "cbcl_q104_p                              0.166449\n",
      "cbcl_q56h_p                              0.165431\n",
      "cbcl_q58_p                               0.150049\n",
      "cbcl_q112_p                              0.138653\n",
      "cbcl_q42_p                               0.134137\n",
      "cbcl_q83_p                               0.124152\n",
      "cbcl_q67_p                               0.120872\n",
      "cbcl_q05_p                               0.120356\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.106961\n",
      "avg_cbcl_q57_p_cbcl_q97_p                0.103992\n",
      "cbcl_q93_p                               0.101624\n",
      "cbcl_q87_p                               0.100586\n",
      "Name: Factor 4, dtype: float64\n",
      "Factor 5:\n",
      " avg_cbcl_q56c_p_cbcl_q56f_p    0.671208\n",
      "cbcl_q56b_p                    0.610904\n",
      "cbcl_q56a_p                    0.554242\n",
      "cbcl_q56g_p                    0.553294\n",
      "cbcl_q56h_p                    0.422173\n",
      "cbcl_q51_p                     0.407212\n",
      "cbcl_q56e_p                    0.323985\n",
      "cbcl_q56d_p                    0.293442\n",
      "cbcl_q49_p                     0.161501\n",
      "cbcl_q36_p                     0.151977\n",
      "cbcl_q30_p                     0.142850\n",
      "cbcl_q24_p                     0.123910\n",
      "cbcl_q47_p                     0.119262\n",
      "cbcl_q54_p                     0.114384\n",
      "cbcl_q112_p                    0.108377\n",
      "cbcl_q92_p                     0.102961\n",
      "Name: Factor 5, dtype: float64\n",
      "Factor 6:\n",
      " cbcl_q75_p     0.630346\n",
      "cbcl_q65_p     0.525288\n",
      "cbcl_q69_p     0.510136\n",
      "cbcl_q42_p     0.500382\n",
      "cbcl_q111_p    0.456136\n",
      "cbcl_q71_p     0.411863\n",
      "cbcl_q64_p     0.214251\n",
      "cbcl_q05_p     0.188719\n",
      "cbcl_q63_p     0.175561\n",
      "cbcl_q32_p     0.175018\n",
      "cbcl_q80_p     0.170421\n",
      "cbcl_q102_p    0.169801\n",
      "cbcl_q86_p     0.162424\n",
      "cbcl_q83_p     0.158160\n",
      "cbcl_q88_p     0.157136\n",
      "cbcl_q29_p     0.154891\n",
      "cbcl_q17_p     0.140455\n",
      "cbcl_q112_p    0.138921\n",
      "cbcl_q103_p    0.137359\n",
      "cbcl_q24_p     0.123618\n",
      "cbcl_q110_p    0.122197\n",
      "cbcl_q87_p     0.119936\n",
      "cbcl_q77_p     0.116565\n",
      "Name: Factor 6, dtype: float64\n",
      "Factor 7:\n",
      " cbcl_q60_p                   0.726504\n",
      "cbcl_q59_p                   0.709374\n",
      "cbcl_q96_p                   0.532834\n",
      "cbcl_q67_p                   0.278674\n",
      "cbcl_q110_p                  0.169785\n",
      "cbcl_q84_p                   0.159885\n",
      "avg_cbcl_q81_p_cbcl_q82_p    0.148425\n",
      "cbcl_q72_p                   0.137141\n",
      "cbcl_q56g_p                  0.115555\n",
      "cbcl_q15_p                   0.107969\n",
      "cbcl_q01_p                   0.107109\n",
      "cbcl_q64_p                   0.105044\n",
      "Name: Factor 7, dtype: float64\n",
      "Factor 8:\n",
      " cbcl_q35_p     0.594795\n",
      "cbcl_q52_p     0.552469\n",
      "cbcl_q31_p     0.520435\n",
      "cbcl_q32_p     0.486210\n",
      "cbcl_q18_p     0.450086\n",
      "cbcl_q91_p     0.438002\n",
      "cbcl_q33_p     0.392228\n",
      "cbcl_q112_p    0.338311\n",
      "cbcl_q103_p    0.315611\n",
      "cbcl_q34_p     0.300360\n",
      "cbcl_q71_p     0.267188\n",
      "cbcl_q50_p     0.263162\n",
      "cbcl_q30_p     0.256798\n",
      "cbcl_q45_p     0.201332\n",
      "cbcl_q12_p     0.200065\n",
      "cbcl_q67_p     0.190862\n",
      "cbcl_q88_p     0.164731\n",
      "cbcl_q09_p     0.135727\n",
      "cbcl_q15_p     0.129259\n",
      "cbcl_q17_p     0.105124\n",
      "cbcl_q72_p     0.101719\n",
      "cbcl_q87_p     0.100365\n",
      "Name: Factor 8, dtype: float64\n",
      "Factor 9:\n",
      " cbcl_q100_p                             0.687832\n",
      "cbcl_q76_p                              0.613924\n",
      "cbcl_q45_p                              0.316958\n",
      "cbcl_q47_p                              0.310779\n",
      "cbcl_q50_p                              0.289776\n",
      "cbcl_q24_p                              0.273696\n",
      "cbcl_q112_p                             0.227290\n",
      "cbcl_q44_p                              0.213904\n",
      "cbcl_q92_p                              0.193566\n",
      "avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p    0.158004\n",
      "cbcl_q49_p                              0.149044\n",
      "cbcl_q46_p                              0.142740\n",
      "cbcl_q09_p                              0.136880\n",
      "cbcl_q95_p                              0.136730\n",
      "cbcl_q41_p                              0.135627\n",
      "cbcl_q58_p                              0.133149\n",
      "cbcl_q103_p                             0.131863\n",
      "cbcl_q86_p                              0.128459\n",
      "cbcl_q29_p                              0.127652\n",
      "cbcl_q102_p                             0.124129\n",
      "cbcl_q42_p                              0.112911\n",
      "cbcl_q90_p                              0.108359\n",
      "cbcl_q104_p                             0.102741\n",
      "cbcl_q71_p                              0.102356\n",
      "Name: Factor 9, dtype: float64\n",
      "Factor 10:\n",
      " cbcl_q06_p                   0.676911\n",
      "cbcl_q108_p                  0.657440\n",
      "cbcl_q107_p                  0.631127\n",
      "cbcl_q49_p                   0.401699\n",
      "cbcl_q44_p                   0.174298\n",
      "cbcl_q58_p                   0.146819\n",
      "cbcl_q56d_p                  0.140439\n",
      "cbcl_q72_p                   0.138737\n",
      "cbcl_q109_p                  0.137590\n",
      "cbcl_q24_p                   0.118506\n",
      "cbcl_q30_p                   0.117198\n",
      "avg_cbcl_q81_p_cbcl_q82_p    0.115018\n",
      "cbcl_q45_p                   0.111385\n",
      "cbcl_q43_p                   0.106638\n",
      "Name: Factor 10, dtype: float64\n",
      "Factor 11:\n",
      " avg_cbcl_q57_p_cbcl_q97_p                0.576936\n",
      "cbcl_q37_p                               0.549355\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.492618\n",
      "cbcl_q16_p                               0.491314\n",
      "cbcl_q90_p                               0.443915\n",
      "avg_cbcl_q81_p_cbcl_q82_p                0.434601\n",
      "cbcl_q15_p                               0.372845\n",
      "cbcl_q39_p                               0.320676\n",
      "cbcl_q94_p                               0.301440\n",
      "cbcl_q26_p                               0.290615\n",
      "cbcl_q67_p                               0.289189\n",
      "avg_cbcl_q22_p_cbcl_q23_p_cbcl_q28_p     0.282005\n",
      "cbcl_q68_p                               0.241901\n",
      "cbcl_q89_p                               0.234553\n",
      "cbcl_q43_p                               0.234387\n",
      "cbcl_q72_p                               0.219517\n",
      "avg_cbcl_q25_p_cbcl_q48_p                0.179372\n",
      "cbcl_q95_p                               0.170329\n",
      "cbcl_q18_p                               0.163929\n",
      "cbcl_q96_p                               0.163836\n",
      "cbcl_q66_p                               0.148634\n",
      "cbcl_q41_p                               0.143027\n",
      "cbcl_q91_p                               0.138205\n",
      "cbcl_q34_p                               0.122292\n",
      "cbcl_q61_p                               0.122257\n",
      "cbcl_q65_p                               0.121842\n",
      "cbcl_q44_p                               0.115469\n",
      "cbcl_q69_p                               0.111514\n",
      "cbcl_q46_p                               0.102239\n",
      "Name: Factor 11, dtype: float64\n",
      "Factor 12:\n",
      " avg_cbcl_q25_p_cbcl_q48_p    0.540112\n",
      "cbcl_q38_p                   0.501708\n",
      "cbcl_q30_p                   0.355991\n",
      "cbcl_q34_p                   0.354786\n",
      "cbcl_q12_p                   0.309178\n",
      "cbcl_q37_p                   0.307000\n",
      "cbcl_q111_p                  0.261602\n",
      "cbcl_q42_p                   0.228257\n",
      "cbcl_q01_p                   0.201562\n",
      "cbcl_q33_p                   0.198643\n",
      "cbcl_q35_p                   0.177473\n",
      "cbcl_q05_p                   0.171541\n",
      "cbcl_q64_p                   0.166297\n",
      "cbcl_q103_p                  0.144972\n",
      "cbcl_q11_p                   0.124176\n",
      "avg_cbcl_q57_p_cbcl_q97_p    0.112559\n",
      "cbcl_q93_p                   0.110877\n",
      "cbcl_q09_p                   0.109516\n",
      "cbcl_q16_p                   0.106656\n",
      "cbcl_q19_p                   0.103665\n",
      "Name: Factor 12, dtype: float64\n",
      "Factor 13:\n",
      " cbcl_q07_p                              0.560800\n",
      "cbcl_q74_p                              0.511327\n",
      "cbcl_q63_p                              0.433183\n",
      "cbcl_q94_p                              0.376700\n",
      "cbcl_q93_p                              0.328225\n",
      "cbcl_q39_p                              0.260033\n",
      "cbcl_q58_p                              0.256986\n",
      "cbcl_q32_p                              0.227083\n",
      "cbcl_q43_p                              0.225034\n",
      "cbcl_q83_p                              0.220361\n",
      "cbcl_q16_p                              0.217383\n",
      "cbcl_q41_p                              0.209965\n",
      "cbcl_q104_p                             0.196188\n",
      "cbcl_q03_p                              0.195047\n",
      "avg_cbcl_q22_p_cbcl_q23_p_cbcl_q28_p    0.189067\n",
      "cbcl_q44_p                              0.178633\n",
      "cbcl_q27_p                              0.171968\n",
      "cbcl_q92_p                              0.165986\n",
      "cbcl_q09_p                              0.160321\n",
      "cbcl_q85_p                              0.160095\n",
      "cbcl_q19_p                              0.152485\n",
      "cbcl_q26_p                              0.151150\n",
      "cbcl_q71_p                              0.137255\n",
      "cbcl_q31_p                              0.128718\n",
      "avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p    0.128510\n",
      "cbcl_q86_p                              0.125324\n",
      "cbcl_q69_p                              0.123710\n",
      "cbcl_q04_p                              0.120441\n",
      "cbcl_q112_p                             0.107418\n",
      "cbcl_q56e_p                             0.104758\n",
      "Name: Factor 13, dtype: float64\n",
      "Factor 14:\n",
      " cbcl_q36_p                               0.663200\n",
      "cbcl_q62_p                               0.600772\n",
      "cbcl_q92_p                               0.384184\n",
      "cbcl_q64_p                               0.261386\n",
      "cbcl_q38_p                               0.252518\n",
      "cbcl_q79_p                               0.218149\n",
      "cbcl_q58_p                               0.207581\n",
      "cbcl_q01_p                               0.192430\n",
      "cbcl_q93_p                               0.188367\n",
      "cbcl_q104_p                              0.166818\n",
      "cbcl_q14_p                               0.165527\n",
      "cbcl_q109_p                              0.156875\n",
      "cbcl_q17_p                               0.151597\n",
      "cbcl_q83_p                               0.147894\n",
      "cbcl_q47_p                               0.147167\n",
      "cbcl_q41_p                               0.138656\n",
      "cbcl_q66_p                               0.135421\n",
      "avg_cbcl_q81_p_cbcl_q82_p                0.126545\n",
      "cbcl_q71_p                               0.125869\n",
      "cbcl_q84_p                               0.119936\n",
      "cbcl_q98_p                               0.118783\n",
      "cbcl_q85_p                               0.107283\n",
      "cbcl_q56a_p                              0.107170\n",
      "cbcl_q80_p                               0.105754\n",
      "cbcl_q68_p                               0.105221\n",
      "cbcl_q44_p                               0.105175\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.102238\n",
      "cbcl_q37_p                               0.100825\n",
      "Name: Factor 14, dtype: float64\n",
      "Factor 15:\n",
      " avg_cbcl_q53_p_cbcl_q55_p    0.698567\n",
      "cbcl_q54_p                   0.613983\n",
      "cbcl_q102_p                  0.570556\n",
      "cbcl_q77_p                   0.340252\n",
      "cbcl_q89_p                   0.201638\n",
      "cbcl_q05_p                   0.196389\n",
      "cbcl_q13_p                   0.190324\n",
      "cbcl_q104_p                  0.184692\n",
      "cbcl_q51_p                   0.174982\n",
      "cbcl_q52_p                   0.162977\n",
      "cbcl_q103_p                  0.144714\n",
      "cbcl_q88_p                   0.140188\n",
      "cbcl_q93_p                   0.139091\n",
      "cbcl_q96_p                   0.137867\n",
      "cbcl_q12_p                   0.127688\n",
      "cbcl_q111_p                  0.116082\n",
      "cbcl_q62_p                   0.115492\n",
      "cbcl_q85_p                   0.112962\n",
      "cbcl_q63_p                   0.104060\n",
      "cbcl_q68_p                   0.101833\n",
      "cbcl_q80_p                   0.100797\n",
      "Name: Factor 15, dtype: float64\n",
      "Factor 16:\n",
      " cbcl_q40_p     0.757562\n",
      "cbcl_q70_p     0.751593\n",
      "cbcl_q85_p     0.323489\n",
      "cbcl_q84_p     0.277371\n",
      "cbcl_q47_p     0.261071\n",
      "cbcl_q56g_p    0.176614\n",
      "cbcl_q89_p     0.164558\n",
      "cbcl_q18_p     0.159322\n",
      "cbcl_q72_p     0.127424\n",
      "cbcl_q83_p     0.115054\n",
      "cbcl_q69_p     0.114225\n",
      "cbcl_q39_p     0.113762\n",
      "cbcl_q87_p     0.112470\n",
      "cbcl_q93_p     0.101033\n",
      "Name: Factor 16, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 17):\n",
    "    # all loading values of the factor greater than 0.1\n",
    "    factor_values = factor_loadings_df[f\"Factor {i}\"][factor_loadings_df[f\"Factor {i}\"] > 0.1]\n",
    "\n",
    "    # print(f\"Factor {i}:\\n\", factor_loadings_df[f\"Factor {i}\"].sort_values(ascending=False).head(20))\n",
    "\n",
    "    # descending order\n",
    "    print(f\"Factor {i}:\\n\", factor_values.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_columns = {}\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for i in range(1, 17):\n",
    "    # 筛选出符合条件的列名\n",
    "    factor_values = factor_loadings_df[f\"Factor {i}\"][factor_loadings_df[f\"Factor {i}\"] > 0.1]\n",
    "    # filtered_columns[f\"Factor {i}\"] = factor_values.index.tolist()  # 保存列名（或索引）\n",
    "        # 创建一个临时数据框保存因子名、列名和加载值\n",
    "    temp_df = pd.DataFrame({\n",
    "        f\"Factor {i} Variable\": factor_values.index,     # 存储列名\n",
    "        f\"Factor {i} Loading\": factor_values.values      # 存储加载值\n",
    "    })\n",
    "    \n",
    "    # 将临时数据框合并到结果数据框\n",
    "    result_df = pd.concat([result_df, temp_df], axis=1)\n",
    "\n",
    "\n",
    "# # 保存为CSV文件\n",
    "# filtered_columns_df.to_csv(\"filtered_factor_columns.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit and translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from googletrans import Translator\n",
    "import re\n",
    "import time\n",
    "\n",
    "# 指定源语言和目标语言\n",
    "translator = Translator()\n",
    "\n",
    "# 解析 element.html 文件以获取列名和详细信息\n",
    "with open(\"data/element.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# 创建一个字典来存储列名和对应的详细信息\n",
    "column_details = {}\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# 提取 cbcl_q 列名的正则表达式\n",
    "cbcl_pattern = re.compile(r\"(cbcl_q\\d+[a-z]*_p)\")\n",
    "\n",
    "for i in range(1, 17):\n",
    "    # 筛选出符合条件的加载值\n",
    "    factor_values = factor_loadings_df[f\"Factor {i}\"][factor_loadings_df[f\"Factor {i}\"] > 0.1]\n",
    "    \n",
    "    original_text = []\n",
    "    translated_text = []\n",
    "    for column_name in factor_values.index:\n",
    "        # 查找 column_name 中的所有 cbcl_q 字段\n",
    "        cbcl_items = cbcl_pattern.findall(column_name)  # 提取所有符合 cbcl_qXX_p 或 cbcl_qXXh_p 格式的子串\n",
    "\n",
    "        # 初始化存储每个 cbcl 字段详细信息的列表\n",
    "        original = []\n",
    "        details = []\n",
    "        for cbcl_item in cbcl_items:\n",
    "            # 获取每个 cbcl 字段的详细信息\n",
    "            target = soup.find(lambda tag: tag.name == \"td\" and cbcl_item in tag.get_text(strip=True))\n",
    "            if target:\n",
    "                detail_info = target.find_next(\"td\").get_text(strip=True)\n",
    "                # 保存原始详细信息\n",
    "                original.append(detail_info)\n",
    "                \n",
    "                # 翻译详细信息并添加到结果\n",
    "                try:\n",
    "                    translated_detail = translator.translate(detail_info, src='es', dest='en').text\n",
    "                except AttributeError as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    translated_detail = detail_info\n",
    "                details.append(translated_detail)\n",
    "                time.sleep(0.25)\n",
    "\n",
    "        # 将所有细节合并为单个字符串，并添加到列表中\n",
    "        original_text.append(\"; \".join(original) if original else \"N/A\")\n",
    "        translated_text.append(\"; \".join(details) if details else \"N/A\")\n",
    "\n",
    "    # 创建一个临时数据框保存因子名、列名、加载值和详细信息\n",
    "    temp_df = pd.DataFrame({\n",
    "        f\"Factor {i} Variable\": factor_values.index,  # 存储列名\n",
    "        f\"Factor {i} Loading\": factor_values.values,  # 存储加载值\n",
    "        f\"Factor {i} Detail\": original_text,  # 映射详细信息\n",
    "        f\"Factor {i} Translated_Detail\": translated_text  # 映射翻译后详细信息\n",
    "    })\n",
    "\n",
    "    # 按加载值降序排序\n",
    "    sorted_df = temp_df.sort_values(by=f\"Factor {i} Loading\", ascending=False).reset_index(drop=True)\n",
    "    # 将临时数据框合并到结果数据框\n",
    "    result_df = pd.concat([result_df.reset_index(drop=True), sorted_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 保存结果为CSV文件\n",
    "result_df.to_csv(output_dir + \"/interpretable_information_EN.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from googletrans import Translator\n",
    "import re\n",
    "import time\n",
    "\n",
    "# 指定源语言和目标语言\n",
    "translator = Translator()\n",
    "\n",
    "# 解析 element.html 文件以获取列名和详细信息\n",
    "with open(\"data/element.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# 创建一个字典来存储列名和对应的详细信息\n",
    "column_details = {}\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# 提取 cbcl_q 列名的正则表达式\n",
    "cbcl_pattern = re.compile(r\"(cbcl_q\\d+[a-z]*_p)\")\n",
    "\n",
    "for i in range(1, 17):\n",
    "    # 筛选出符合条件的加载值\n",
    "    factor_values = factor_loadings_df[f\"Factor {i}\"][factor_loadings_df[f\"Factor {i}\"] > 0.1]\n",
    "    \n",
    "    original_text = []\n",
    "    translated_text = []\n",
    "    for column_name in factor_values.index:\n",
    "        # 查找 column_name 中的所有 cbcl_q 字段\n",
    "        cbcl_items = cbcl_pattern.findall(column_name)  # 提取所有符合 cbcl_qXX_p 或 cbcl_qXXh_p 格式的子串\n",
    "\n",
    "        # 初始化存储每个 cbcl 字段详细信息的列表\n",
    "        original = []\n",
    "        details = []\n",
    "        for cbcl_item in cbcl_items:\n",
    "            # 获取每个 cbcl 字段的详细信息\n",
    "            target = soup.find(lambda tag: tag.name == \"td\" and cbcl_item in tag.get_text(strip=True))\n",
    "            if target:\n",
    "                detail_info = target.find_next(\"td\").get_text(strip=True)\n",
    "                # 保存原始详细信息\n",
    "                original.append(detail_info)\n",
    "                \n",
    "                # 翻译详细信息并添加到结果\n",
    "                try:\n",
    "                    translated_detail = translator.translate(detail_info, src='es', dest='zh-cn').text\n",
    "                except AttributeError as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    translated_detail = detail_info\n",
    "                details.append(translated_detail)\n",
    "                time.sleep(0.25)\n",
    "\n",
    "        # 将所有细节合并为单个字符串，并添加到列表中\n",
    "        original_text.append(\"; \".join(original) if original else \"N/A\")\n",
    "        translated_text.append(\"; \".join(details) if details else \"N/A\")\n",
    "\n",
    "    # 创建一个临时数据框保存因子名、列名、加载值和详细信息\n",
    "    temp_df = pd.DataFrame({\n",
    "        f\"Factor {i} Variable\": factor_values.index,  # 存储列名\n",
    "        f\"Factor {i} Loading\": factor_values.values,  # 存储加载值\n",
    "        f\"Factor {i} Detail\": original_text,  # 映射详细信息\n",
    "        f\"Factor {i} Translated_Detail\": translated_text  # 映射翻译后详细信息\n",
    "    })\n",
    "\n",
    "    # 按加载值降序排序\n",
    "    sorted_df = temp_df.sort_values(by=f\"Factor {i} Loading\", ascending=False).reset_index(drop=True)\n",
    "    # 将临时数据框合并到结果数据框\n",
    "    result_df_CN = pd.concat([result_df.reset_index(drop=True), sorted_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 保存结果为CSV文件\n",
    "result_df_CN.to_csv(output_dir + \"/interpretable_information_CN.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_pattern = re.compile(r\"Factor \\d+ Translated_Detail\")\n",
    "selected_columns = [col for col in result_df.columns if EN_pattern.match(col)]\n",
    "\n",
    "result_df[selected_columns].to_csv(output_dir + \"/details.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate labels with the factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'factor_scores_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[0;32m      3\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m combined_label_cluster \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit_predict(\u001b[43mfactor_scores_df\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor3\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'factor_scores_df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "combined_label_cluster = kmeans.fit_predict(factor_scores_df[['Factor1', 'Factor2', 'Factor3']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
