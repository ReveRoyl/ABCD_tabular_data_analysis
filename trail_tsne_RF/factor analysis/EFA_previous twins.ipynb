{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from factor_analyzer import FactorAnalyzer\n",
    "# from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"G:\\ABCD\\script/trail/trail_tsne_RF\"\n",
    "# load data and drop the first column and the subject id\n",
    "data_raw = pd.read_csv(data_path + \"/merged.csv\")\n",
    "label_columns = data_raw.columns[data_raw.columns.str.startswith(\"cbcl\")].tolist()\n",
    "\n",
    "data_raw = data_raw[[\"src_subject_id\"] + label_columns]\n",
    "\n",
    "#out put directory\n",
    "rotation = \"NA\"\n",
    "output_dir = \"G:/ABCD/script/trail/trail_tsne_RF/factor analysis/output/\" + rotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the subject id for data_raw\n",
    "data = data_raw.drop(\"src_subject_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_2816\\2767713118.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  abcd_y_lt = pd.read_csv(\"G:\\ABCD\\\\data\\\\abcd_y_lt.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1个相同值的数量: 7590\n",
      "2个相同值的数量: 1701\n",
      "3个相同值的数量: 54\n",
      "4个相同值的数量: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_2816\\2767713118.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = merged_data.groupby('rel_family_id').apply(lambda x: x.sample(1)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#read form abcd_y_lt.csv\n",
    "abcd_y_lt = pd.read_csv(\"G:\\ABCD\\\\data\\\\abcd_y_lt.csv\")\n",
    "twins = abcd_y_lt[[\"src_subject_id\", \"rel_family_id\"]]\n",
    "twins_unique = twins.drop_duplicates(subset='src_subject_id', keep='first')\n",
    "# 假设 data 是另一个包含 src_subject_id 和其他列的 DataFrame\n",
    "merged_data = pd.merge(twins_unique, data_raw, on=\"src_subject_id\", how=\"inner\")\n",
    "# 假设你的 DataFrame 叫做 df，且 rel_family_id 是你要分析的列\n",
    "value_counts = merged_data['rel_family_id'].value_counts()\n",
    "\n",
    "# 找出所有不同的重复次数\n",
    "unique_counts = value_counts.value_counts().index.sort_values()\n",
    "\n",
    "# 循环输出每个重复次数的数量\n",
    "for count in unique_counts:\n",
    "    count_of_duplicates = (value_counts == count).sum()\n",
    "    print(f\"{count}个相同值的数量: {count_of_duplicates}\")\n",
    "\n",
    "data = merged_data.groupby('rel_family_id').apply(lambda x: x.sample(1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[\"src_subject_id\", \"rel_family_id\"])\n",
    "# data.to_csv(output_dir + \"/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed columns with low frequency: ['cbcl_q02_p', 'cbcl_q73_p', 'cbcl_q99_p', 'cbcl_q101_p', 'cbcl_q105_p']\n",
      "Highly correlated pairs (r > 0.75): [('cbcl_q08_p', 'cbcl_q10_p'), ('cbcl_q08_p', 'cbcl_q78_p'), ('cbcl_q16_p', 'cbcl_q97_p'), ('cbcl_q20_p', 'cbcl_q21_p'), ('cbcl_q22_p', 'cbcl_q28_p'), ('cbcl_q23_p', 'cbcl_q28_p'), ('cbcl_q25_p', 'cbcl_q48_p'), ('cbcl_q53_p', 'cbcl_q55_p'), ('cbcl_q57_p', 'cbcl_q97_p'), ('cbcl_q81_p', 'cbcl_q82_p')]\n",
      "number of connected groups: 7\n",
      "columns for each connected group: [('cbcl_q08_p', 'cbcl_q10_p', 'cbcl_q78_p'), ('cbcl_q16_p', 'cbcl_q57_p', 'cbcl_q97_p'), ('cbcl_q20_p', 'cbcl_q21_p'), ('cbcl_q22_p', 'cbcl_q23_p', 'cbcl_q28_p'), ('cbcl_q25_p', 'cbcl_q48_p'), ('cbcl_q53_p', 'cbcl_q55_p'), ('cbcl_q81_p', 'cbcl_q82_p')]\n"
     ]
    }
   ],
   "source": [
    "# delete columns with low frequency (more than 99.5% of the values are 0)\n",
    "low_frequency_columns = data.columns[data.apply(lambda col: (col == 0).mean() > 0.995)]\n",
    "data_cleaned = data.drop(columns=low_frequency_columns)\n",
    "print(f\"Removed columns with low frequency: {low_frequency_columns.tolist()}\")\n",
    "\n",
    "#load corerlation matrix from polychoric correlation matrix\n",
    "\n",
    "correlation_matrix = pd.read_csv(data_path + \"/factor analysis/output/NA/correlation.csv\", index_col=0)\n",
    "\n",
    "# mark highly correlated pairs (r > 0.75)\n",
    "high_corr_pairs = (correlation_matrix.abs() > 0.75).where(lambda x: np.triu(x, 1)).stack().index.tolist()\n",
    "print(f\"Highly correlated pairs (r > 0.75): {high_corr_pairs}\")\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_connected_groups(pairs):\n",
    "    # 建立图结构\n",
    "    graph = defaultdict(set)\n",
    "    for col1, col2 in pairs:\n",
    "        graph[col1].add(col2)\n",
    "        graph[col2].add(col1)\n",
    "    \n",
    "    # 深度优先搜索（DFS）找到所有连通分量\n",
    "    visited = set()\n",
    "    connected_groups = []\n",
    "\n",
    "    def dfs(node, group):\n",
    "        visited.add(node)\n",
    "        group.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                dfs(neighbor, group)\n",
    "\n",
    "    # 遍历所有节点，找到每个连通分量\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            group = set()\n",
    "            dfs(node, group)\n",
    "            connected_groups.append(tuple(sorted(group)))\n",
    "\n",
    "    return connected_groups\n",
    "\n",
    "# 使用函数\n",
    "result = find_connected_groups(high_corr_pairs)\n",
    "print(\"number of connected groups:\", len(result))\n",
    "print(\"columns for each connected group:\", result)\n",
    "#create dataframe to store the final data\n",
    "data_final = data_cleaned.copy()\n",
    "for group in result:\n",
    "    # calculate the average of the columns in the group\n",
    "    data_final[f\"avg_{'_'.join(group)}\"] = data_cleaned[list(group)].mean(axis=1).round().astype(int)\n",
    "    # delete the original columns\n",
    "    data_final.drop(columns=list(group), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0574 - mae: 0.1356 - val_loss: 0.0377 - val_mae: 0.1302\n",
      "Epoch 2/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.0413 - mae: 0.1361 - val_loss: 0.0352 - val_mae: 0.1208\n",
      "Epoch 3/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.0381 - mae: 0.1266 - val_loss: 0.0336 - val_mae: 0.1161\n",
      "Epoch 4/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.0370 - mae: 0.1241 - val_loss: 0.0327 - val_mae: 0.1144\n",
      "Epoch 5/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0353 - mae: 0.1205 - val_loss: 0.0321 - val_mae: 0.1131\n",
      "Epoch 6/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.0351 - mae: 0.1197 - val_loss: 0.0317 - val_mae: 0.1124\n",
      "Epoch 7/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.0357 - mae: 0.1215 - val_loss: 0.0315 - val_mae: 0.1116\n",
      "Epoch 8/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 0.0346 - mae: 0.1192 - val_loss: 0.0313 - val_mae: 0.1111\n",
      "Epoch 9/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 0.0343 - mae: 0.1179 - val_loss: 0.0312 - val_mae: 0.1115\n",
      "Epoch 10/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.0346 - mae: 0.1192 - val_loss: 0.0311 - val_mae: 0.1113\n",
      "Epoch 11/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 0.0340 - mae: 0.1177 - val_loss: 0.0310 - val_mae: 0.1101\n",
      "Epoch 12/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 0.0341 - mae: 0.1181 - val_loss: 0.0309 - val_mae: 0.1101\n",
      "Epoch 13/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.0339 - mae: 0.1172 - val_loss: 0.0309 - val_mae: 0.1102\n",
      "Epoch 14/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 0.0338 - mae: 0.1173 - val_loss: 0.0308 - val_mae: 0.1105\n",
      "Epoch 15/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.0337 - mae: 0.1169 - val_loss: 0.0308 - val_mae: 0.1099\n",
      "Epoch 16/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 0.0338 - mae: 0.1169 - val_loss: 0.0307 - val_mae: 0.1103\n",
      "Epoch 17/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 0.0343 - mae: 0.1184 - val_loss: 0.0308 - val_mae: 0.1097\n",
      "Epoch 18/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 0.0339 - mae: 0.1173 - val_loss: 0.0307 - val_mae: 0.1094\n",
      "Epoch 19/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 0.0338 - mae: 0.1172 - val_loss: 0.0307 - val_mae: 0.1094\n",
      "Epoch 20/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0339 - mae: 0.1175 - val_loss: 0.0307 - val_mae: 0.1096\n",
      "Epoch 21/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - loss: 0.0337 - mae: 0.1169 - val_loss: 0.0307 - val_mae: 0.1106\n",
      "Epoch 22/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.0337 - mae: 0.1172 - val_loss: 0.0307 - val_mae: 0.1102\n",
      "Epoch 23/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 0.0340 - mae: 0.1176 - val_loss: 0.0307 - val_mae: 0.1102\n",
      "Epoch 24/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.0335 - mae: 0.1165 - val_loss: 0.0307 - val_mae: 0.1097\n",
      "Epoch 25/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 0.0337 - mae: 0.1170 - val_loss: 0.0306 - val_mae: 0.1099\n",
      "Epoch 26/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.0336 - mae: 0.1167 - val_loss: 0.0307 - val_mae: 0.1102\n",
      "Epoch 27/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 0.0333 - mae: 0.1159 - val_loss: 0.0307 - val_mae: 0.1097\n",
      "Epoch 28/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.0341 - mae: 0.1179 - val_loss: 0.0307 - val_mae: 0.1098\n",
      "Epoch 29/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.0334 - mae: 0.1161 - val_loss: 0.0307 - val_mae: 0.1103\n",
      "Epoch 30/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 0.0332 - mae: 0.1156 - val_loss: 0.0307 - val_mae: 0.1102\n",
      "Epoch 31/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 0.0338 - mae: 0.1176 - val_loss: 0.0306 - val_mae: 0.1097\n",
      "Epoch 32/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.0339 - mae: 0.1174 - val_loss: 0.0307 - val_mae: 0.1098\n",
      "Epoch 33/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.0335 - mae: 0.1164 - val_loss: 0.0307 - val_mae: 0.1104\n",
      "Epoch 34/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.0338 - mae: 0.1174 - val_loss: 0.0306 - val_mae: 0.1100\n",
      "Epoch 35/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 0.0337 - mae: 0.1171 - val_loss: 0.0306 - val_mae: 0.1097\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step\n",
      "        PC1       PC2       PC3       PC4       PC5\n",
      "0  0.982992  0.491465  0.632173  0.244294  0.402664\n",
      "1  0.542898  0.743577  0.986712  0.307553  0.850750\n",
      "2  1.242774  0.044213  0.724405  0.616775  0.236103\n",
      "3  0.316466  0.807533  1.160315  1.245235  0.752625\n",
      "4  0.810278  0.463693  0.728856  0.181417  0.314987\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Data Normalization\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# 2. Define a Simplified Autoencoder Structure\n",
    "input_dim = data.shape[1]\n",
    "encoding_dim = 5  # Capturing five main features\n",
    "\n",
    "# Input Layer\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# Encoding Layer\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Decoding Layer\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "# Construct Autoencoder Model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "# Encoder Model (to extract features)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# 3. Compile and Train the Model\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Use EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(data_scaled, data_scaled, epochs=100, batch_size=32, shuffle=True, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# 4. Extract Main Components\n",
    "encoded_data = encoder.predict(data_scaled)\n",
    "\n",
    "# Convert the extracted components to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=[f'PC{i+1}' for i in range(encoding_dim)])\n",
    "\n",
    "# View the extracted components\n",
    "print(encoded_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "Variance Explained: 0.3031\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 用训练好的Autoencoder对原始数据进行重构\n",
    "reconstructed_data = autoencoder.predict(data_scaled)\n",
    "\n",
    "# 计算每个特征的总方差\n",
    "total_variance = np.var(data_scaled, axis=0)\n",
    "\n",
    "# 计算重构误差 (MSE)\n",
    "reconstruction_error = mean_squared_error(data_scaled, reconstructed_data)\n",
    "\n",
    "# 计算解释的方差比例\n",
    "variance_explained_total = 1 - (reconstruction_error / np.mean(total_variance))\n",
    "\n",
    "print(f'Variance Explained: {variance_explained_total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor 1 Variance Explained: 4.46%\n",
      "Factor 2 Variance Explained: 5.89%\n",
      "Factor 3 Variance Explained: 5.91%\n",
      "Factor 4 Variance Explained: 7.37%\n",
      "Factor 5 Variance Explained: 6.69%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设你的负荷矩阵是 loading_matrix，形状为 (n_features, n_factors)\n",
    "loading_matrix = np.array(encoded_df)  # 替换为你的实际负荷矩阵\n",
    "\n",
    "# 计算每个因子的解释方差\n",
    "variance_explained = np.sum(loading_matrix ** 2, axis=0)\n",
    "\n",
    "# 将解释方差转换为百分比\n",
    "total_variance = np.sum(variance_explained)\n",
    "variance_explained_ratio = (variance_explained / total_variance) * 100\n",
    "\n",
    "# 输出每个因子的解释方差百分比\n",
    "for i, ratio in enumerate(variance_explained_ratio):\n",
    "    # print(f'Factor {i+1} Variance Explained: {ratio:.2f}%')\n",
    "    variance_explained_each = ratio*variance_explained_total\n",
    "    print(f'Factor {i+1} Variance Explained: {variance_explained_each:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设 data_scaled 是标准化或者缩放后的矩阵\n",
    "# 识别大部分元素为 0 的行（大于 95% 的元素为 0）\n",
    "threshold = 0.95  # 定义阈值\n",
    "zero_fraction = np.mean(data_scaled == 0, axis=1)  # 计算每一行中 0 的比例\n",
    "rows_to_keep = zero_fraction <= threshold  # 选择那些 0 的比例小于等于 95% 的行\n",
    "\n",
    "# 只保留非大部分为 0 的行\n",
    "data_scaled = data_scaled[rows_to_keep, :]\n",
    "\n",
    "# # 打印处理后的数据\n",
    "# print(\"处理后的数据：\")\n",
    "# print(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = pd.read_csv(data_path + \"/factor analysis/output/NA/nmf_loadings.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cbcl_q01_p</th>\n",
       "      <td>3.989038e-02</td>\n",
       "      <td>2.444018e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>1.808884e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q03_p</th>\n",
       "      <td>9.319816e-11</td>\n",
       "      <td>3.094057e-01</td>\n",
       "      <td>4.204389e-02</td>\n",
       "      <td>1.394705e-01</td>\n",
       "      <td>4.359725e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q04_p</th>\n",
       "      <td>1.392938e-02</td>\n",
       "      <td>5.225275e-02</td>\n",
       "      <td>3.524376e-02</td>\n",
       "      <td>5.949454e-02</td>\n",
       "      <td>3.363701e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q05_p</th>\n",
       "      <td>7.334232e-02</td>\n",
       "      <td>1.433246e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q06_p</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.773437e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q108_p</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>1.627950e-01</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q109_p</th>\n",
       "      <td>5.009898e-02</td>\n",
       "      <td>1.153337e-01</td>\n",
       "      <td>5.627567e-02</td>\n",
       "      <td>7.371634e-02</td>\n",
       "      <td>2.369333e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q110_p</th>\n",
       "      <td>4.994746e-03</td>\n",
       "      <td>1.533680e-03</td>\n",
       "      <td>7.240329e-04</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q111_p</th>\n",
       "      <td>6.195912e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q112_p</th>\n",
       "      <td>2.348582e-01</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.408723e-07</td>\n",
       "      <td>1.513654e-01</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       V1            V2            V3            V4  \\\n",
       "cbcl_q01_p   3.989038e-02  2.444018e-02  2.220446e-16  2.220446e-16   \n",
       "cbcl_q03_p   9.319816e-11  3.094057e-01  4.204389e-02  1.394705e-01   \n",
       "cbcl_q04_p   1.392938e-02  5.225275e-02  3.524376e-02  5.949454e-02   \n",
       "cbcl_q05_p   7.334232e-02  1.433246e-02  2.220446e-16  2.220446e-16   \n",
       "cbcl_q06_p   2.220446e-16  2.220446e-16  2.773437e-02  2.220446e-16   \n",
       "...                   ...           ...           ...           ...   \n",
       "cbcl_q108_p  2.220446e-16  2.220446e-16  1.627950e-01  2.220446e-16   \n",
       "cbcl_q109_p  5.009898e-02  1.153337e-01  5.627567e-02  7.371634e-02   \n",
       "cbcl_q110_p  4.994746e-03  1.533680e-03  7.240329e-04  2.220446e-16   \n",
       "cbcl_q111_p  6.195912e-02  2.220446e-16  2.220446e-16  2.220446e-16   \n",
       "cbcl_q112_p  2.348582e-01  2.220446e-16  2.408723e-07  1.513654e-01   \n",
       "\n",
       "                       V5  \n",
       "cbcl_q01_p   1.808884e-01  \n",
       "cbcl_q03_p   4.359725e-02  \n",
       "cbcl_q04_p   3.363701e-01  \n",
       "cbcl_q05_p   2.220446e-16  \n",
       "cbcl_q06_p   2.220446e-16  \n",
       "...                   ...  \n",
       "cbcl_q108_p  2.220446e-16  \n",
       "cbcl_q109_p  2.369333e-02  \n",
       "cbcl_q110_p  2.220446e-16  \n",
       "cbcl_q111_p  2.220446e-16  \n",
       "cbcl_q112_p  2.220446e-16  \n",
       "\n",
       "[114 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cbcl_q78_p</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>5.604418e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q08_p</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>5.040500e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q10_p</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>4.114465e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q04_p</th>\n",
       "      <td>1.392938e-02</td>\n",
       "      <td>5.225275e-02</td>\n",
       "      <td>3.524376e-02</td>\n",
       "      <td>5.949454e-02</td>\n",
       "      <td>3.363701e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q17_p</th>\n",
       "      <td>5.066473e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.520370e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q34_p</th>\n",
       "      <td>3.319961e-02</td>\n",
       "      <td>2.184172e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q77_p</th>\n",
       "      <td>3.474557e-02</td>\n",
       "      <td>3.704395e-03</td>\n",
       "      <td>1.746362e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q54_p</th>\n",
       "      <td>4.278502e-02</td>\n",
       "      <td>7.263356e-10</td>\n",
       "      <td>1.766913e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q30_p</th>\n",
       "      <td>4.513445e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbcl_q56d_p</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>6.673583e-02</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       V1            V2            V3            V4  \\\n",
       "cbcl_q78_p   2.220446e-16  2.220446e-16  2.220446e-16  2.220446e-16   \n",
       "cbcl_q08_p   2.220446e-16  2.220446e-16  2.220446e-16  2.220446e-16   \n",
       "cbcl_q10_p   2.220446e-16  2.220446e-16  2.220446e-16  2.220446e-16   \n",
       "cbcl_q04_p   1.392938e-02  5.225275e-02  3.524376e-02  5.949454e-02   \n",
       "cbcl_q17_p   5.066473e-02  2.220446e-16  2.220446e-16  2.220446e-16   \n",
       "...                   ...           ...           ...           ...   \n",
       "cbcl_q34_p   3.319961e-02  2.184172e-02  2.220446e-16  2.220446e-16   \n",
       "cbcl_q77_p   3.474557e-02  3.704395e-03  1.746362e-02  2.220446e-16   \n",
       "cbcl_q54_p   4.278502e-02  7.263356e-10  1.766913e-02  2.220446e-16   \n",
       "cbcl_q30_p   4.513445e-02  2.220446e-16  2.220446e-16  2.220446e-16   \n",
       "cbcl_q56d_p  2.220446e-16  2.220446e-16  6.673583e-02  2.220446e-16   \n",
       "\n",
       "                       V5  \n",
       "cbcl_q78_p   5.604418e-01  \n",
       "cbcl_q08_p   5.040500e-01  \n",
       "cbcl_q10_p   4.114465e-01  \n",
       "cbcl_q04_p   3.363701e-01  \n",
       "cbcl_q17_p   2.520370e-01  \n",
       "...                   ...  \n",
       "cbcl_q34_p   2.220446e-16  \n",
       "cbcl_q77_p   2.220446e-16  \n",
       "cbcl_q54_p   2.220446e-16  \n",
       "cbcl_q30_p   2.220446e-16  \n",
       "cbcl_q56d_p  2.220446e-16  \n",
       "\n",
       "[114 rows x 5 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#按照因子的V1的大小排序\n",
    "correlation_matrix.reindex(correlation_matrix['V5'].abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    df0 = correlation_matrix['V{0}'.format(i+1)].reindex(\n",
    "        correlation_matrix['V{0}'.format(i+1)].abs().sort_values(ascending=False).index\n",
    "    ).to_frame(name='V{0}'.format(i+1))\n",
    "    df0 = df0.reset_index().rename(columns={'index': 'Row_Name'})\n",
    "    df = pd.concat([df, df0], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import translate_text\n",
    "# G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=114, step=1)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, number_of_factors):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(df, number_of_factors):\n",
    "\n",
    "    # 解析 element.html 文件以获取列名和详细信息\n",
    "    with open(\"data/element.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    # 创建一个字典来存储列名和对应的详细信息\n",
    "    column_details = {}\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # 提取 cbcl_q 列名的正则表达式\n",
    "    cbcl_pattern = re.compile(r\"(cbcl_q\\d+[a-z]*_p)\")\n",
    "\n",
    "    for i in range(0, number_of_factors):\n",
    "        # 筛选出符合条件的加载值\n",
    "        # factor_values = df[f\"Factor {i}\"][df[f\"Factor {i}\"] > 0.1]\n",
    "        \n",
    "        original_text = []\n",
    "        translated_text = []\n",
    "        for column_name in df.iloc[:,i]:\n",
    "            # 查找 column_name 中的所有 cbcl_q 字段\n",
    "            cbcl_items = cbcl_pattern.findall(column_name)  # 提取所有符合 cbcl_qXX_p 或 cbcl_qXXh_p 格式的子串\n",
    "\n",
    "            # 初始化存储每个 cbcl 字段详细信息的列表\n",
    "            original = []\n",
    "            details = []\n",
    "            for cbcl_item in cbcl_items:\n",
    "                # 获取每个 cbcl 字段的详细信息\n",
    "                target = soup.find(lambda tag: tag.name == \"td\" and cbcl_item in tag.get_text(strip=True))\n",
    "                if target:\n",
    "                    detail_info = target.find_next(\"td\").get_text(strip=True)\n",
    "                    # 保存原始详细信息\n",
    "                    original.append(detail_info)\n",
    "                    \n",
    "                    # 翻译详细信息并添加到结果\n",
    "                    try:\n",
    "                        translated_detail = GoogleTranslator(source='es', target='zh-CN').translate(detail_info)\n",
    "                    except AttributeError as e:\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "                        translated_detail = detail_info\n",
    "                    details.append(translated_detail)\n",
    "                    time.sleep(0.25)\n",
    "\n",
    "            # 将所有细节合并为单个字符串，并添加到列表中\n",
    "            original_text.append(\"; \".join(original) if original else \"N/A\")\n",
    "            translated_text.append(\"; \".join(details) if details else \"N/A\")\n",
    "        # 创建一个临时数据框保存因子名、列名、加载值和详细信息\n",
    "        temp_df = pd.DataFrame({\n",
    "            # f\"Factor {i} Variable\": factor_values.index,  # 存储列名\n",
    "            # f\"Factor {i} Loading\": factor_values.values,  # 存储加载值\n",
    "            f\"Factor {i} Detail\": original_text,  # 映射详细信息\n",
    "            f\"Factor {i} Translated_Detail\": translated_text  # 映射翻译后详细信息\n",
    "        })\n",
    "\n",
    "        # 按加载值降序排序\n",
    "        # sorted_df = temp_df.sort_values(by=f\"Factor {i} Loading\", ascending=False).reset_index(drop=True)\n",
    "        # 将临时数据框合并到结果数据框\n",
    "        result_df = pd.concat([result_df.reset_index(drop=True), temp_df.reset_index(drop=True)], axis=1)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[307], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m             translated_detail \u001b[38;5;241m=\u001b[39m detail_info\n\u001b[0;32m     41\u001b[0m         details\u001b[38;5;241m.\u001b[39mappend(translated_detail)\n\u001b[1;32m---> 42\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 将所有细节合并为单个字符串，并添加到列表中\u001b[39;00m\n\u001b[0;32m     45\u001b[0m original_text\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(original) \u001b[38;5;28;01mif\u001b[39;00m original \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = df.iloc[:, [0]]\n",
    "number_of_factors =1 \n",
    "# 解析 element.html 文件以获取列名和详细信息\n",
    "with open(\"data/element.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# 创建一个字典来存储列名和对应的详细信息\n",
    "column_details = {}\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# 提取 cbcl_q 列名的正则表达式\n",
    "cbcl_pattern = re.compile(r\"(cbcl_q\\d+[a-z]*_p)\")\n",
    "\n",
    "for i in range(0, number_of_factors + 1):\n",
    "    # 筛选出符合条件的加载值\n",
    "    # factor_values = df[f\"Factor {i}\"][df[f\"Factor {i}\"] > 0.1]\n",
    "    \n",
    "    original_text = []\n",
    "    translated_text = []\n",
    "    for column_name in df.iloc[:,i]:\n",
    "        # 查找 column_name 中的所有 cbcl_q 字段\n",
    "        cbcl_items = cbcl_pattern.findall(column_name)  # 提取所有符合 cbcl_qXX_p 或 cbcl_qXXh_p 格式的子串\n",
    "\n",
    "        # 初始化存储每个 cbcl 字段详细信息的列表\n",
    "        original = []\n",
    "        details = []\n",
    "        for cbcl_item in cbcl_items:\n",
    "            # 获取每个 cbcl 字段的详细信息\n",
    "            target = soup.find(lambda tag: tag.name == \"td\" and cbcl_item in tag.get_text(strip=True))\n",
    "            if target:\n",
    "                detail_info = target.find_next(\"td\").get_text(strip=True)\n",
    "                # 保存原始详细信息\n",
    "                original.append(detail_info)\n",
    "                \n",
    "                # 翻译详细信息并添加到结果\n",
    "                try:\n",
    "                    translated_detail = GoogleTranslator(source='es', target='zh-CN').translate(detail_info)\n",
    "                except AttributeError as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    translated_detail = detail_info\n",
    "                details.append(translated_detail)\n",
    "                time.sleep(0.25)\n",
    "\n",
    "        # 将所有细节合并为单个字符串，并添加到列表中\n",
    "        original_text.append(\"; \".join(original) if original else \"N/A\")\n",
    "        translated_text.append(\"; \".join(details) if details else \"N/A\")\n",
    "    # 创建一个临时数据框保存因子名、列名、加载值和详细信息\n",
    "    temp_df = pd.DataFrame({\n",
    "        # f\"Factor {i} Variable\": factor_values.index,  # 存储列名\n",
    "        # f\"Factor {i} Loading\": factor_values.values,  # 存储加载值\n",
    "        f\"Factor {i} Detail\": original_text,  # 映射详细信息\n",
    "        f\"Factor {i} Translated_Detail\": translated_text  # 映射翻译后详细信息\n",
    "    })\n",
    "\n",
    "    # 按加载值降序排序\n",
    "    # sorted_df = temp_df.sort_values(by=f\"Factor {i} Loading\", ascending=False).reset_index(drop=True)\n",
    "    # 将临时数据框合并到结果数据框\n",
    "    result_df = pd.concat([result_df.reset_index(drop=True), temp_df.reset_index(drop=True)], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>Row_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>cbcl_q22_p</td>\n",
       "      <td>cbcl_q56f_p</td>\n",
       "      <td>cbcl_q44_p</td>\n",
       "      <td>cbcl_q78_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>cbcl_q03_p</td>\n",
       "      <td>cbcl_q56b_p</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbcl_q50_p</td>\n",
       "      <td>cbcl_q95_p</td>\n",
       "      <td>cbcl_q56e_p</td>\n",
       "      <td>cbcl_q58_p</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cbcl_q45_p</td>\n",
       "      <td>cbcl_q28_p</td>\n",
       "      <td>cbcl_q56a_p</td>\n",
       "      <td>cbcl_q55_p</td>\n",
       "      <td>cbcl_q04_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cbcl_q42_p</td>\n",
       "      <td>cbcl_q86_p</td>\n",
       "      <td>cbcl_q49_p</td>\n",
       "      <td>cbcl_q32_p</td>\n",
       "      <td>cbcl_q17_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>cbcl_q06_p</td>\n",
       "      <td>cbcl_q92_p</td>\n",
       "      <td>cbcl_q38_p</td>\n",
       "      <td>cbcl_q77_p</td>\n",
       "      <td>cbcl_q34_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>cbcl_q81_p</td>\n",
       "      <td>cbcl_q98_p</td>\n",
       "      <td>cbcl_q59_p</td>\n",
       "      <td>cbcl_q01_p</td>\n",
       "      <td>cbcl_q77_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>cbcl_q72_p</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>cbcl_q63_p</td>\n",
       "      <td>cbcl_q54_p</td>\n",
       "      <td>cbcl_q54_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>cbcl_q79_p</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>cbcl_q18_p</td>\n",
       "      <td>cbcl_q30_p</td>\n",
       "      <td>cbcl_q30_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Row_Name     Row_Name     Row_Name     Row_Name     Row_Name\n",
       "0     cbcl_q71_p   cbcl_q22_p  cbcl_q56f_p   cbcl_q44_p   cbcl_q78_p\n",
       "1    cbcl_q112_p   cbcl_q03_p  cbcl_q56b_p   cbcl_q71_p   cbcl_q08_p\n",
       "2     cbcl_q50_p   cbcl_q95_p  cbcl_q56e_p   cbcl_q58_p   cbcl_q10_p\n",
       "3     cbcl_q45_p   cbcl_q28_p  cbcl_q56a_p   cbcl_q55_p   cbcl_q04_p\n",
       "4     cbcl_q42_p   cbcl_q86_p   cbcl_q49_p   cbcl_q32_p   cbcl_q17_p\n",
       "..           ...          ...          ...          ...          ...\n",
       "109   cbcl_q06_p   cbcl_q92_p   cbcl_q38_p   cbcl_q77_p   cbcl_q34_p\n",
       "110   cbcl_q81_p   cbcl_q98_p   cbcl_q59_p   cbcl_q01_p   cbcl_q77_p\n",
       "111   cbcl_q72_p   cbcl_q10_p   cbcl_q63_p   cbcl_q54_p   cbcl_q54_p\n",
       "112   cbcl_q79_p   cbcl_q08_p   cbcl_q18_p   cbcl_q30_p   cbcl_q30_p\n",
       "113  cbcl_q56d_p  cbcl_q56d_p   cbcl_q71_p  cbcl_q56d_p  cbcl_q56d_p\n",
       "\n",
       "[114 rows x 5 columns]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Row_Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V1</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V2</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V3</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V4</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V5</th>\n",
       "      <th>CBCL_Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>2.520958e-01</td>\n",
       "      <td>cbcl_q22_p</td>\n",
       "      <td>3.304722e-01</td>\n",
       "      <td>cbcl_q56f_p</td>\n",
       "      <td>3.309346e-01</td>\n",
       "      <td>cbcl_q44_p</td>\n",
       "      <td>5.121333e-01</td>\n",
       "      <td>cbcl_q78_p</td>\n",
       "      <td>5.604418e-01</td>\n",
       "      <td>Self-conscious or easily embarrassed Se cohíbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>2.348582e-01</td>\n",
       "      <td>cbcl_q03_p</td>\n",
       "      <td>3.094057e-01</td>\n",
       "      <td>cbcl_q56b_p</td>\n",
       "      <td>3.289139e-01</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>2.670481e-01</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>5.040500e-01</td>\n",
       "      <td>Worries Se preocupa mucho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbcl_q50_p</td>\n",
       "      <td>2.283924e-01</td>\n",
       "      <td>cbcl_q95_p</td>\n",
       "      <td>2.751811e-01</td>\n",
       "      <td>cbcl_q56e_p</td>\n",
       "      <td>2.386312e-01</td>\n",
       "      <td>cbcl_q58_p</td>\n",
       "      <td>1.927971e-01</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>4.114465e-01</td>\n",
       "      <td>Too fearful or anxious Demasiado ansioso(a) o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cbcl_q45_p</td>\n",
       "      <td>1.692854e-01</td>\n",
       "      <td>cbcl_q28_p</td>\n",
       "      <td>2.482104e-01</td>\n",
       "      <td>cbcl_q56a_p</td>\n",
       "      <td>2.303975e-01</td>\n",
       "      <td>cbcl_q55_p</td>\n",
       "      <td>1.913884e-01</td>\n",
       "      <td>cbcl_q04_p</td>\n",
       "      <td>3.363701e-01</td>\n",
       "      <td>Nervous, highstrung, or tense Nervioso(a), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cbcl_q42_p</td>\n",
       "      <td>1.686497e-01</td>\n",
       "      <td>cbcl_q86_p</td>\n",
       "      <td>2.086195e-01</td>\n",
       "      <td>cbcl_q49_p</td>\n",
       "      <td>1.911072e-01</td>\n",
       "      <td>cbcl_q32_p</td>\n",
       "      <td>1.698548e-01</td>\n",
       "      <td>cbcl_q17_p</td>\n",
       "      <td>2.520370e-01</td>\n",
       "      <td>Would rather be alone than with others Prefier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>cbcl_q06_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q92_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q38_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q77_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q34_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Bowel movements outside toilet Hace sus necesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>cbcl_q81_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q98_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q59_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q01_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q77_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Steals at home Roba en casa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>cbcl_q72_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q63_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q54_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q54_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Sets fires Prende fuegos/inicia incendios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>cbcl_q79_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q18_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q30_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q30_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Speech problem Problemas con el habla (describa)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Problems with eyes (not if corrected by glasse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Row_Name            V1     Row_Name            V2     Row_Name  \\\n",
       "0     cbcl_q71_p  2.520958e-01   cbcl_q22_p  3.304722e-01  cbcl_q56f_p   \n",
       "1    cbcl_q112_p  2.348582e-01   cbcl_q03_p  3.094057e-01  cbcl_q56b_p   \n",
       "2     cbcl_q50_p  2.283924e-01   cbcl_q95_p  2.751811e-01  cbcl_q56e_p   \n",
       "3     cbcl_q45_p  1.692854e-01   cbcl_q28_p  2.482104e-01  cbcl_q56a_p   \n",
       "4     cbcl_q42_p  1.686497e-01   cbcl_q86_p  2.086195e-01   cbcl_q49_p   \n",
       "..           ...           ...          ...           ...          ...   \n",
       "109   cbcl_q06_p  2.220446e-16   cbcl_q92_p  2.220446e-16   cbcl_q38_p   \n",
       "110   cbcl_q81_p  2.220446e-16   cbcl_q98_p  2.220446e-16   cbcl_q59_p   \n",
       "111   cbcl_q72_p  2.220446e-16   cbcl_q10_p  2.220446e-16   cbcl_q63_p   \n",
       "112   cbcl_q79_p  2.220446e-16   cbcl_q08_p  2.220446e-16   cbcl_q18_p   \n",
       "113  cbcl_q56d_p  2.220446e-16  cbcl_q56d_p  2.220446e-16   cbcl_q71_p   \n",
       "\n",
       "               V3     Row_Name            V4     Row_Name            V5  \\\n",
       "0    3.309346e-01   cbcl_q44_p  5.121333e-01   cbcl_q78_p  5.604418e-01   \n",
       "1    3.289139e-01   cbcl_q71_p  2.670481e-01   cbcl_q08_p  5.040500e-01   \n",
       "2    2.386312e-01   cbcl_q58_p  1.927971e-01   cbcl_q10_p  4.114465e-01   \n",
       "3    2.303975e-01   cbcl_q55_p  1.913884e-01   cbcl_q04_p  3.363701e-01   \n",
       "4    1.911072e-01   cbcl_q32_p  1.698548e-01   cbcl_q17_p  2.520370e-01   \n",
       "..            ...          ...           ...          ...           ...   \n",
       "109  2.220446e-16   cbcl_q77_p  2.220446e-16   cbcl_q34_p  2.220446e-16   \n",
       "110  2.220446e-16   cbcl_q01_p  2.220446e-16   cbcl_q77_p  2.220446e-16   \n",
       "111  2.220446e-16   cbcl_q54_p  2.220446e-16   cbcl_q54_p  2.220446e-16   \n",
       "112  2.220446e-16   cbcl_q30_p  2.220446e-16   cbcl_q30_p  2.220446e-16   \n",
       "113  2.220446e-16  cbcl_q56d_p  2.220446e-16  cbcl_q56d_p  2.220446e-16   \n",
       "\n",
       "                                          CBCL_Details  \n",
       "0    Self-conscious or easily embarrassed Se cohíbe...  \n",
       "1                            Worries Se preocupa mucho  \n",
       "2    Too fearful or anxious Demasiado ansioso(a) o ...  \n",
       "3    Nervous, highstrung, or tense Nervioso(a), ten...  \n",
       "4    Would rather be alone than with others Prefier...  \n",
       "..                                                 ...  \n",
       "109  Bowel movements outside toilet Hace sus necesi...  \n",
       "110                        Steals at home Roba en casa  \n",
       "111          Sets fires Prende fuegos/inicia incendios  \n",
       "112   Speech problem Problemas con el habla (describa)  \n",
       "113  Problems with eyes (not if corrected by glasse...  \n",
       "\n",
       "[114 rows x 11 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbcl_q71_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbcl_q112_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbcl_q50_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cbcl_q45_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cbcl_q42_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>cbcl_q06_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>cbcl_q81_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>cbcl_q72_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>cbcl_q79_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Row_Name\n",
       "0     cbcl_q71_p\n",
       "1    cbcl_q112_p\n",
       "2     cbcl_q50_p\n",
       "3     cbcl_q45_p\n",
       "4     cbcl_q42_p\n",
       "..           ...\n",
       "109   cbcl_q06_p\n",
       "110   cbcl_q81_p\n",
       "111   cbcl_q72_p\n",
       "112   cbcl_q79_p\n",
       "113  cbcl_q56d_p\n",
       "\n",
       "[114 rows x 1 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor 0 Detail</th>\n",
       "      <th>Factor 0 Translated_Detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Self-conscious or easily embarrassed Se cohíbe...</td>\n",
       "      <td>自我意识或容易感到尴尬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worries Se preocupa mucho</td>\n",
       "      <td>担心 他非常担心</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Too fearful or anxious Demasiado ansioso(a) o ...</td>\n",
       "      <td>过于恐惧或焦虑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nervous, highstrung, or tense Nervioso(a), ten...</td>\n",
       "      <td>紧张、高度紧张或紧张</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would rather be alone than with others Prefier...</td>\n",
       "      <td>宁愿独自一人也不愿与他人在一起</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Bowel movements outside toilet Hace sus necesi...</td>\n",
       "      <td>在厕所外排便 他在衣服上或在不适当的地方大小便</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Steals at home Roba en casa</td>\n",
       "      <td>在家偷窃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Sets fires Prende fuegos/inicia incendios</td>\n",
       "      <td>放火 点火/点火</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Speech problem Problemas con el habla (describa)</td>\n",
       "      <td>言语问题 言语问题（描述）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Problems with eyes (not if corrected by glasse...</td>\n",
       "      <td>眼睛问题（如果戴眼镜矫正则不会）\\n            眼镜/镜片/眼镜）</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Factor 0 Detail  \\\n",
       "0    Self-conscious or easily embarrassed Se cohíbe...   \n",
       "1                            Worries Se preocupa mucho   \n",
       "2    Too fearful or anxious Demasiado ansioso(a) o ...   \n",
       "3    Nervous, highstrung, or tense Nervioso(a), ten...   \n",
       "4    Would rather be alone than with others Prefier...   \n",
       "..                                                 ...   \n",
       "109  Bowel movements outside toilet Hace sus necesi...   \n",
       "110                        Steals at home Roba en casa   \n",
       "111          Sets fires Prende fuegos/inicia incendios   \n",
       "112   Speech problem Problemas con el habla (describa)   \n",
       "113  Problems with eyes (not if corrected by glasse...   \n",
       "\n",
       "                  Factor 0 Translated_Detail  \n",
       "0                                自我意识或容易感到尴尬  \n",
       "1                                   担心 他非常担心  \n",
       "2                                    过于恐惧或焦虑  \n",
       "3                                 紧张、高度紧张或紧张  \n",
       "4                            宁愿独自一人也不愿与他人在一起  \n",
       "..                                       ...  \n",
       "109                  在厕所外排便 他在衣服上或在不适当的地方大小便  \n",
       "110                                     在家偷窃  \n",
       "111                                 放火 点火/点火  \n",
       "112                            言语问题 言语问题（描述）  \n",
       "113  眼睛问题（如果戴眼镜矫正则不会）\\n            眼镜/镜片/眼镜）  \n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = translate_text(data.iloc[:, [0]] , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[279], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 翻译详细信息并添加到结果\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     translated_detail \u001b[38;5;241m=\u001b[39m \u001b[43mGoogleTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzh-CN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetail_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\deep_translator\\google.py:67\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpayload_key:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpayload_key] \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m---> 67\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    794\u001b[0m     conn,\n\u001b[0;32m    795\u001b[0m     method,\n\u001b[0;32m    796\u001b[0m     url,\n\u001b[0;32m    797\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    798\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    799\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    800\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    801\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    802\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    803\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    804\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    806\u001b[0m )\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\urllib3\\connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\urllib3\\connection.py:653\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[0;32m    651\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 653\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[0;32m    673\u001b[0m \u001b[38;5;66;03m# Forwarding proxies can never have a verified target since\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;66;03m# the proxy is the one doing the verification. Should instead\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# use a CONNECT tunnel in order to verify the target.\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;66;03m# See: https://github.com/urllib3/urllib3/issues/3267.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\urllib3\\connection.py:806\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[0;32m    804\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 806\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\urllib3\\util\\ssl_.py:440\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "number_of_factors = 5\n",
    "# 解析 element.html 文件以获取列名和详细信息\n",
    "with open(\"data/element.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# 创建一个字典来存储列名和对应的详细信息\n",
    "column_details = {}\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# 提取 cbcl_q 列名的正则表达式\n",
    "cbcl_pattern = re.compile(r\"(cbcl_q\\d+[a-z]*_p)\")\n",
    "\n",
    "for i in range(1, number_of_factors + 1):\n",
    "    # 筛选出符合条件的加载值\n",
    "    # factor_values = df[f\"Factor {i}\"][df[f\"Factor {i}\"] > 0.1]\n",
    "    \n",
    "    original_text = []\n",
    "    translated_text = []\n",
    "    for column_name in df.iloc[:,i]:\n",
    "        # 查找 column_name 中的所有 cbcl_q 字段\n",
    "        cbcl_items = cbcl_pattern.findall(column_name)  # 提取所有符合 cbcl_qXX_p 或 cbcl_qXXh_p 格式的子串\n",
    "\n",
    "        # 初始化存储每个 cbcl 字段详细信息的列表\n",
    "        original = []\n",
    "        details = []\n",
    "        for cbcl_item in cbcl_items:\n",
    "            # 获取每个 cbcl 字段的详细信息\n",
    "            target = soup.find(lambda tag: tag.name == \"td\" and cbcl_item in tag.get_text(strip=True))\n",
    "            if target:\n",
    "                detail_info = target.find_next(\"td\").get_text(strip=True)\n",
    "                # 保存原始详细信息\n",
    "                original.append(detail_info)\n",
    "                \n",
    "                # 翻译详细信息并添加到结果\n",
    "                try:\n",
    "                    translated_detail = GoogleTranslator(source='es', target='zh-CN').translate(detail_info)\n",
    "                except AttributeError as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    translated_detail = detail_info\n",
    "                details.append(translated_detail)\n",
    "                time.sleep(0.25)\n",
    "\n",
    "        # 将所有细节合并为单个字符串，并添加到列表中\n",
    "        original_text.append(\"; \".join(original) if original else \"N/A\")\n",
    "        translated_text.append(\"; \".join(details) if details else \"N/A\")\n",
    "\n",
    "    temp_df = pd.DataFrame({\n",
    "        # f\"Factor {i} Variable\": factor_values.index,  # 存储列名\n",
    "        # f\"Factor {i} Loading\": factor_values.values,  # 存储加载值\n",
    "        f\"Factor {i} Detail\": original_text,  # 映射详细信息\n",
    "        f\"Factor {i} Translated_Detail\": translated_text  # 映射翻译后详细信息\n",
    "    })\n",
    "\n",
    "    # 按加载值降序排序\n",
    "    # sorted_df = temp_df.sort_values(by=f\"Factor {i} Loading\", ascending=False).reset_index(drop=True)\n",
    "    # 将临时数据框合并到结果数据框\n",
    "    result_df = pd.concat([result_df.reset_index(drop=True), temp_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor 1 Detail</th>\n",
       "      <th>Factor 1 Translated_Detail</th>\n",
       "      <th>Factor 2 Detail</th>\n",
       "      <th>Factor 2 Translated_Detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disobedient at home Desobedece en casa</td>\n",
       "      <td>在家不听话</td>\n",
       "      <td>Stomachaches Dolores de estómago</td>\n",
       "      <td>胃痛 胃痛</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Argues a lot Discute mucho</td>\n",
       "      <td>争论很多 争论很多</td>\n",
       "      <td>Headaches Dolores de cabeza</td>\n",
       "      <td>头痛 头痛</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Temper tantrums or hot temper Le dan rabietas ...</td>\n",
       "      <td>发脾气或脾气暴躁 他发脾气或脾气不好</td>\n",
       "      <td>Rashes or other skin problems Salpullido o irr...</td>\n",
       "      <td>皮疹或其他皮肤问题</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breaks rules at home, school or elsewhere No r...</td>\n",
       "      <td>违反家庭、学校或其他地方的规则 不尊重/违反家庭、学校或其他地方的规则\\n         ...</td>\n",
       "      <td>Aches or pains (not stomach or headaches) Dolo...</td>\n",
       "      <td>疼痛（不是胃痛或头痛）\\n            头）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stubborn, sullen, or irritable Obstinado(a), m...</td>\n",
       "      <td>固执、闷闷不乐或易怒</td>\n",
       "      <td>Constipated, doesn't move bowels Padece de est...</td>\n",
       "      <td>便秘，拉不动大便</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Talks or walks in sleep Habla o camina cuando ...</td>\n",
       "      <td>睡觉时说话或走路 睡觉时说话或走路</td>\n",
       "      <td>Gets teased a lot Los demás se burlan de él/el...</td>\n",
       "      <td>经常被取笑其他人经常取笑他/她</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Thumb-sucking Se chupa el dedo</td>\n",
       "      <td>吮吸拇指</td>\n",
       "      <td>Plays with own sex parts in public Se toca/jue...</td>\n",
       "      <td>在公共场合玩弄自己的性器官</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Can't sit still, restless, or hyperactive No p...</td>\n",
       "      <td>无法静坐、焦躁不安或过度活跃焦躁不安或过度活跃</td>\n",
       "      <td>Prefers being with older kids Prefiere estar c...</td>\n",
       "      <td>喜欢和年龄较大的孩子在一起 喜欢和比他/她大的孩子在一起</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Can't concentrate, can't pay attention for lon...</td>\n",
       "      <td>无法集中注意力，无法长时间集中注意力</td>\n",
       "      <td>Deliberately harms self or attempts suicide Se...</td>\n",
       "      <td>故意伤害自己或企图自杀\\n            自杀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Problems with eyes (not if corrected by glasse...</td>\n",
       "      <td>眼睛问题（如果戴眼镜矫正则不会）\\n            眼镜/镜片/眼镜）</td>\n",
       "      <td>Self-conscious or easily embarrassed Se cohíbe...</td>\n",
       "      <td>自我意识或容易感到尴尬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Factor 1 Detail  \\\n",
       "0               Disobedient at home Desobedece en casa   \n",
       "1                           Argues a lot Discute mucho   \n",
       "2    Temper tantrums or hot temper Le dan rabietas ...   \n",
       "3    Breaks rules at home, school or elsewhere No r...   \n",
       "4    Stubborn, sullen, or irritable Obstinado(a), m...   \n",
       "..                                                 ...   \n",
       "109  Talks or walks in sleep Habla o camina cuando ...   \n",
       "110                     Thumb-sucking Se chupa el dedo   \n",
       "111  Can't sit still, restless, or hyperactive No p...   \n",
       "112  Can't concentrate, can't pay attention for lon...   \n",
       "113  Problems with eyes (not if corrected by glasse...   \n",
       "\n",
       "                            Factor 1 Translated_Detail  \\\n",
       "0                                                在家不听话   \n",
       "1                                            争论很多 争论很多   \n",
       "2                                   发脾气或脾气暴躁 他发脾气或脾气不好   \n",
       "3    违反家庭、学校或其他地方的规则 不尊重/违反家庭、学校或其他地方的规则\\n         ...   \n",
       "4                                           固执、闷闷不乐或易怒   \n",
       "..                                                 ...   \n",
       "109                                  睡觉时说话或走路 睡觉时说话或走路   \n",
       "110                                               吮吸拇指   \n",
       "111                            无法静坐、焦躁不安或过度活跃焦躁不安或过度活跃   \n",
       "112                                 无法集中注意力，无法长时间集中注意力   \n",
       "113            眼睛问题（如果戴眼镜矫正则不会）\\n            眼镜/镜片/眼镜）   \n",
       "\n",
       "                                       Factor 2 Detail  \\\n",
       "0                     Stomachaches Dolores de estómago   \n",
       "1                          Headaches Dolores de cabeza   \n",
       "2    Rashes or other skin problems Salpullido o irr...   \n",
       "3    Aches or pains (not stomach or headaches) Dolo...   \n",
       "4    Constipated, doesn't move bowels Padece de est...   \n",
       "..                                                 ...   \n",
       "109  Gets teased a lot Los demás se burlan de él/el...   \n",
       "110  Plays with own sex parts in public Se toca/jue...   \n",
       "111  Prefers being with older kids Prefiere estar c...   \n",
       "112  Deliberately harms self or attempts suicide Se...   \n",
       "113  Self-conscious or easily embarrassed Se cohíbe...   \n",
       "\n",
       "       Factor 2 Translated_Detail  \n",
       "0                           胃痛 胃痛  \n",
       "1                           头痛 头痛  \n",
       "2                       皮疹或其他皮肤问题  \n",
       "3     疼痛（不是胃痛或头痛）\\n            头）  \n",
       "4                        便秘，拉不动大便  \n",
       "..                            ...  \n",
       "109               经常被取笑其他人经常取笑他/她  \n",
       "110                 在公共场合玩弄自己的性器官  \n",
       "111  喜欢和年龄较大的孩子在一起 喜欢和比他/她大的孩子在一起  \n",
       "112   故意伤害自己或企图自杀\\n            自杀  \n",
       "113                   自我意识或容易感到尴尬  \n",
       "\n",
       "[114 rows x 4 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = []\n",
    "details = []\n",
    "for items in  df.iloc[:,i]:\n",
    "    target = soup.find(lambda tag: tag.name == \"td\" and items in tag.get_text(strip=True))\n",
    "    if target:\n",
    "        detail_info = target.find_next(\"td\").get_text(strip=True)\n",
    "        original.append(detail_info)\n",
    "        translated_detail = GoogleTranslator(source='es', target='zh-CN').translate(detail_info)\n",
    "        translated_text.append(translated_detail)\n",
    "        time.sleep(0.25)\n",
    "original_text.append(\"; \".join(original) if original else \"N/A\")\n",
    "translated_text.append(\"; \".join(details) if details else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      cbcl_q56f_p\n",
       "1      cbcl_q56b_p\n",
       "2      cbcl_q56e_p\n",
       "3      cbcl_q56a_p\n",
       "4       cbcl_q49_p\n",
       "          ...     \n",
       "109     cbcl_q38_p\n",
       "110     cbcl_q59_p\n",
       "111     cbcl_q63_p\n",
       "112     cbcl_q18_p\n",
       "113     cbcl_q71_p\n",
       "Name: Row_Name, Length: 114, dtype: object"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V1</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V2</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V3</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V4</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V5</th>\n",
       "      <th>CBCL_Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>2.520958e-01</td>\n",
       "      <td>cbcl_q22_p</td>\n",
       "      <td>3.304722e-01</td>\n",
       "      <td>cbcl_q56f_p</td>\n",
       "      <td>3.309346e-01</td>\n",
       "      <td>cbcl_q44_p</td>\n",
       "      <td>5.121333e-01</td>\n",
       "      <td>cbcl_q78_p</td>\n",
       "      <td>5.604418e-01</td>\n",
       "      <td>Self-conscious or easily embarrassed Se cohíbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>2.348582e-01</td>\n",
       "      <td>cbcl_q03_p</td>\n",
       "      <td>3.094057e-01</td>\n",
       "      <td>cbcl_q56b_p</td>\n",
       "      <td>3.289139e-01</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>2.670481e-01</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>5.040500e-01</td>\n",
       "      <td>Worries Se preocupa mucho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbcl_q50_p</td>\n",
       "      <td>2.283924e-01</td>\n",
       "      <td>cbcl_q95_p</td>\n",
       "      <td>2.751811e-01</td>\n",
       "      <td>cbcl_q56e_p</td>\n",
       "      <td>2.386312e-01</td>\n",
       "      <td>cbcl_q58_p</td>\n",
       "      <td>1.927971e-01</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>4.114465e-01</td>\n",
       "      <td>Too fearful or anxious Demasiado ansioso(a) o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cbcl_q45_p</td>\n",
       "      <td>1.692854e-01</td>\n",
       "      <td>cbcl_q28_p</td>\n",
       "      <td>2.482104e-01</td>\n",
       "      <td>cbcl_q56a_p</td>\n",
       "      <td>2.303975e-01</td>\n",
       "      <td>cbcl_q55_p</td>\n",
       "      <td>1.913884e-01</td>\n",
       "      <td>cbcl_q04_p</td>\n",
       "      <td>3.363701e-01</td>\n",
       "      <td>Nervous, highstrung, or tense Nervioso(a), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cbcl_q42_p</td>\n",
       "      <td>1.686497e-01</td>\n",
       "      <td>cbcl_q86_p</td>\n",
       "      <td>2.086195e-01</td>\n",
       "      <td>cbcl_q49_p</td>\n",
       "      <td>1.911072e-01</td>\n",
       "      <td>cbcl_q32_p</td>\n",
       "      <td>1.698548e-01</td>\n",
       "      <td>cbcl_q17_p</td>\n",
       "      <td>2.520370e-01</td>\n",
       "      <td>Would rather be alone than with others Prefier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>cbcl_q06_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q92_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q38_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q77_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q34_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Bowel movements outside toilet Hace sus necesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>cbcl_q81_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q98_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q59_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q01_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q77_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Steals at home Roba en casa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>cbcl_q72_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q63_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q54_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q54_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Sets fires Prende fuegos/inicia incendios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>cbcl_q79_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q18_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q30_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q30_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Speech problem Problemas con el habla (describa)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>Problems with eyes (not if corrected by glasse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Row_Name            V1     Row_Name            V2     Row_Name  \\\n",
       "0     cbcl_q71_p  2.520958e-01   cbcl_q22_p  3.304722e-01  cbcl_q56f_p   \n",
       "1    cbcl_q112_p  2.348582e-01   cbcl_q03_p  3.094057e-01  cbcl_q56b_p   \n",
       "2     cbcl_q50_p  2.283924e-01   cbcl_q95_p  2.751811e-01  cbcl_q56e_p   \n",
       "3     cbcl_q45_p  1.692854e-01   cbcl_q28_p  2.482104e-01  cbcl_q56a_p   \n",
       "4     cbcl_q42_p  1.686497e-01   cbcl_q86_p  2.086195e-01   cbcl_q49_p   \n",
       "..           ...           ...          ...           ...          ...   \n",
       "109   cbcl_q06_p  2.220446e-16   cbcl_q92_p  2.220446e-16   cbcl_q38_p   \n",
       "110   cbcl_q81_p  2.220446e-16   cbcl_q98_p  2.220446e-16   cbcl_q59_p   \n",
       "111   cbcl_q72_p  2.220446e-16   cbcl_q10_p  2.220446e-16   cbcl_q63_p   \n",
       "112   cbcl_q79_p  2.220446e-16   cbcl_q08_p  2.220446e-16   cbcl_q18_p   \n",
       "113  cbcl_q56d_p  2.220446e-16  cbcl_q56d_p  2.220446e-16   cbcl_q71_p   \n",
       "\n",
       "               V3     Row_Name            V4     Row_Name            V5  \\\n",
       "0    3.309346e-01   cbcl_q44_p  5.121333e-01   cbcl_q78_p  5.604418e-01   \n",
       "1    3.289139e-01   cbcl_q71_p  2.670481e-01   cbcl_q08_p  5.040500e-01   \n",
       "2    2.386312e-01   cbcl_q58_p  1.927971e-01   cbcl_q10_p  4.114465e-01   \n",
       "3    2.303975e-01   cbcl_q55_p  1.913884e-01   cbcl_q04_p  3.363701e-01   \n",
       "4    1.911072e-01   cbcl_q32_p  1.698548e-01   cbcl_q17_p  2.520370e-01   \n",
       "..            ...          ...           ...          ...           ...   \n",
       "109  2.220446e-16   cbcl_q77_p  2.220446e-16   cbcl_q34_p  2.220446e-16   \n",
       "110  2.220446e-16   cbcl_q01_p  2.220446e-16   cbcl_q77_p  2.220446e-16   \n",
       "111  2.220446e-16   cbcl_q54_p  2.220446e-16   cbcl_q54_p  2.220446e-16   \n",
       "112  2.220446e-16   cbcl_q30_p  2.220446e-16   cbcl_q30_p  2.220446e-16   \n",
       "113  2.220446e-16  cbcl_q56d_p  2.220446e-16  cbcl_q56d_p  2.220446e-16   \n",
       "\n",
       "                                          CBCL_Details  \n",
       "0    Self-conscious or easily embarrassed Se cohíbe...  \n",
       "1                            Worries Se preocupa mucho  \n",
       "2    Too fearful or anxious Demasiado ansioso(a) o ...  \n",
       "3    Nervous, highstrung, or tense Nervioso(a), ten...  \n",
       "4    Would rather be alone than with others Prefier...  \n",
       "..                                                 ...  \n",
       "109  Bowel movements outside toilet Hace sus necesi...  \n",
       "110                        Steals at home Roba en casa  \n",
       "111          Sets fires Prende fuegos/inicia incendios  \n",
       "112   Speech problem Problemas con el habla (describa)  \n",
       "113  Problems with eyes (not if corrected by glasse...  \n",
       "\n",
       "[114 rows x 11 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V1</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V2</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V3</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V4</th>\n",
       "      <th>Row_Name</th>\n",
       "      <th>V5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>2.520958e-01</td>\n",
       "      <td>cbcl_q22_p</td>\n",
       "      <td>3.304722e-01</td>\n",
       "      <td>cbcl_q56f_p</td>\n",
       "      <td>3.309346e-01</td>\n",
       "      <td>cbcl_q44_p</td>\n",
       "      <td>5.121333e-01</td>\n",
       "      <td>cbcl_q78_p</td>\n",
       "      <td>5.604418e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>2.348582e-01</td>\n",
       "      <td>cbcl_q03_p</td>\n",
       "      <td>3.094057e-01</td>\n",
       "      <td>cbcl_q56b_p</td>\n",
       "      <td>3.289139e-01</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>2.670481e-01</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>5.040500e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cbcl_q50_p</td>\n",
       "      <td>2.283924e-01</td>\n",
       "      <td>cbcl_q95_p</td>\n",
       "      <td>2.751811e-01</td>\n",
       "      <td>cbcl_q56e_p</td>\n",
       "      <td>2.386312e-01</td>\n",
       "      <td>cbcl_q58_p</td>\n",
       "      <td>1.927971e-01</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>4.114465e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cbcl_q45_p</td>\n",
       "      <td>1.692854e-01</td>\n",
       "      <td>cbcl_q28_p</td>\n",
       "      <td>2.482104e-01</td>\n",
       "      <td>cbcl_q56a_p</td>\n",
       "      <td>2.303975e-01</td>\n",
       "      <td>cbcl_q55_p</td>\n",
       "      <td>1.913884e-01</td>\n",
       "      <td>cbcl_q04_p</td>\n",
       "      <td>3.363701e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cbcl_q42_p</td>\n",
       "      <td>1.686497e-01</td>\n",
       "      <td>cbcl_q86_p</td>\n",
       "      <td>2.086195e-01</td>\n",
       "      <td>cbcl_q49_p</td>\n",
       "      <td>1.911072e-01</td>\n",
       "      <td>cbcl_q32_p</td>\n",
       "      <td>1.698548e-01</td>\n",
       "      <td>cbcl_q17_p</td>\n",
       "      <td>2.520370e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>cbcl_q06_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q92_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q38_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q77_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q34_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>cbcl_q81_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q98_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q59_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q01_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q77_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>cbcl_q72_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q63_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q54_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q54_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>cbcl_q79_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q18_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q30_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q30_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Row_Name            V1     Row_Name            V2     Row_Name  \\\n",
       "0     cbcl_q71_p  2.520958e-01   cbcl_q22_p  3.304722e-01  cbcl_q56f_p   \n",
       "1    cbcl_q112_p  2.348582e-01   cbcl_q03_p  3.094057e-01  cbcl_q56b_p   \n",
       "2     cbcl_q50_p  2.283924e-01   cbcl_q95_p  2.751811e-01  cbcl_q56e_p   \n",
       "3     cbcl_q45_p  1.692854e-01   cbcl_q28_p  2.482104e-01  cbcl_q56a_p   \n",
       "4     cbcl_q42_p  1.686497e-01   cbcl_q86_p  2.086195e-01   cbcl_q49_p   \n",
       "..           ...           ...          ...           ...          ...   \n",
       "109   cbcl_q06_p  2.220446e-16   cbcl_q92_p  2.220446e-16   cbcl_q38_p   \n",
       "110   cbcl_q81_p  2.220446e-16   cbcl_q98_p  2.220446e-16   cbcl_q59_p   \n",
       "111   cbcl_q72_p  2.220446e-16   cbcl_q10_p  2.220446e-16   cbcl_q63_p   \n",
       "112   cbcl_q79_p  2.220446e-16   cbcl_q08_p  2.220446e-16   cbcl_q18_p   \n",
       "113  cbcl_q56d_p  2.220446e-16  cbcl_q56d_p  2.220446e-16   cbcl_q71_p   \n",
       "\n",
       "               V3     Row_Name            V4     Row_Name            V5  \n",
       "0    3.309346e-01   cbcl_q44_p  5.121333e-01   cbcl_q78_p  5.604418e-01  \n",
       "1    3.289139e-01   cbcl_q71_p  2.670481e-01   cbcl_q08_p  5.040500e-01  \n",
       "2    2.386312e-01   cbcl_q58_p  1.927971e-01   cbcl_q10_p  4.114465e-01  \n",
       "3    2.303975e-01   cbcl_q55_p  1.913884e-01   cbcl_q04_p  3.363701e-01  \n",
       "4    1.911072e-01   cbcl_q32_p  1.698548e-01   cbcl_q17_p  2.520370e-01  \n",
       "..            ...          ...           ...          ...           ...  \n",
       "109  2.220446e-16   cbcl_q77_p  2.220446e-16   cbcl_q34_p  2.220446e-16  \n",
       "110  2.220446e-16   cbcl_q01_p  2.220446e-16   cbcl_q77_p  2.220446e-16  \n",
       "111  2.220446e-16   cbcl_q54_p  2.220446e-16   cbcl_q54_p  2.220446e-16  \n",
       "112  2.220446e-16   cbcl_q30_p  2.220446e-16   cbcl_q30_p  2.220446e-16  \n",
       "113  2.220446e-16  cbcl_q56d_p  2.220446e-16  cbcl_q56d_p  2.220446e-16  \n",
       "\n",
       "[114 rows x 10 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 iloc 获取第一列并转换为 Series，然后调用 apply()\n",
    "details = []\n",
    "for i in range(5):\n",
    "    detail = df.iloc[:, i*2].apply(get_cbcl_details)\n",
    "    details.append(detail)\n",
    "details = pd.concat(details, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[268], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRow_Name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_cbcl_details\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\pandas\\core\\frame.py:10361\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10347\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10349\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10350\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10351\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10359\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10360\u001b[0m )\n\u001b[1;32m> 10361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[184], line 25\u001b[0m, in \u001b[0;36mget_cbcl_details\u001b[1;34m(cbcl_item)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 使用正则表达式提取所有的 cbcl_q 字段\u001b[39;00m\n\u001b[0;32m     24\u001b[0m cbcl_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(cbcl_q\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+[a-z]*_p)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m cbcl_items \u001b[38;5;241m=\u001b[39m \u001b[43mcbcl_pattern\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbcl_item\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 存储每个 cbcl 字段的详细信息\u001b[39;00m\n\u001b[0;32m     28\u001b[0m details \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "test = df['Row_Name'].iloc[:,:1].apply(get_cbcl_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlation_matrix_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# correlation_matrix = encoded_df.join(data).corr()\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#load csv\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# correlation_matrix_filtered = correlation_matrix.drop([\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\"]).loc[:, \"PC1\":\"PC5\"]\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcorrelation_matrix_filtered\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'correlation_matrix_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "correlation_matrix = encoded_df.join(data).corr()\n",
    "#load csv\n",
    "\n",
    "correlation_matrix_filtered = correlation_matrix.drop([\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\"]).loc[:, \"PC1\":\"PC5\"]\n",
    "correlation_matrix_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index1</th>\n",
       "      <th>PC1</th>\n",
       "      <th>Index2</th>\n",
       "      <th>PC2</th>\n",
       "      <th>Index3</th>\n",
       "      <th>PC3</th>\n",
       "      <th>Index4</th>\n",
       "      <th>PC4</th>\n",
       "      <th>Index5</th>\n",
       "      <th>PC5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cbcl_q07_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q07_p</td>\n",
       "      <td>1.173915e-01</td>\n",
       "      <td>cbcl_q07_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q07_p</td>\n",
       "      <td>1.089160e-01</td>\n",
       "      <td>cbcl_q07_p</td>\n",
       "      <td>5.073395e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>5.040500e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cbcl_q09_p</td>\n",
       "      <td>1.195485e-01</td>\n",
       "      <td>cbcl_q09_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q09_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q09_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q09_p</td>\n",
       "      <td>1.861457e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>4.114465e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cbcl_q11_p</td>\n",
       "      <td>1.398559e-01</td>\n",
       "      <td>cbcl_q11_p</td>\n",
       "      <td>5.412335e-12</td>\n",
       "      <td>cbcl_q11_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q11_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q11_p</td>\n",
       "      <td>4.863420e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>cbcl_q108_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q108_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q108_p</td>\n",
       "      <td>1.627950e-01</td>\n",
       "      <td>cbcl_q108_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q108_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>cbcl_q109_p</td>\n",
       "      <td>5.009898e-02</td>\n",
       "      <td>cbcl_q109_p</td>\n",
       "      <td>1.153337e-01</td>\n",
       "      <td>cbcl_q109_p</td>\n",
       "      <td>5.627567e-02</td>\n",
       "      <td>cbcl_q109_p</td>\n",
       "      <td>7.371634e-02</td>\n",
       "      <td>cbcl_q109_p</td>\n",
       "      <td>2.369333e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>cbcl_q110_p</td>\n",
       "      <td>4.994746e-03</td>\n",
       "      <td>cbcl_q110_p</td>\n",
       "      <td>1.533680e-03</td>\n",
       "      <td>cbcl_q110_p</td>\n",
       "      <td>7.240329e-04</td>\n",
       "      <td>cbcl_q110_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q110_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>cbcl_q111_p</td>\n",
       "      <td>6.195912e-02</td>\n",
       "      <td>cbcl_q111_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q111_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q111_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q111_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>2.348582e-01</td>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>2.408723e-07</td>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>1.513654e-01</td>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Index1           PC1       Index2           PC2       Index3  \\\n",
       "5     cbcl_q07_p  2.220446e-16   cbcl_q07_p  1.173915e-01   cbcl_q07_p   \n",
       "6     cbcl_q08_p  2.220446e-16   cbcl_q08_p  2.220446e-16   cbcl_q08_p   \n",
       "7     cbcl_q09_p  1.195485e-01   cbcl_q09_p  2.220446e-16   cbcl_q09_p   \n",
       "8     cbcl_q10_p  2.220446e-16   cbcl_q10_p  2.220446e-16   cbcl_q10_p   \n",
       "9     cbcl_q11_p  1.398559e-01   cbcl_q11_p  5.412335e-12   cbcl_q11_p   \n",
       "..           ...           ...          ...           ...          ...   \n",
       "109  cbcl_q108_p  2.220446e-16  cbcl_q108_p  2.220446e-16  cbcl_q108_p   \n",
       "110  cbcl_q109_p  5.009898e-02  cbcl_q109_p  1.153337e-01  cbcl_q109_p   \n",
       "111  cbcl_q110_p  4.994746e-03  cbcl_q110_p  1.533680e-03  cbcl_q110_p   \n",
       "112  cbcl_q111_p  6.195912e-02  cbcl_q111_p  2.220446e-16  cbcl_q111_p   \n",
       "113  cbcl_q112_p  2.348582e-01  cbcl_q112_p  2.220446e-16  cbcl_q112_p   \n",
       "\n",
       "              PC3       Index4           PC4       Index5           PC5  \n",
       "5    2.220446e-16   cbcl_q07_p  1.089160e-01   cbcl_q07_p  5.073395e-02  \n",
       "6    2.220446e-16   cbcl_q08_p  2.220446e-16   cbcl_q08_p  5.040500e-01  \n",
       "7    2.220446e-16   cbcl_q09_p  2.220446e-16   cbcl_q09_p  1.861457e-01  \n",
       "8    2.220446e-16   cbcl_q10_p  2.220446e-16   cbcl_q10_p  4.114465e-01  \n",
       "9    2.220446e-16   cbcl_q11_p  2.220446e-16   cbcl_q11_p  4.863420e-02  \n",
       "..            ...          ...           ...          ...           ...  \n",
       "109  1.627950e-01  cbcl_q108_p  2.220446e-16  cbcl_q108_p  2.220446e-16  \n",
       "110  5.627567e-02  cbcl_q109_p  7.371634e-02  cbcl_q109_p  2.369333e-02  \n",
       "111  7.240329e-04  cbcl_q110_p  2.220446e-16  cbcl_q110_p  2.220446e-16  \n",
       "112  2.220446e-16  cbcl_q111_p  2.220446e-16  cbcl_q111_p  2.220446e-16  \n",
       "113  2.408723e-07  cbcl_q112_p  1.513654e-01  cbcl_q112_p  2.220446e-16  \n",
       "\n",
       "[109 rows x 10 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取索引和指定的列\n",
    "index_list = correlation_matrix.index\n",
    "# columns = [\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\"]\n",
    "columns = [\"V1\", \"V2\", \"V3\", \"V4\", \"V5\"]\n",
    "\n",
    "# 初始化空的列表用于存储新数据\n",
    "new_data = []\n",
    "\n",
    "# 交替排列 index 和每个 PC 列\n",
    "for col in columns:\n",
    "    new_data.append(index_list)\n",
    "    new_data.append(correlation_matrix[col].values)\n",
    "\n",
    "# 转置数据并创建 DataFrame\n",
    "new_df = pd.DataFrame(list(zip(*new_data)), columns=[\"Index1\", \"PC1\", \"Index2\", \"PC2\", \"Index3\", \"PC3\", \"Index4\", \"PC4\", \"Index5\", \"PC5\"])\n",
    "\n",
    "new_df = new_df.drop(index=[0, 1, 2, 3, 4])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义列对的索引\n",
    "column_pairs = [(\"Index1\", \"PC1\"), (\"Index2\", \"PC2\"), (\"Index3\", \"PC3\"), (\"Index4\", \"PC4\"), (\"Index5\", \"PC5\")]\n",
    "\n",
    "# 对每对列进行单独排序\n",
    "for index_col, pc_col in column_pairs:\n",
    "    # 只对当前的列对进行排序，不影响其他列\n",
    "    new_df[[index_col, pc_col]] = new_df[[index_col, pc_col]].sort_values(by=pc_col, ascending=False).values\n",
    "\n",
    "# new_df.to_csv(\"correlation_matrix.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index1</th>\n",
       "      <th>PC1</th>\n",
       "      <th>Index2</th>\n",
       "      <th>PC2</th>\n",
       "      <th>Index3</th>\n",
       "      <th>PC3</th>\n",
       "      <th>Index4</th>\n",
       "      <th>PC4</th>\n",
       "      <th>Index5</th>\n",
       "      <th>PC5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>0.252096</td>\n",
       "      <td>cbcl_q22_p</td>\n",
       "      <td>0.330472</td>\n",
       "      <td>cbcl_q56f_p</td>\n",
       "      <td>0.330935</td>\n",
       "      <td>cbcl_q44_p</td>\n",
       "      <td>0.512133</td>\n",
       "      <td>cbcl_q78_p</td>\n",
       "      <td>0.560442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>0.234858</td>\n",
       "      <td>cbcl_q95_p</td>\n",
       "      <td>0.275181</td>\n",
       "      <td>cbcl_q56b_p</td>\n",
       "      <td>0.328914</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>0.267048</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>0.50405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cbcl_q50_p</td>\n",
       "      <td>0.228392</td>\n",
       "      <td>cbcl_q28_p</td>\n",
       "      <td>0.24821</td>\n",
       "      <td>cbcl_q56e_p</td>\n",
       "      <td>0.238631</td>\n",
       "      <td>cbcl_q58_p</td>\n",
       "      <td>0.192797</td>\n",
       "      <td>cbcl_q10_p</td>\n",
       "      <td>0.411447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cbcl_q45_p</td>\n",
       "      <td>0.169285</td>\n",
       "      <td>cbcl_q86_p</td>\n",
       "      <td>0.20862</td>\n",
       "      <td>cbcl_q56a_p</td>\n",
       "      <td>0.230398</td>\n",
       "      <td>cbcl_q55_p</td>\n",
       "      <td>0.191388</td>\n",
       "      <td>cbcl_q17_p</td>\n",
       "      <td>0.252037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cbcl_q42_p</td>\n",
       "      <td>0.16865</td>\n",
       "      <td>cbcl_q43_p</td>\n",
       "      <td>0.200406</td>\n",
       "      <td>cbcl_q49_p</td>\n",
       "      <td>0.191107</td>\n",
       "      <td>cbcl_q32_p</td>\n",
       "      <td>0.169855</td>\n",
       "      <td>cbcl_q93_p</td>\n",
       "      <td>0.211002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>cbcl_q60_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q56h_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q23_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q52_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q51_p</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>cbcl_q61_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q62_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q25_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q51_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q50_p</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>cbcl_q67_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q64_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q08_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q50_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q49_p</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>cbcl_q68_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q70_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q26_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q49_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q48_p</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>cbcl_q56f_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q07_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q56f_p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Index1       PC1       Index2       PC2       Index3       PC3  \\\n",
       "5     cbcl_q71_p  0.252096   cbcl_q22_p  0.330472  cbcl_q56f_p  0.330935   \n",
       "6    cbcl_q112_p  0.234858   cbcl_q95_p  0.275181  cbcl_q56b_p  0.328914   \n",
       "7     cbcl_q50_p  0.228392   cbcl_q28_p   0.24821  cbcl_q56e_p  0.238631   \n",
       "8     cbcl_q45_p  0.169285   cbcl_q86_p   0.20862  cbcl_q56a_p  0.230398   \n",
       "9     cbcl_q42_p   0.16865   cbcl_q43_p  0.200406   cbcl_q49_p  0.191107   \n",
       "..           ...       ...          ...       ...          ...       ...   \n",
       "109   cbcl_q60_p       0.0  cbcl_q56h_p       0.0   cbcl_q23_p       0.0   \n",
       "110   cbcl_q61_p       0.0   cbcl_q62_p       0.0   cbcl_q25_p       0.0   \n",
       "111   cbcl_q67_p       0.0   cbcl_q64_p       0.0   cbcl_q08_p       0.0   \n",
       "112   cbcl_q68_p       0.0   cbcl_q70_p       0.0   cbcl_q26_p       0.0   \n",
       "113  cbcl_q56f_p       0.0  cbcl_q112_p       0.0   cbcl_q07_p       0.0   \n",
       "\n",
       "          Index4       PC4       Index5       PC5  \n",
       "5     cbcl_q44_p  0.512133   cbcl_q78_p  0.560442  \n",
       "6     cbcl_q71_p  0.267048   cbcl_q08_p   0.50405  \n",
       "7     cbcl_q58_p  0.192797   cbcl_q10_p  0.411447  \n",
       "8     cbcl_q55_p  0.191388   cbcl_q17_p  0.252037  \n",
       "9     cbcl_q32_p  0.169855   cbcl_q93_p  0.211002  \n",
       "..           ...       ...          ...       ...  \n",
       "109   cbcl_q52_p       0.0   cbcl_q51_p       0.0  \n",
       "110   cbcl_q51_p       0.0   cbcl_q50_p       0.0  \n",
       "111   cbcl_q50_p       0.0   cbcl_q49_p       0.0  \n",
       "112   cbcl_q49_p       0.0   cbcl_q48_p       0.0  \n",
       "113  cbcl_q56f_p       0.0  cbcl_q112_p       0.0  \n",
       "\n",
       "[109 rows x 10 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index1</th>\n",
       "      <th>PC1</th>\n",
       "      <th>Index2</th>\n",
       "      <th>PC2</th>\n",
       "      <th>Index3</th>\n",
       "      <th>PC3</th>\n",
       "      <th>Index4</th>\n",
       "      <th>PC4</th>\n",
       "      <th>Index5</th>\n",
       "      <th>PC5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cbcl_q44_p</td>\n",
       "      <td>0.658562</td>\n",
       "      <td>cbcl_q112_p</td>\n",
       "      <td>0.749548</td>\n",
       "      <td>cbcl_q87_p</td>\n",
       "      <td>0.625685</td>\n",
       "      <td>cbcl_q41_p</td>\n",
       "      <td>0.691131</td>\n",
       "      <td>cbcl_q03_p</td>\n",
       "      <td>0.719998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cbcl_q45_p</td>\n",
       "      <td>0.563849</td>\n",
       "      <td>cbcl_q50_p</td>\n",
       "      <td>0.661836</td>\n",
       "      <td>cbcl_q86_p</td>\n",
       "      <td>0.600941</td>\n",
       "      <td>avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p</td>\n",
       "      <td>0.668094</td>\n",
       "      <td>cbcl_q95_p</td>\n",
       "      <td>0.706209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p</td>\n",
       "      <td>0.562464</td>\n",
       "      <td>cbcl_q71_p</td>\n",
       "      <td>0.659321</td>\n",
       "      <td>cbcl_q95_p</td>\n",
       "      <td>0.590855</td>\n",
       "      <td>cbcl_q93_p</td>\n",
       "      <td>0.65752</td>\n",
       "      <td>cbcl_q86_p</td>\n",
       "      <td>0.705767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cbcl_q86_p</td>\n",
       "      <td>0.560706</td>\n",
       "      <td>cbcl_q32_p</td>\n",
       "      <td>0.618183</td>\n",
       "      <td>avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p</td>\n",
       "      <td>0.58445</td>\n",
       "      <td>cbcl_q19_p</td>\n",
       "      <td>0.657316</td>\n",
       "      <td>cbcl_q87_p</td>\n",
       "      <td>0.617334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cbcl_q41_p</td>\n",
       "      <td>0.551942</td>\n",
       "      <td>cbcl_q45_p</td>\n",
       "      <td>0.603183</td>\n",
       "      <td>cbcl_q03_p</td>\n",
       "      <td>0.577649</td>\n",
       "      <td>cbcl_q03_p</td>\n",
       "      <td>0.65202</td>\n",
       "      <td>cbcl_q27_p</td>\n",
       "      <td>0.561603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>cbcl_q56g_p</td>\n",
       "      <td>0.118983</td>\n",
       "      <td>cbcl_q60_p</td>\n",
       "      <td>0.067201</td>\n",
       "      <td>cbcl_q72_p</td>\n",
       "      <td>0.106907</td>\n",
       "      <td>cbcl_q56d_p</td>\n",
       "      <td>0.095307</td>\n",
       "      <td>cbcl_q107_p</td>\n",
       "      <td>0.071093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>cbcl_q59_p</td>\n",
       "      <td>0.101791</td>\n",
       "      <td>cbcl_q59_p</td>\n",
       "      <td>0.063268</td>\n",
       "      <td>cbcl_q59_p</td>\n",
       "      <td>0.09496</td>\n",
       "      <td>cbcl_q06_p</td>\n",
       "      <td>0.091715</td>\n",
       "      <td>cbcl_q98_p</td>\n",
       "      <td>0.042366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>cbcl_q72_p</td>\n",
       "      <td>0.101046</td>\n",
       "      <td>cbcl_q108_p</td>\n",
       "      <td>0.062521</td>\n",
       "      <td>cbcl_q110_p</td>\n",
       "      <td>0.076616</td>\n",
       "      <td>cbcl_q98_p</td>\n",
       "      <td>0.05389</td>\n",
       "      <td>cbcl_q110_p</td>\n",
       "      <td>0.032064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>cbcl_q110_p</td>\n",
       "      <td>0.074265</td>\n",
       "      <td>cbcl_q98_p</td>\n",
       "      <td>0.041541</td>\n",
       "      <td>cbcl_q98_p</td>\n",
       "      <td>0.053602</td>\n",
       "      <td>cbcl_q110_p</td>\n",
       "      <td>0.052574</td>\n",
       "      <td>cbcl_q79_p</td>\n",
       "      <td>0.016709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cbcl_q98_p</td>\n",
       "      <td>0.067795</td>\n",
       "      <td>cbcl_q72_p</td>\n",
       "      <td>0.036714</td>\n",
       "      <td>cbcl_q44_p</td>\n",
       "      <td>-0.11962</td>\n",
       "      <td>cbcl_q75_p</td>\n",
       "      <td>0.016323</td>\n",
       "      <td>cbcl_q17_p</td>\n",
       "      <td>-0.007429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Index1       PC1       Index2       PC2  \\\n",
       "5                              cbcl_q44_p  0.658562  cbcl_q112_p  0.749548   \n",
       "6                              cbcl_q45_p  0.563849   cbcl_q50_p  0.661836   \n",
       "7    avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p  0.562464   cbcl_q71_p  0.659321   \n",
       "8                              cbcl_q86_p  0.560706   cbcl_q32_p  0.618183   \n",
       "9                              cbcl_q41_p  0.551942   cbcl_q45_p  0.603183   \n",
       "..                                    ...       ...          ...       ...   \n",
       "103                           cbcl_q56g_p  0.118983   cbcl_q60_p  0.067201   \n",
       "104                            cbcl_q59_p  0.101791   cbcl_q59_p  0.063268   \n",
       "105                            cbcl_q72_p  0.101046  cbcl_q108_p  0.062521   \n",
       "106                           cbcl_q110_p  0.074265   cbcl_q98_p  0.041541   \n",
       "107                            cbcl_q98_p  0.067795   cbcl_q72_p  0.036714   \n",
       "\n",
       "                                   Index3       PC3  \\\n",
       "5                              cbcl_q87_p  0.625685   \n",
       "6                              cbcl_q86_p  0.600941   \n",
       "7                              cbcl_q95_p  0.590855   \n",
       "8    avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p   0.58445   \n",
       "9                              cbcl_q03_p  0.577649   \n",
       "..                                    ...       ...   \n",
       "103                            cbcl_q72_p  0.106907   \n",
       "104                            cbcl_q59_p   0.09496   \n",
       "105                           cbcl_q110_p  0.076616   \n",
       "106                            cbcl_q98_p  0.053602   \n",
       "107                            cbcl_q44_p  -0.11962   \n",
       "\n",
       "                                   Index4       PC4       Index5       PC5  \n",
       "5                              cbcl_q41_p  0.691131   cbcl_q03_p  0.719998  \n",
       "6    avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p  0.668094   cbcl_q95_p  0.706209  \n",
       "7                              cbcl_q93_p   0.65752   cbcl_q86_p  0.705767  \n",
       "8                              cbcl_q19_p  0.657316   cbcl_q87_p  0.617334  \n",
       "9                              cbcl_q03_p   0.65202   cbcl_q27_p  0.561603  \n",
       "..                                    ...       ...          ...       ...  \n",
       "103                           cbcl_q56d_p  0.095307  cbcl_q107_p  0.071093  \n",
       "104                            cbcl_q06_p  0.091715   cbcl_q98_p  0.042366  \n",
       "105                            cbcl_q98_p   0.05389  cbcl_q110_p  0.032064  \n",
       "106                           cbcl_q110_p  0.052574   cbcl_q79_p  0.016709  \n",
       "107                            cbcl_q75_p  0.016323   cbcl_q17_p -0.007429  \n",
       "\n",
       "[103 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 1, Average RMSE: 0.33886045973714163\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 2, Average RMSE: 0.3257940067190376\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 3, Average RMSE: 0.31681422516820434\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "J = 4, Average RMSE: 0.30991279810462624\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 5, Average RMSE: 0.3026250189044651\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "J = 6, Average RMSE: 0.29707249050530427\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "J = 7, Average RMSE: 0.2913985062655547\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "J = 8, Average RMSE: 0.2878472329276103\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 9, Average RMSE: 0.2842513377721056\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "J = 10, Average RMSE: 0.27790517052154656\n",
      "Optimal J: 10, Minimum RMSE: 0.27790517052154656\n",
      "Epoch 1/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 0.1382\n",
      "Epoch 2/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 0.1019\n",
      "Epoch 3/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.0949\n",
      "Epoch 4/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0900 \n",
      "Epoch 5/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 0.0891\n",
      "Epoch 6/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0876\n",
      "Epoch 7/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0855 \n",
      "Epoch 8/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 0.0851\n",
      "Epoch 9/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 0.0832\n",
      "Epoch 10/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0817\n",
      "Epoch 11/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0821 \n",
      "Epoch 12/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 0.0814\n",
      "Epoch 13/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0805\n",
      "Epoch 14/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0805\n",
      "Epoch 15/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0810\n",
      "Epoch 16/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 0.0803\n",
      "Epoch 17/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0797\n",
      "Epoch 18/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0793\n",
      "Epoch 19/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0802 \n",
      "Epoch 20/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - loss: 0.0784\n",
      "Epoch 21/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0797\n",
      "Epoch 22/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.0792\n",
      "Epoch 23/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 0.0787\n",
      "Epoch 24/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0776\n",
      "Epoch 25/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0785\n",
      "Epoch 26/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0782\n",
      "Epoch 27/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0771\n",
      "Epoch 28/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0789\n",
      "Epoch 29/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0769\n",
      "Epoch 30/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 0.0764\n",
      "Epoch 31/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0772 \n",
      "Epoch 32/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0779\n",
      "Epoch 33/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0778\n",
      "Epoch 34/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0765 \n",
      "Epoch 35/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0778\n",
      "Epoch 36/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0774 \n",
      "Epoch 37/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0757\n",
      "Epoch 38/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0750\n",
      "Epoch 39/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0775\n",
      "Epoch 40/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0765\n",
      "Epoch 41/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0759\n",
      "Epoch 42/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0776\n",
      "Epoch 43/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0758\n",
      "Epoch 44/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 0.0765\n",
      "Epoch 45/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0764 \n",
      "Epoch 46/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0752\n",
      "Epoch 47/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 0.0759\n",
      "Epoch 48/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0762\n",
      "Epoch 49/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0755\n",
      "Epoch 50/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0766\n",
      "Epoch 51/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0756\n",
      "Epoch 52/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0763\n",
      "Epoch 53/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0748\n",
      "Epoch 54/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0748\n",
      "Epoch 55/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0757\n",
      "Epoch 56/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0755\n",
      "Epoch 57/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0760\n",
      "Epoch 58/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0745\n",
      "Epoch 59/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 0.0765\n",
      "Epoch 60/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0751\n",
      "Epoch 61/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0754\n",
      "Epoch 62/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.0753\n",
      "Epoch 63/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0756\n",
      "Epoch 64/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0737\n",
      "Epoch 65/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0752\n",
      "Epoch 66/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0743\n",
      "Epoch 67/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 0.0745\n",
      "Epoch 68/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0754\n",
      "Epoch 69/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0754  \n",
      "Epoch 70/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0746\n",
      "Epoch 71/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.0744\n",
      "Epoch 72/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0746\n",
      "Epoch 73/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0742  \n",
      "Epoch 74/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0746\n",
      "Epoch 75/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0752\n",
      "Epoch 76/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.0745\n",
      "Epoch 77/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0760\n",
      "Epoch 78/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0735\n",
      "Epoch 79/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.0744\n",
      "Epoch 80/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 0.0741\n",
      "Epoch 81/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0740\n",
      "Epoch 82/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0745\n",
      "Epoch 83/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0731\n",
      "Epoch 84/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0748\n",
      "Epoch 85/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0740\n",
      "Epoch 86/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0741\n",
      "Epoch 87/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 0.0749\n",
      "Epoch 88/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0736\n",
      "Epoch 89/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0738\n",
      "Epoch 90/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0745\n",
      "Epoch 91/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0744  \n",
      "Epoch 92/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0747\n",
      "Epoch 93/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0741\n",
      "Epoch 94/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0742\n",
      "Epoch 95/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0736\n",
      "Epoch 96/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 0.0741\n",
      "Epoch 97/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 0.0746\n",
      "Epoch 98/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0740\n",
      "Epoch 99/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.0729\n",
      "Epoch 100/100\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f437105550>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Load data from dataframe\n",
    "d = data.shape[1]  # Input dimension based on dataframe\n",
    "X = data.values  # Convert dataframe to numpy array\n",
    "\n",
    "# Define a function to build a deep autoencoder model\n",
    "def build_autoencoder(input_dim, J):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder layers\n",
    "    encoded = Dense(8 * J, activation='relu')(input_layer)\n",
    "    bottleneck = Dense(J, activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder layers\n",
    "    decoded = Dense(8 * J, activation='relu')(bottleneck)\n",
    "    output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    # Define the autoencoder model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return autoencoder\n",
    "\n",
    "# Grid search for optimal J (1 <= J <= 10)\n",
    "min_rmse = float('inf')\n",
    "optimal_J = None\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for J in range(1, 11):\n",
    "    rmses = []\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        \n",
    "        # Build and compile the model\n",
    "        autoencoder = build_autoencoder(input_dim=d, J=J)\n",
    "        autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model\n",
    "        autoencoder.fit(X_train, X_train, epochs=100, batch_size=64, verbose=0, validation_data=(X_val, X_val))\n",
    "        \n",
    "        # Predict on validation set\n",
    "        X_val_pred = autoencoder.predict(X_val)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = sqrt(mean_squared_error(X_val, X_val_pred))\n",
    "        rmses.append(rmse)\n",
    "    \n",
    "    # Average RMSE for current J\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    print(f'J = {J}, Average RMSE: {avg_rmse}')\n",
    "    \n",
    "    # Update optimal J\n",
    "    if avg_rmse < min_rmse:\n",
    "        min_rmse = avg_rmse\n",
    "        optimal_J = J\n",
    "\n",
    "print(f'Optimal J: {optimal_J}, Minimum RMSE: {min_rmse}')\n",
    "\n",
    "# Build the final model with the optimal J\n",
    "autoencoder = build_autoencoder(input_dim=d, J=optimal_J)\n",
    "autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "autoencoder.fit(X, X, epochs=100, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "详细信息: Can't concentrate, can't pay attention for long No puede concentrarse o prestar atención por mucho tiempo; Can't sit still, restless, or hyperactive No puede quedarse quieto(a); es inquieto(a) o hiperactivo(a); Inattentive or easily distracted No presta atención o se distrae fácilmente\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from googletrans import Translator\n",
    "import re\n",
    "import time\n",
    "\n",
    "# 指定源语言和目标语言\n",
    "translator = Translator()\n",
    "\n",
    "def get_cbcl_details(cbcl_item):\n",
    "    \"\"\"\n",
    "    根据提供的 cbcl_q 字段组合（如 \"avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p\"）从 element.html 文件中获取详细信息。\n",
    "    \n",
    "    参数:\n",
    "        cbcl_item (str): 要查找的 cbcl_q 字段组合（如 \"avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p\"）。\n",
    "    \n",
    "    返回:\n",
    "        str: 详细信息的组合，如果找不到则返回 \"N/A\"。\n",
    "    \"\"\"\n",
    "    # 解析 element.html 文件\n",
    "    with open(\"data/element.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    # 使用正则表达式提取所有的 cbcl_q 字段\n",
    "    cbcl_pattern = re.compile(r\"(cbcl_q\\d+[a-z]*_p)\")\n",
    "    cbcl_items = cbcl_pattern.findall(cbcl_item)\n",
    "    \n",
    "    # 存储每个 cbcl 字段的详细信息\n",
    "    details = []\n",
    "\n",
    "    for cbcl in cbcl_items:\n",
    "        # 在 HTML 中查找包含 cbcl 的 <td> 标签\n",
    "        target = soup.find(lambda tag: tag.name == \"td\" and cbcl in tag.get_text(strip=True))\n",
    "        \n",
    "        # 获取详细信息\n",
    "        if target:\n",
    "            detail_info = target.find_next(\"td\").get_text(strip=True)\n",
    "            details.append(detail_info)\n",
    "        else:\n",
    "            details.append(\"N/A\")\n",
    "    \n",
    "    # 合并所有详细信息为一个字符串\n",
    "    combined_details = \"; \".join(details) if details else \"N/A\"\n",
    "\n",
    "    return combined_details\n",
    "\n",
    "# 示例调用\n",
    "detail = get_cbcl_details(\"avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p\")\n",
    "print(\"详细信息:\", detail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建空列表来存储每列的详细信息\n",
    "for col in [\"Index1\", \"Index2\", \"Index3\", \"Index4\", \"Index5\"]:\n",
    "    details = []  # 存储该列的详细信息\n",
    "\n",
    "    # 对每一行应用 get_cbcl_details 函数\n",
    "    for cbcl_item in new_df[col]:\n",
    "        detail_info = get_cbcl_details(cbcl_item)\n",
    "        details.append(detail_info)\n",
    "\n",
    "    # 将结果添加到 DataFrame 中\n",
    "    new_df[f\"{col}_Detail\"] = details\n",
    "\n",
    "# 查看添加了详细信息后的 DataFrame\n",
    "new_df.to_csv(\"correlation_matrix_with_details.csv\",index=False)\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.insert(0, 'src_subject_id', X_baseline.iloc[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>New_Column</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aalh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aarh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aglh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_agrh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_bs</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cdelh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cderh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_crcxlh</th>\n",
       "      <th>...</th>\n",
       "      <th>rsfmri_var_cortgordon_gp91lh</th>\n",
       "      <th>rsfmri_var_cortgordon_gp92lh</th>\n",
       "      <th>rsfmri_var_cortgordon_gp93lh</th>\n",
       "      <th>rsfmri_var_cortgordon_gp94lh</th>\n",
       "      <th>rsfmri_var_cortgordon_gp95lh</th>\n",
       "      <th>rsfmri_var_cortgordon_gp96lh</th>\n",
       "      <th>rsfmri_var_cortgordon_gp97lh</th>\n",
       "      <th>rsfmri_var_cortgordon_gp98lh</th>\n",
       "      <th>rsfmri_var_cortgordon_gp99lh</th>\n",
       "      <th>rsfmri_var_cortgordon_gp9lh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>-0.090258</td>\n",
       "      <td>-0.083909</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.048167</td>\n",
       "      <td>0.036012</td>\n",
       "      <td>0.013843</td>\n",
       "      <td>-0.020096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070026</td>\n",
       "      <td>0.241027</td>\n",
       "      <td>0.271503</td>\n",
       "      <td>0.141462</td>\n",
       "      <td>0.140140</td>\n",
       "      <td>0.037668</td>\n",
       "      <td>0.294336</td>\n",
       "      <td>0.213838</td>\n",
       "      <td>0.045987</td>\n",
       "      <td>0.094689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>-0.098593</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>-0.170801</td>\n",
       "      <td>-0.134319</td>\n",
       "      <td>-0.022124</td>\n",
       "      <td>0.092985</td>\n",
       "      <td>0.061854</td>\n",
       "      <td>0.011913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072261</td>\n",
       "      <td>0.125446</td>\n",
       "      <td>0.118057</td>\n",
       "      <td>0.177558</td>\n",
       "      <td>0.069124</td>\n",
       "      <td>0.061450</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.423981</td>\n",
       "      <td>0.086047</td>\n",
       "      <td>0.103453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>-0.065449</td>\n",
       "      <td>0.122893</td>\n",
       "      <td>-0.006246</td>\n",
       "      <td>-0.118878</td>\n",
       "      <td>-0.027118</td>\n",
       "      <td>-0.026523</td>\n",
       "      <td>-0.045971</td>\n",
       "      <td>-0.065351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022169</td>\n",
       "      <td>0.028780</td>\n",
       "      <td>0.084394</td>\n",
       "      <td>0.024809</td>\n",
       "      <td>0.097594</td>\n",
       "      <td>0.034820</td>\n",
       "      <td>0.034840</td>\n",
       "      <td>0.076949</td>\n",
       "      <td>0.030869</td>\n",
       "      <td>0.236822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>-0.148170</td>\n",
       "      <td>-0.049085</td>\n",
       "      <td>-0.026814</td>\n",
       "      <td>-0.028232</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>-0.153769</td>\n",
       "      <td>-0.073198</td>\n",
       "      <td>-0.138927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032624</td>\n",
       "      <td>0.046383</td>\n",
       "      <td>0.059366</td>\n",
       "      <td>0.059687</td>\n",
       "      <td>0.046924</td>\n",
       "      <td>0.041598</td>\n",
       "      <td>0.038637</td>\n",
       "      <td>0.048176</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>0.426372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.148117</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>-0.104190</td>\n",
       "      <td>-0.037308</td>\n",
       "      <td>0.121178</td>\n",
       "      <td>0.034533</td>\n",
       "      <td>-0.183537</td>\n",
       "      <td>-0.258663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039444</td>\n",
       "      <td>0.101833</td>\n",
       "      <td>0.102724</td>\n",
       "      <td>0.049330</td>\n",
       "      <td>0.080309</td>\n",
       "      <td>0.085158</td>\n",
       "      <td>0.106023</td>\n",
       "      <td>0.117981</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.040434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11157</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030591</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>-0.138875</td>\n",
       "      <td>-0.096117</td>\n",
       "      <td>-0.008607</td>\n",
       "      <td>-0.111538</td>\n",
       "      <td>-0.111535</td>\n",
       "      <td>-0.161508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068745</td>\n",
       "      <td>0.182518</td>\n",
       "      <td>0.110985</td>\n",
       "      <td>0.041953</td>\n",
       "      <td>0.068067</td>\n",
       "      <td>0.086526</td>\n",
       "      <td>0.072267</td>\n",
       "      <td>0.164568</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.070268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11158</th>\n",
       "      <td>NDAR_INVG0F3TJPW</td>\n",
       "      <td>NDAR_INVG0F3TJPW</td>\n",
       "      <td>-0.089590</td>\n",
       "      <td>-0.002218</td>\n",
       "      <td>-0.150480</td>\n",
       "      <td>0.052007</td>\n",
       "      <td>0.119924</td>\n",
       "      <td>-0.258660</td>\n",
       "      <td>-0.164504</td>\n",
       "      <td>-0.068733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061562</td>\n",
       "      <td>0.083194</td>\n",
       "      <td>0.093745</td>\n",
       "      <td>0.040504</td>\n",
       "      <td>0.053332</td>\n",
       "      <td>0.052622</td>\n",
       "      <td>0.071286</td>\n",
       "      <td>0.128881</td>\n",
       "      <td>0.016509</td>\n",
       "      <td>0.050277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11159</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.112247</td>\n",
       "      <td>-0.010195</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>-0.112359</td>\n",
       "      <td>-0.043313</td>\n",
       "      <td>0.045530</td>\n",
       "      <td>-0.188547</td>\n",
       "      <td>-0.048601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024539</td>\n",
       "      <td>0.101225</td>\n",
       "      <td>0.131841</td>\n",
       "      <td>0.026207</td>\n",
       "      <td>0.046078</td>\n",
       "      <td>0.036895</td>\n",
       "      <td>0.150434</td>\n",
       "      <td>0.087546</td>\n",
       "      <td>0.037467</td>\n",
       "      <td>0.026891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11160</th>\n",
       "      <td>NDAR_INVG0GY52R2</td>\n",
       "      <td>NDAR_INVG0GY52R2</td>\n",
       "      <td>0.051633</td>\n",
       "      <td>-0.127992</td>\n",
       "      <td>-0.172488</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>-0.014129</td>\n",
       "      <td>-0.236400</td>\n",
       "      <td>-0.056846</td>\n",
       "      <td>0.076282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051574</td>\n",
       "      <td>0.056057</td>\n",
       "      <td>0.125025</td>\n",
       "      <td>0.052557</td>\n",
       "      <td>0.061239</td>\n",
       "      <td>0.088988</td>\n",
       "      <td>0.085553</td>\n",
       "      <td>0.625010</td>\n",
       "      <td>0.046204</td>\n",
       "      <td>0.032801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11161</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.018827</td>\n",
       "      <td>-0.116764</td>\n",
       "      <td>-0.085335</td>\n",
       "      <td>0.049574</td>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.172729</td>\n",
       "      <td>-0.047047</td>\n",
       "      <td>-0.034302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116271</td>\n",
       "      <td>0.130321</td>\n",
       "      <td>0.137134</td>\n",
       "      <td>0.036241</td>\n",
       "      <td>0.061580</td>\n",
       "      <td>0.104490</td>\n",
       "      <td>0.088658</td>\n",
       "      <td>0.217588</td>\n",
       "      <td>0.028241</td>\n",
       "      <td>0.041765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11162 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id        New_Column  rsfmri_cor_ngd_au_scs_aalh  \\\n",
       "0      NDAR_INV003RTV85  NDAR_INV003RTV85                   -0.090258   \n",
       "1      NDAR_INV005V6D2C  NDAR_INV005V6D2C                   -0.098593   \n",
       "2      NDAR_INV007W6H7B  NDAR_INV007W6H7B                   -0.065449   \n",
       "3      NDAR_INV00BD7VDC  NDAR_INV00BD7VDC                   -0.148170   \n",
       "4                   NaN               NaN                   -0.148117   \n",
       "...                 ...               ...                         ...   \n",
       "11157               NaN               NaN                    0.030591   \n",
       "11158  NDAR_INVG0F3TJPW  NDAR_INVG0F3TJPW                   -0.089590   \n",
       "11159               NaN               NaN                   -0.112247   \n",
       "11160  NDAR_INVG0GY52R2  NDAR_INVG0GY52R2                    0.051633   \n",
       "11161               NaN               NaN                   -0.018827   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_aarh  rsfmri_cor_ngd_au_scs_aglh  \\\n",
       "0                       -0.083909                    0.004247   \n",
       "1                        0.093085                   -0.170801   \n",
       "2                        0.122893                   -0.006246   \n",
       "3                       -0.049085                   -0.026814   \n",
       "4                        0.037320                   -0.104190   \n",
       "...                           ...                         ...   \n",
       "11157                    0.003116                   -0.138875   \n",
       "11158                   -0.002218                   -0.150480   \n",
       "11159                   -0.010195                    0.038551   \n",
       "11160                   -0.127992                   -0.172488   \n",
       "11161                   -0.116764                   -0.085335   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_agrh  rsfmri_cor_ngd_au_scs_bs  \\\n",
       "0                       -0.054861                 -0.048167   \n",
       "1                       -0.134319                 -0.022124   \n",
       "2                       -0.118878                 -0.027118   \n",
       "3                       -0.028232                  0.008625   \n",
       "4                       -0.037308                  0.121178   \n",
       "...                           ...                       ...   \n",
       "11157                   -0.096117                 -0.008607   \n",
       "11158                    0.052007                  0.119924   \n",
       "11159                   -0.112359                 -0.043313   \n",
       "11160                    0.013249                 -0.014129   \n",
       "11161                    0.049574                  0.160182   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_cdelh  rsfmri_cor_ngd_au_scs_cderh  \\\n",
       "0                         0.036012                     0.013843   \n",
       "1                         0.092985                     0.061854   \n",
       "2                        -0.026523                    -0.045971   \n",
       "3                        -0.153769                    -0.073198   \n",
       "4                         0.034533                    -0.183537   \n",
       "...                            ...                          ...   \n",
       "11157                    -0.111538                    -0.111535   \n",
       "11158                    -0.258660                    -0.164504   \n",
       "11159                     0.045530                    -0.188547   \n",
       "11160                    -0.236400                    -0.056846   \n",
       "11161                     0.172729                    -0.047047   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_crcxlh  ...  rsfmri_var_cortgordon_gp91lh  \\\n",
       "0                         -0.020096  ...                      0.070026   \n",
       "1                          0.011913  ...                      0.072261   \n",
       "2                         -0.065351  ...                      0.022169   \n",
       "3                         -0.138927  ...                      0.032624   \n",
       "4                         -0.258663  ...                      0.039444   \n",
       "...                             ...  ...                           ...   \n",
       "11157                     -0.161508  ...                      0.068745   \n",
       "11158                     -0.068733  ...                      0.061562   \n",
       "11159                     -0.048601  ...                      0.024539   \n",
       "11160                      0.076282  ...                      0.051574   \n",
       "11161                     -0.034302  ...                      0.116271   \n",
       "\n",
       "       rsfmri_var_cortgordon_gp92lh  rsfmri_var_cortgordon_gp93lh  \\\n",
       "0                          0.241027                      0.271503   \n",
       "1                          0.125446                      0.118057   \n",
       "2                          0.028780                      0.084394   \n",
       "3                          0.046383                      0.059366   \n",
       "4                          0.101833                      0.102724   \n",
       "...                             ...                           ...   \n",
       "11157                      0.182518                      0.110985   \n",
       "11158                      0.083194                      0.093745   \n",
       "11159                      0.101225                      0.131841   \n",
       "11160                      0.056057                      0.125025   \n",
       "11161                      0.130321                      0.137134   \n",
       "\n",
       "       rsfmri_var_cortgordon_gp94lh  rsfmri_var_cortgordon_gp95lh  \\\n",
       "0                          0.141462                      0.140140   \n",
       "1                          0.177558                      0.069124   \n",
       "2                          0.024809                      0.097594   \n",
       "3                          0.059687                      0.046924   \n",
       "4                          0.049330                      0.080309   \n",
       "...                             ...                           ...   \n",
       "11157                      0.041953                      0.068067   \n",
       "11158                      0.040504                      0.053332   \n",
       "11159                      0.026207                      0.046078   \n",
       "11160                      0.052557                      0.061239   \n",
       "11161                      0.036241                      0.061580   \n",
       "\n",
       "       rsfmri_var_cortgordon_gp96lh  rsfmri_var_cortgordon_gp97lh  \\\n",
       "0                          0.037668                      0.294336   \n",
       "1                          0.061450                      0.101900   \n",
       "2                          0.034820                      0.034840   \n",
       "3                          0.041598                      0.038637   \n",
       "4                          0.085158                      0.106023   \n",
       "...                             ...                           ...   \n",
       "11157                      0.086526                      0.072267   \n",
       "11158                      0.052622                      0.071286   \n",
       "11159                      0.036895                      0.150434   \n",
       "11160                      0.088988                      0.085553   \n",
       "11161                      0.104490                      0.088658   \n",
       "\n",
       "       rsfmri_var_cortgordon_gp98lh  rsfmri_var_cortgordon_gp99lh  \\\n",
       "0                          0.213838                      0.045987   \n",
       "1                          0.423981                      0.086047   \n",
       "2                          0.076949                      0.030869   \n",
       "3                          0.048176                      0.017002   \n",
       "4                          0.117981                      0.048400   \n",
       "...                             ...                           ...   \n",
       "11157                      0.164568                      0.021918   \n",
       "11158                      0.128881                      0.016509   \n",
       "11159                      0.087546                      0.037467   \n",
       "11160                      0.625010                      0.046204   \n",
       "11161                      0.217588                      0.028241   \n",
       "\n",
       "       rsfmri_var_cortgordon_gp9lh  \n",
       "0                         0.094689  \n",
       "1                         0.103453  \n",
       "2                         0.236822  \n",
       "3                         0.426372  \n",
       "4                         0.040434  \n",
       "...                            ...  \n",
       "11157                     0.070268  \n",
       "11158                     0.050277  \n",
       "11159                     0.026891  \n",
       "11160                     0.032801  \n",
       "11161                     0.041765  \n",
       "\n",
       "[11162 rows x 1000 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = data_raw.drop(columns = \"New_Column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_raw = pd.read_csv(\"G:\\ABCD\\data\\mri_y_rsfmr_cor_gp_aseg.csv\")\n",
    "feature_raw = pd.read_csv(\"G:\\ABCD\\data\\mri_y_rsfmr_cor_gp_aseg.csv\")\n",
    "# merge data and X on src_subject_id\n",
    "\n",
    "X_baseline = feature_raw[feature_raw[\"eventname\"] == \"baseline_year_1_arm_1\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 data 和 X 按照 \"src_subject_id\" 列合并\n",
    "merged_df = pd.merge(data[\"src_subject_id\"], X_baseline, on=\"src_subject_id\", how=\"inner\")\n",
    "X = merged_df.drop(columns=[\"src_subject_id\",\"eventname\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aalh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aarh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aglh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_agrh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_bs</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cdelh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cderh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_crcxlh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_crcxrh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_hplh</th>\n",
       "      <th>...</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_hplh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_hprh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_pllh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_plrh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_ptlh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_ptrh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_thplh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_thprh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_vtdclh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_vtdcrh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055373</td>\n",
       "      <td>0.010663</td>\n",
       "      <td>0.077314</td>\n",
       "      <td>0.134695</td>\n",
       "      <td>0.022511</td>\n",
       "      <td>-0.038182</td>\n",
       "      <td>-0.176619</td>\n",
       "      <td>-0.063200</td>\n",
       "      <td>0.011281</td>\n",
       "      <td>0.104524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021614</td>\n",
       "      <td>-0.083068</td>\n",
       "      <td>0.036022</td>\n",
       "      <td>-0.003157</td>\n",
       "      <td>0.193484</td>\n",
       "      <td>0.050168</td>\n",
       "      <td>0.026512</td>\n",
       "      <td>0.016188</td>\n",
       "      <td>0.017193</td>\n",
       "      <td>0.053266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.012065</td>\n",
       "      <td>-0.058936</td>\n",
       "      <td>-0.138518</td>\n",
       "      <td>0.066717</td>\n",
       "      <td>-0.062600</td>\n",
       "      <td>-0.087190</td>\n",
       "      <td>-0.186163</td>\n",
       "      <td>-0.098255</td>\n",
       "      <td>0.088928</td>\n",
       "      <td>-0.002340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069426</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>-0.071978</td>\n",
       "      <td>0.046170</td>\n",
       "      <td>0.144247</td>\n",
       "      <td>-0.080032</td>\n",
       "      <td>-0.004278</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>-0.072794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.046328</td>\n",
       "      <td>-0.110550</td>\n",
       "      <td>-0.096390</td>\n",
       "      <td>-0.261646</td>\n",
       "      <td>-0.040385</td>\n",
       "      <td>0.258654</td>\n",
       "      <td>-0.155618</td>\n",
       "      <td>-0.188302</td>\n",
       "      <td>-0.300512</td>\n",
       "      <td>0.029960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578557</td>\n",
       "      <td>0.530182</td>\n",
       "      <td>0.089146</td>\n",
       "      <td>0.222993</td>\n",
       "      <td>0.122496</td>\n",
       "      <td>-0.499588</td>\n",
       "      <td>0.085961</td>\n",
       "      <td>-0.361897</td>\n",
       "      <td>-0.306012</td>\n",
       "      <td>-0.171299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.029424</td>\n",
       "      <td>-0.098254</td>\n",
       "      <td>-0.157998</td>\n",
       "      <td>-0.006393</td>\n",
       "      <td>-0.046835</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>-0.160298</td>\n",
       "      <td>-0.051735</td>\n",
       "      <td>0.145002</td>\n",
       "      <td>-0.044259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079179</td>\n",
       "      <td>0.102173</td>\n",
       "      <td>-0.057520</td>\n",
       "      <td>0.015970</td>\n",
       "      <td>0.100648</td>\n",
       "      <td>-0.045624</td>\n",
       "      <td>0.059917</td>\n",
       "      <td>-0.073928</td>\n",
       "      <td>-0.032104</td>\n",
       "      <td>-0.033315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028203</td>\n",
       "      <td>-0.006111</td>\n",
       "      <td>-0.045453</td>\n",
       "      <td>-0.041866</td>\n",
       "      <td>-0.034370</td>\n",
       "      <td>-0.050579</td>\n",
       "      <td>-0.131665</td>\n",
       "      <td>-0.049449</td>\n",
       "      <td>-0.020300</td>\n",
       "      <td>-0.080431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055287</td>\n",
       "      <td>-0.081229</td>\n",
       "      <td>-0.032886</td>\n",
       "      <td>-0.059226</td>\n",
       "      <td>0.093143</td>\n",
       "      <td>0.053695</td>\n",
       "      <td>-0.011197</td>\n",
       "      <td>-0.026894</td>\n",
       "      <td>0.027513</td>\n",
       "      <td>-0.074254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9342</th>\n",
       "      <td>-0.023167</td>\n",
       "      <td>0.018374</td>\n",
       "      <td>-0.115867</td>\n",
       "      <td>0.027363</td>\n",
       "      <td>-0.020531</td>\n",
       "      <td>-0.208167</td>\n",
       "      <td>-0.202320</td>\n",
       "      <td>0.044998</td>\n",
       "      <td>-0.030610</td>\n",
       "      <td>-0.024677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161818</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.037556</td>\n",
       "      <td>0.074350</td>\n",
       "      <td>-0.011557</td>\n",
       "      <td>0.068328</td>\n",
       "      <td>-0.012995</td>\n",
       "      <td>-0.021544</td>\n",
       "      <td>-0.022381</td>\n",
       "      <td>-0.024477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9343</th>\n",
       "      <td>0.060254</td>\n",
       "      <td>0.040693</td>\n",
       "      <td>0.016569</td>\n",
       "      <td>-0.021491</td>\n",
       "      <td>0.052835</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>-0.234214</td>\n",
       "      <td>0.024804</td>\n",
       "      <td>-0.050119</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052088</td>\n",
       "      <td>0.080394</td>\n",
       "      <td>-0.014290</td>\n",
       "      <td>0.026424</td>\n",
       "      <td>0.229135</td>\n",
       "      <td>0.076307</td>\n",
       "      <td>-0.086621</td>\n",
       "      <td>-0.088756</td>\n",
       "      <td>0.044557</td>\n",
       "      <td>-0.096324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9344</th>\n",
       "      <td>0.007412</td>\n",
       "      <td>-0.089075</td>\n",
       "      <td>-0.191196</td>\n",
       "      <td>0.021005</td>\n",
       "      <td>0.022065</td>\n",
       "      <td>-0.032718</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.055383</td>\n",
       "      <td>-0.276528</td>\n",
       "      <td>-0.012432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121496</td>\n",
       "      <td>0.066029</td>\n",
       "      <td>0.048063</td>\n",
       "      <td>-0.017240</td>\n",
       "      <td>0.142219</td>\n",
       "      <td>-0.017623</td>\n",
       "      <td>-0.063445</td>\n",
       "      <td>-0.100549</td>\n",
       "      <td>0.085462</td>\n",
       "      <td>0.079171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9345</th>\n",
       "      <td>-0.159067</td>\n",
       "      <td>-0.052097</td>\n",
       "      <td>-0.320385</td>\n",
       "      <td>-0.004716</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>-0.194241</td>\n",
       "      <td>-0.118141</td>\n",
       "      <td>-0.249773</td>\n",
       "      <td>-0.130421</td>\n",
       "      <td>-0.127866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074095</td>\n",
       "      <td>0.042162</td>\n",
       "      <td>-0.050792</td>\n",
       "      <td>-0.138313</td>\n",
       "      <td>0.166585</td>\n",
       "      <td>0.135075</td>\n",
       "      <td>-0.042237</td>\n",
       "      <td>-0.056728</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>-0.077982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9346</th>\n",
       "      <td>-0.015071</td>\n",
       "      <td>0.069801</td>\n",
       "      <td>-0.027091</td>\n",
       "      <td>-0.079858</td>\n",
       "      <td>-0.043853</td>\n",
       "      <td>-0.061442</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>-0.132110</td>\n",
       "      <td>-0.092790</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108699</td>\n",
       "      <td>0.151245</td>\n",
       "      <td>-0.014940</td>\n",
       "      <td>0.091691</td>\n",
       "      <td>0.095635</td>\n",
       "      <td>0.094026</td>\n",
       "      <td>0.031043</td>\n",
       "      <td>-0.074412</td>\n",
       "      <td>0.049758</td>\n",
       "      <td>-0.059234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9347 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rsfmri_cor_ngd_au_scs_aalh  rsfmri_cor_ngd_au_scs_aarh  \\\n",
       "0                       0.055373                    0.010663   \n",
       "1                      -0.012065                   -0.058936   \n",
       "2                      -0.046328                   -0.110550   \n",
       "3                      -0.029424                   -0.098254   \n",
       "4                      -0.028203                   -0.006111   \n",
       "...                          ...                         ...   \n",
       "9342                   -0.023167                    0.018374   \n",
       "9343                    0.060254                    0.040693   \n",
       "9344                    0.007412                   -0.089075   \n",
       "9345                   -0.159067                   -0.052097   \n",
       "9346                   -0.015071                    0.069801   \n",
       "\n",
       "      rsfmri_cor_ngd_au_scs_aglh  rsfmri_cor_ngd_au_scs_agrh  \\\n",
       "0                       0.077314                    0.134695   \n",
       "1                      -0.138518                    0.066717   \n",
       "2                      -0.096390                   -0.261646   \n",
       "3                      -0.157998                   -0.006393   \n",
       "4                      -0.045453                   -0.041866   \n",
       "...                          ...                         ...   \n",
       "9342                   -0.115867                    0.027363   \n",
       "9343                    0.016569                   -0.021491   \n",
       "9344                   -0.191196                    0.021005   \n",
       "9345                   -0.320385                   -0.004716   \n",
       "9346                   -0.027091                   -0.079858   \n",
       "\n",
       "      rsfmri_cor_ngd_au_scs_bs  rsfmri_cor_ngd_au_scs_cdelh  \\\n",
       "0                     0.022511                    -0.038182   \n",
       "1                    -0.062600                    -0.087190   \n",
       "2                    -0.040385                     0.258654   \n",
       "3                    -0.046835                     0.001867   \n",
       "4                    -0.034370                    -0.050579   \n",
       "...                        ...                          ...   \n",
       "9342                 -0.020531                    -0.208167   \n",
       "9343                  0.052835                     0.015180   \n",
       "9344                  0.022065                    -0.032718   \n",
       "9345                  0.012645                    -0.194241   \n",
       "9346                 -0.043853                    -0.061442   \n",
       "\n",
       "      rsfmri_cor_ngd_au_scs_cderh  rsfmri_cor_ngd_au_scs_crcxlh  \\\n",
       "0                       -0.176619                     -0.063200   \n",
       "1                       -0.186163                     -0.098255   \n",
       "2                       -0.155618                     -0.188302   \n",
       "3                       -0.160298                     -0.051735   \n",
       "4                       -0.131665                     -0.049449   \n",
       "...                           ...                           ...   \n",
       "9342                    -0.202320                      0.044998   \n",
       "9343                    -0.234214                      0.024804   \n",
       "9344                     0.004512                      0.055383   \n",
       "9345                    -0.118141                     -0.249773   \n",
       "9346                     0.000225                     -0.132110   \n",
       "\n",
       "      rsfmri_cor_ngd_au_scs_crcxrh  rsfmri_cor_ngd_au_scs_hplh  ...  \\\n",
       "0                         0.011281                    0.104524  ...   \n",
       "1                         0.088928                   -0.002340  ...   \n",
       "2                        -0.300512                    0.029960  ...   \n",
       "3                         0.145002                   -0.044259  ...   \n",
       "4                        -0.020300                   -0.080431  ...   \n",
       "...                            ...                         ...  ...   \n",
       "9342                     -0.030610                   -0.024677  ...   \n",
       "9343                     -0.050119                    0.074444  ...   \n",
       "9344                     -0.276528                   -0.012432  ...   \n",
       "9345                     -0.130421                   -0.127866  ...   \n",
       "9346                     -0.092790                    0.008556  ...   \n",
       "\n",
       "      rsfmri_cor_ngd_vs_scs_hplh  rsfmri_cor_ngd_vs_scs_hprh  \\\n",
       "0                      -0.021614                   -0.083068   \n",
       "1                       0.069426                    0.025448   \n",
       "2                       0.578557                    0.530182   \n",
       "3                       0.079179                    0.102173   \n",
       "4                       0.055287                   -0.081229   \n",
       "...                          ...                         ...   \n",
       "9342                    0.161818                    0.005746   \n",
       "9343                    0.052088                    0.080394   \n",
       "9344                    0.121496                    0.066029   \n",
       "9345                    0.074095                    0.042162   \n",
       "9346                    0.108699                    0.151245   \n",
       "\n",
       "      rsfmri_cor_ngd_vs_scs_pllh  rsfmri_cor_ngd_vs_scs_plrh  \\\n",
       "0                       0.036022                   -0.003157   \n",
       "1                      -0.071978                    0.046170   \n",
       "2                       0.089146                    0.222993   \n",
       "3                      -0.057520                    0.015970   \n",
       "4                      -0.032886                   -0.059226   \n",
       "...                          ...                         ...   \n",
       "9342                    0.037556                    0.074350   \n",
       "9343                   -0.014290                    0.026424   \n",
       "9344                    0.048063                   -0.017240   \n",
       "9345                   -0.050792                   -0.138313   \n",
       "9346                   -0.014940                    0.091691   \n",
       "\n",
       "      rsfmri_cor_ngd_vs_scs_ptlh  rsfmri_cor_ngd_vs_scs_ptrh  \\\n",
       "0                       0.193484                    0.050168   \n",
       "1                       0.144247                   -0.080032   \n",
       "2                       0.122496                   -0.499588   \n",
       "3                       0.100648                   -0.045624   \n",
       "4                       0.093143                    0.053695   \n",
       "...                          ...                         ...   \n",
       "9342                   -0.011557                    0.068328   \n",
       "9343                    0.229135                    0.076307   \n",
       "9344                    0.142219                   -0.017623   \n",
       "9345                    0.166585                    0.135075   \n",
       "9346                    0.095635                    0.094026   \n",
       "\n",
       "      rsfmri_cor_ngd_vs_scs_thplh  rsfmri_cor_ngd_vs_scs_thprh  \\\n",
       "0                        0.026512                     0.016188   \n",
       "1                       -0.004278                     0.002052   \n",
       "2                        0.085961                    -0.361897   \n",
       "3                        0.059917                    -0.073928   \n",
       "4                       -0.011197                    -0.026894   \n",
       "...                           ...                          ...   \n",
       "9342                    -0.012995                    -0.021544   \n",
       "9343                    -0.086621                    -0.088756   \n",
       "9344                    -0.063445                    -0.100549   \n",
       "9345                    -0.042237                    -0.056728   \n",
       "9346                     0.031043                    -0.074412   \n",
       "\n",
       "      rsfmri_cor_ngd_vs_scs_vtdclh  rsfmri_cor_ngd_vs_scs_vtdcrh  \n",
       "0                         0.017193                      0.053266  \n",
       "1                         0.006714                     -0.072794  \n",
       "2                        -0.306012                     -0.171299  \n",
       "3                        -0.032104                     -0.033315  \n",
       "4                         0.027513                     -0.074254  \n",
       "...                            ...                           ...  \n",
       "9342                     -0.022381                     -0.024477  \n",
       "9343                      0.044557                     -0.096324  \n",
       "9344                      0.085462                      0.079171  \n",
       "9345                      0.009536                     -0.077982  \n",
       "9346                      0.049758                     -0.059234  \n",
       "\n",
       "[9347 rows x 247 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =encoded_df[\"PC1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.669889\n",
       "1       0.114958\n",
       "2       0.034764\n",
       "3       0.290378\n",
       "4       0.604888\n",
       "          ...   \n",
       "9342    0.928409\n",
       "9343    0.989277\n",
       "9344    0.252261\n",
       "9345    0.333049\n",
       "9346    0.000000\n",
       "Name: PC1, Length: 9347, dtype: float32"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4173 - mae: 0.4777 - val_loss: 0.2631 - val_mae: 0.3814\n",
      "Epoch 2/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2171 - mae: 0.3537 - val_loss: 0.2574 - val_mae: 0.3864\n",
      "Epoch 3/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2059 - mae: 0.3396 - val_loss: 0.2607 - val_mae: 0.3934\n",
      "Epoch 4/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - loss: 0.1708 - mae: 0.3207 - val_loss: 0.2588 - val_mae: 0.3827\n",
      "Epoch 5/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.1645 - mae: 0.3092 - val_loss: 0.2699 - val_mae: 0.4008\n",
      "Epoch 6/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 0.1433 - mae: 0.2900 - val_loss: 0.2749 - val_mae: 0.3940\n",
      "Epoch 7/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 0.1148 - mae: 0.2611 - val_loss: 0.2936 - val_mae: 0.4110\n",
      "Epoch 8/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1089 - mae: 0.2530 - val_loss: 0.2952 - val_mae: 0.4126\n",
      "Epoch 9/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 0.1032 - mae: 0.2407 - val_loss: 0.3222 - val_mae: 0.4264\n",
      "Epoch 10/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 0.0899 - mae: 0.2249 - val_loss: 0.3209 - val_mae: 0.4283\n",
      "Epoch 11/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.0863 - mae: 0.2232 - val_loss: 0.3405 - val_mae: 0.4397\n",
      "Epoch 12/100\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0848 - mae: 0.2151 - val_loss: 0.3441 - val_mae: 0.4333\n",
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step\n",
      "R² Score on Training Set: 0.09854901088539969\n",
      "R² Score on Test Set: -0.12295400598763218\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 分割数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 定义深度学习模型\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # 第一层\n",
    "    keras.layers.Dense(32, activation='relu'),                                   # 第二层\n",
    "    keras.layers.Dense(1)                                                        # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 定义 EarlyStopping 回调\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',     # 监控验证损失\n",
    "    patience=10,            # 在 10 个 epoch 内无改进时停止训练\n",
    "    restore_best_weights=True  # 恢复最佳权重\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,             # 最大迭代次数\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,   # 使用部分训练数据进行验证\n",
    "    callbacks=[early_stopping],  # 添加 EarlyStopping 回调\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 使用模型进行预测\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# 计算训练集和测试集的 R²（决定系数）\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"R² Score on Training Set: {r2_train}\")\n",
    "print(f\"R² Score on Test Set: {r2_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFA from previous research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 原始数据，使用多行字符串（可以替换为您自己的数据）\n",
    "raw_data = \"\"\"\n",
    "| Composite (Attacks/threatens)                                | **0.90** | 0.03     | −0.14    | −0.03    | 0.02     |\n",
    "| Cruelty, bullying, or meanness to others                     | **0.88** | −0.05    | −0.10    | −0.01    | −0.03    |\n",
    "| Composite (Disobeys rules)                                   | **0.81** | −0.11    | 0.16     | −0.01    | −0.07    |\n",
    "| Gets in many fights                                          | **0.78** | −0.13    | 0.02     | 0.01     | 0.05     |\n",
    "| Temper tantrums or hot temper                                | **0.77** | 0.25     | −0.08    | 0.01     | −0.11    |\n",
    "| Argues a lot                                                 | **0.76** | 0.18     | 0.02     | 0.01     | −0.19    |\n",
    "| Composite (Destroys)                                         | **0.72** | −0.06    | 0.15     | −0.01    | 0.06     |\n",
    "| Screams a lot                                                | **0.72** | 0.17     | −0.03    | 0.01     | −0.04    |\n",
    "| Doesn’t seem to feel guilty after misbehaving                | **0.71** | −0.11    | 0.11     | −0.03    | 0.05     |\n",
    "| Swearing or obscene language                                 | **0.70** | −0.01    | −0.04    | 0.02     | 0.02     |\n",
    "| Teases a lot                                                 | **0.69** | −0.03    | 0.08     | 0.06     | −0.12    |\n",
    "| Composite (Steals)                                           | **0.69** | −0.23    | 0.12     | 0.01     | 0.10     |\n",
    "| Stubborn, sullen, or irritable                               | **0.69** | 0.27     | −0.09    | 0.08     | −0.05    |\n",
    "| Lying or cheating                                            | **0.68** | −0.18    | 0.16     | 0.05     | 0.00     |\n",
    "| Cruel to animals                                             | **0.67** | −0.05    | −0.03    | −0.10    | 0.15     |\n",
    "| Runs away from home                                          | **0.60** | 0.11     | 0.05     | 0.00     | 0.10     |\n",
    "| Sudden changes in mood or feelings                           | **0.60** | 0.32     | −0.02    | 0.08     | 0.03     |\n",
    "| Easily jealous                                               | **0.57** | 0.29     | 0.02     | −0.02    | −0.07    |\n",
    "| Composite (Peer problems)                                    | **0.53** | 0.07     | 0.14     | −0.03    | 0.26     |\n",
    "| Suspicious                                                   | **0.53** | 0.18     | 0.08     | 0.01     | 0.14     |\n",
    "| Demands a lot of attention                                   | **0.51** | 0.27     | 0.29     | 0.00     | −0.25    |\n",
    "| Thinks about sex too much                                    | **0.51** | −0.08    | 0.12     | 0.11     | 0.03     |\n",
    "| Hangs around with others who get in trouble                  | **0.51** | −0.18    | 0.19     | 0.04     | 0.01     |\n",
    "| Feels others are out to get him/her                          | **0.50** | 0.36     | 0.00     | −0.05    | 0.11     |\n",
    "| Sets fires                                                   | **0.50** | −0.18    | 0.19     | −0.08    | 0.06     |\n",
    "| Sulks a lot                                                  | **0.49** | 0.34     | −0.09    | 0.12     | 0.12     |\n",
    "| Showing off or clowning                                      | **0.49** | −0.04    | 0.36     | 0.04     | −0.28    |\n",
    "| Bragging, boasting                                           | **0.48** | 0.05     | 0.21     | 0.08     | −0.31    |\n",
    "| Whining                                                      | **0.41** | 0.27     | 0.09     | 0.09     | −0.09    |\n",
    "| Too fearful or anxious                                       | −0.13    | **0.70** | 0.33     | 0.02     | 0.03     |\n",
    "| Worries                                                      | −0.06    | **0.67** | 0.18     | 0.13     | 0.00     |\n",
    "| Feels he/she has to be perfect                               | −0.01    | **0.67** | −0.01    | 0.00     | −0.02    |\n",
    "| Feels too guilty                                             | −0.02    | **0.65** | 0.18     | 0.06     | −0.01    |\n",
    "| Nervous, high-strung, or tense                               | 0.04     | **0.57** | 0.38     | 0.00     | −0.04    |\n",
    "| Fears he/she might think or do something bad                 | 0.05     | **0.56** | 0.19     | −0.03    | 0.04     |\n",
    "| Feels worthless or inferior                                  | 0.28     | **0.55** | 0.04     | −0.03    | 0.14     |\n",
    "| Self-conscious or easily embarrassed                         | 0.06     | **0.46** | 0.07     | 0.08     | 0.26     |\n",
    "| Fears going to school                                        | 0.09     | **0.40** | 0.08     | 0.10     | 0.27     |\n",
    "| Fears certain animals, situations, or places, other than school | −0.05    | **0.37** | 0.24     | 0.08     | 0.08     |\n",
    "| Complains of loneliness                                      | 0.25     | **0.36** | 0.16     | 0.04     | 0.13     |\n",
    "| Composite (Distracted/Hyperactive)                           | 0.21     | −0.04    | **0.77** | −0.06    | 0.00     |\n",
    "| Daydreams or gets lost in his/her thoughts                   | −0.12    | 0.05     | **0.64** | 0.02     | 0.20     |\n",
    "| Stares blankly                                               | −0.03    | −0.03    | **0.60** | 0.04     | 0.36     |\n",
    "| Confused or seems to be in a fog                             | −0.06    | 0.07     | **0.60** | −0.02    | 0.36     |\n",
    "| Poorly coordinated or clumsy                                 | −0.02    | −0.08    | **0.58** | 0.24     | 0.15     |\n",
    "| Nervous movements or twitching                               | −0.01    | 0.24     | **0.54** | 0.00     | −0.02    |\n",
    "| Fails to finish things he/she starts                         | 0.27     | −0.01    | **0.53** | 0.02     | 0.05     |\n",
    "| Talks too much                                               | 0.20     | 0.04     | **0.52** | 0.12     | −0.26    |\n",
    "| Can’t get his/her mind off certain thoughts; obsessions      | 0.16     | 0.30     | **0.50** | −0.05    | −0.02    |\n",
    "| Poor school work                                             | 0.29     | −0.14    | **0.49** | −0.04    | 0.18     |\n",
    "| Repeats certain acts over and over; compulsions              | 0.20     | 0.11     | **0.48** | −0.03    | 0.14     |\n",
    "| Strange ideas                                                | 0.18     | 0.04     | **0.45** | 0.04     | 0.18     |\n",
    "| Acts too young for his/her age                               | 0.20     | 0.05     | **0.45** | −0.09    | 0.12     |\n",
    "| Gets hurt a lot, accident prone                              | 0.04     | −0.06    | **0.41** | 0.31     | −0.02    |\n",
    "| Prefers being with younger kids                              | 0.11     | 0.05     | **0.35** | 0.03     | 0.19     |\n",
    "| Nausea, feels sick                                           | −0.01    | 0.04     | −0.06    | **0.89** | −0.05    |\n",
    "| Stomachaches                                                 | −0.01    | 0.05     | −0.08    | **0.82** | −0.03    |\n",
    "| Vomiting, throwing up                                        | 0.02     | −0.19    | −0.04    | **0.75** | 0.06     |\n",
    "| Headaches                                                    | 0.02     | 0.02     | −0.02    | **0.62** | 0.01     |\n",
    "| Aches or pains (not stomach or headaches)                    | 0.00     | 0.05     | 0.05     | **0.57** | −0.01    |\n",
    "| Feels dizzy or lightheaded                                   | −0.05    | 0.15     | 0.08     | **0.53** | 0.11     |\n",
    "| Other (physical problems without known physical cause)       | 0.02     | 0.04     | 0.11     | **0.48** | 0.02     |\n",
    "| Problems with eyes (not if corrected by glasses)             | 0.00     | −0.04    | 0.04     | **0.36** | 0.23     |\n",
    "| Rashes or other skin problems                                | 0.01     | 0.00     | 0.10     | **0.35** | 0.04     |\n",
    "| Withdrawn, doesn’t get involved with others                  | 0.17     | 0.17     | 0.03     | 0.05     | **0.65** |\n",
    "| Would rather be alone than with others                       | 0.13     | 0.11     | 0.05     | 0.01     | **0.56** |\n",
    "| Too shy or timid                                             | −0.10    | 0.31     | −0.01    | 0.06     | **0.55** |\n",
    "| Refuses to talk                                              | 0.27     | 0.13     | −0.02    | 0.04     | **0.51** |\n",
    "| Underactive, slow moving, or lacks energy                    | 0.07     | 0.00     | 0.12     | 0.34     | **0.45** |\n",
    "| Secretive, keeps things to self                              | 0.32     | 0.08     | 0.02     | 0.08     | 0.40     |\n",
    "| Strange behavior                                             | 0.32     | 0.05     | 0.41     | −0.01    | 0.23     |\n",
    "| There is very little he/she enjoys                           | 0.39     | 0.16     | 0.01     | 0.01     | 0.34     |\n",
    "| Unhappy, sad, or depressed                                   | 0.38     | 0.42     | −0.09    | 0.12     | 0.23     |\n",
    "| Unusually loud                                               | 0.39     | 0.08     | 0.42     | 0.11     | −0.21    |\n",
    "| Deliberately harms self or attempts suicide                  | 0.39     | 0.37     | 0.06     | −0.08    | 0.10     |\n",
    "| Feels or complains that no one loves him/her                 | 0.54     | 0.47     | −0.10    | −0.05    | 0.07     |\n",
    "| Impulsive or acts without thinking                           | 0.49     | 0.02     | 0.49     | −0.05    | −0.11    |\n",
    "| Talks about killing self                                     | 0.44     | 0.38     | −0.01    | −0.03    | 0.05     |\n",
    "| Overtired without good reason                                | 0.14     | 0.07     | 0.07     | 0.35     | 0.32     |\n",
    "| Composite (Sex play)                                         | 0.33     | −0.03    | 0.17     | 0.03     | −0.01    |\n",
    "| Composite (Weight problems)                                  | 0.14     | −0.01    | 0.04     | 0.22     | 0.16     |\n",
    "| Composite (Hallucinations)                                   | 0.16     | 0.01     | 0.26     | 0.20     | 0.18     |\n",
    "| Bowel movements outside toilet                               | 0.13     | −0.07    | 0.12     | 0.14     | 0.19     |\n",
    "| Trouble sleeping                                             | 0.04     | 0.25     | 0.30     | 0.23     | 0.01     |\n",
    "| Wets self during the day                                     | 0.08     | −0.01    | 0.25     | 0.12     | 0.17     |\n",
    "| Wets the bed                                                 | 0.13     | −0.10    | 0.15     | 0.07     | 0.06     |\n",
    "| Wishes to be of opposite sex                                 | 0.07     | 0.11     | 0.11     | −0.01    | 0.24     |\n",
    "| Clings to adults or too dependent                            | 0.13     | 0.28     | 0.29     | 0.06     | 0.10     |\n",
    "| Cries a lot                                                  | 0.31     | 0.31     | 0.10     | 0.05     | 0.08     |\n",
    "| Doesn’t eat well                                             | 0.16     | 0.08     | 0.16     | 0.12     | 0.10     |\n",
    "| Gets teased a lot                                            | 0.30     | 0.06     | 0.23     | 0.04     | 0.28     |\n",
    "| Bites fingernails                                            | 0.07     | 0.10     | 0.23     | 0.05     | −0.04    |\n",
    "| Nightmares                                                   | 0.03     | 0.19     | 0.27     | 0.27     | −0.03    |\n",
    "| Constipated, doesn’t move bowels                             | −0.01    | 0.13     | 0.11     | 0.29     | 0.09     |\n",
    "| Picks nose, skin, or other parts of body                     | 0.17     | 0.09     | 0.33     | 0.08     | −0.04    |\n",
    "| Prefers being with older kids                                | 0.30     | −0.02    | 0.20     | 0.11     | 0.03     |\n",
    "| Sleeps less than most kids                                   | 0.06     | 0.16     | 0.33     | 0.15     | 0.04     |\n",
    "| Sleeps more than most kids during day and/or night           | 0.10     | −0.01    | 0.11     | 0.21     | 0.26     |\n",
    "| Speech problem                                               | 0.00     | −0.07    | 0.34     | −0.01    | 0.23     |\n",
    "| Stores up too many things he/she doesn’t need                | 0.18     | 0.13     | 0.25     | 0.11     | 0.04     |\n",
    "| Talks or walks in sleep                                      | 0.02     | 0.03     | 0.24     | 0.25     | −0.14    |\n",
    "| Thumb-sucking                                                | 0.11     | −0.03    | 0.07     | 0.07     | 0.01     |\n",
    "\"\"\"\n",
    "\n",
    "# 解析数据并提取行为和数值\n",
    "lines = raw_data.strip().split(\"\\n\")\n",
    "data = []\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    # 使用正则表达式提取行为和数值\n",
    "    parts = line.strip(\"|\").split(\"|\")\n",
    "    behavior = parts[0].strip()\n",
    "    # 将数值部分拆分，移除星号，并转换为浮点数\n",
    "    values = [float(part.strip().replace(\"−\", \"-\").replace(\"*\", \"\").replace(\"**\", \"\")) for part in parts[1:]]\n",
    "    data.append([behavior] + values)\n",
    "\n",
    "# 创建DataFrame并保存为CSV\n",
    "df = pd.DataFrame(data, columns=[\"Behavior\", \"Factor1\", \"Factor2\", \"Factor3\", \"Factor4\", \"Factor5\"])\n",
    "# df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "# print(\"数据已成功转换为CSV格式并保存为 output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一列的平方和计算结果：\n",
      "Factor1    15.0487\n",
      "Factor2     5.9763\n",
      "Factor3     7.1084\n",
      "Factor4     4.6421\n",
      "Factor5     3.7880\n",
      "dtype: float64\n",
      "Factor1    41.157712\n",
      "Factor2    16.344989\n",
      "Factor3    19.441246\n",
      "Factor4    12.695995\n",
      "Factor5    10.360059\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "squared_sums = (df.iloc[:, 1:] ** 2).sum()\n",
    "\n",
    "print(\"每一列的平方和计算结果：\")\n",
    "print(squared_sums)\n",
    "# 计算每一列的平方和占比\n",
    "squared_sums_percentage = (squared_sums / squared_sums.sum()) * 100\n",
    "\n",
    "print(squared_sums_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed columns with low frequency: ['cbcl_q02_p', 'cbcl_q73_p', 'cbcl_q99_p', 'cbcl_q101_p', 'cbcl_q105_p']\n",
      "Highly correlated pairs (r > 0.75): [('cbcl_q08_p', 'cbcl_q10_p'), ('cbcl_q08_p', 'cbcl_q78_p'), ('cbcl_q16_p', 'cbcl_q97_p'), ('cbcl_q20_p', 'cbcl_q21_p'), ('cbcl_q22_p', 'cbcl_q28_p'), ('cbcl_q23_p', 'cbcl_q28_p'), ('cbcl_q25_p', 'cbcl_q48_p'), ('cbcl_q40_p', 'cbcl_q70_p'), ('cbcl_q53_p', 'cbcl_q55_p'), ('cbcl_q56c_p', 'cbcl_q56f_p'), ('cbcl_q57_p', 'cbcl_q97_p'), ('cbcl_q81_p', 'cbcl_q82_p')]\n"
     ]
    }
   ],
   "source": [
    "# delete columns with low frequency (more than 99.5% of the values are 0)\n",
    "low_frequency_columns = data.columns[data.apply(lambda col: (col == 0).mean() > 0.995)]\n",
    "data_cleaned = data.drop(columns=low_frequency_columns)\n",
    "print(f\"Removed columns with low frequency: {low_frequency_columns.tolist()}\")\n",
    "\n",
    "#load corerlation matrix from polychoric correlation matrix\n",
    "\n",
    "correlation_matrix = pd.read_csv(data_path + \"/factor analysis/output/polychoric_correlation_matrix_twins.csv\", index_col=0)\n",
    "\n",
    "# mark highly correlated pairs (r > 0.75)\n",
    "high_corr_pairs = (correlation_matrix.abs() > 0.75).where(lambda x: np.triu(x, 1)).stack().index.tolist()\n",
    "print(f\"Highly correlated pairs (r > 0.75): {high_corr_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of connected groups: 9\n",
      "columns for each connected group: [('cbcl_q08_p', 'cbcl_q10_p', 'cbcl_q78_p'), ('cbcl_q16_p', 'cbcl_q57_p', 'cbcl_q97_p'), ('cbcl_q20_p', 'cbcl_q21_p'), ('cbcl_q22_p', 'cbcl_q23_p', 'cbcl_q28_p'), ('cbcl_q25_p', 'cbcl_q48_p'), ('cbcl_q40_p', 'cbcl_q70_p'), ('cbcl_q53_p', 'cbcl_q55_p'), ('cbcl_q56c_p', 'cbcl_q56f_p'), ('cbcl_q81_p', 'cbcl_q82_p')]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_connected_groups(pairs):\n",
    "    # 建立图结构\n",
    "    graph = defaultdict(set)\n",
    "    for col1, col2 in pairs:\n",
    "        graph[col1].add(col2)\n",
    "        graph[col2].add(col1)\n",
    "    \n",
    "    # 深度优先搜索（DFS）找到所有连通分量\n",
    "    visited = set()\n",
    "    connected_groups = []\n",
    "\n",
    "    def dfs(node, group):\n",
    "        visited.add(node)\n",
    "        group.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            if neighbor not in visited:\n",
    "                dfs(neighbor, group)\n",
    "\n",
    "    # 遍历所有节点，找到每个连通分量\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            group = set()\n",
    "            dfs(node, group)\n",
    "            connected_groups.append(tuple(sorted(group)))\n",
    "\n",
    "    return connected_groups\n",
    "\n",
    "# 使用函数\n",
    "result = find_connected_groups(high_corr_pairs)\n",
    "print(\"number of connected groups:\", len(result))\n",
    "print(\"columns for each connected group:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe to store the final data\n",
    "data_final = data_cleaned.copy()\n",
    "for group in result:\n",
    "    # calculate the average of the columns in the group\n",
    "    data_final[f\"avg_{'_'.join(group)}\"] = data_cleaned[list(group)].mean(axis=1).round().astype(int)\n",
    "    # delete the original columns\n",
    "    data_final.drop(columns=list(group), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #scale data_final\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# data_scaled = scaler.fit_transform(data_final)\n",
    "\n",
    "# #make data_scaled to dataframe\n",
    "# data_scaled = pd.DataFrame(data_scaled, columns=data_final.columns)\n",
    "data_scaled = data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled.drop(columns=[\"src_subject_id\", \"rel_family_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bartlett's Test Chi-square: 238146.69520761166, p-value: 0.0\n",
      "KMO Test Score: 0.9624037123398348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\PhD\\Lib\\site-packages\\factor_analyzer\\utils.py:244: UserWarning: The inverse of the variance-covariance matrix was calculated using the Moore-Penrose generalized matrix inversion, due to its determinant being at or very close to zero.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor Loadings DataFrame:\n",
      "                              Factor 1  Factor 2  Factor 3  Factor 4  Factor 5  \\\n",
      "cbcl_q01_p                   0.158746  0.054357 -0.137513  0.317554 -0.009661   \n",
      "cbcl_q03_p                   0.583481 -0.001042  0.031194  0.142819  0.032724   \n",
      "cbcl_q04_p                   0.147039 -0.034074 -0.041008  0.566630  0.037088   \n",
      "cbcl_q05_p                   0.196349 -0.046049 -0.013099  0.202941  0.009528   \n",
      "cbcl_q06_p                  -0.054184  0.004284 -0.008707 -0.017063  0.009467   \n",
      "...                               ...       ...       ...       ...       ...   \n",
      "avg_cbcl_q25_p_cbcl_q48_p    0.006006  0.040872  0.114927  0.026406  0.012949   \n",
      "avg_cbcl_q40_p_cbcl_q70_p   -0.058122  0.004397  0.236370 -0.087217  0.070249   \n",
      "avg_cbcl_q53_p_cbcl_q55_p   -0.021818  0.035933  0.024859 -0.082826 -0.033896   \n",
      "avg_cbcl_q56c_p_cbcl_q56f_p  0.005402  0.015606 -0.031792 -0.036812  0.678849   \n",
      "avg_cbcl_q81_p_cbcl_q82_p   -0.052846  0.014157  0.557623  0.051722 -0.048664   \n",
      "\n",
      "                             Factor 6  Factor 7  Factor 8  Factor 9  \\\n",
      "cbcl_q01_p                   0.038407 -0.115113  0.068428  0.092101   \n",
      "cbcl_q03_p                  -0.018788  0.002771 -0.110690  0.034025   \n",
      "cbcl_q04_p                   0.030110  0.027396 -0.037527  0.049811   \n",
      "cbcl_q05_p                   0.183087  0.035233  0.050562 -0.007544   \n",
      "cbcl_q06_p                  -0.038526 -0.028323 -0.023193  0.713100   \n",
      "...                               ...       ...       ...       ...   \n",
      "avg_cbcl_q25_p_cbcl_q48_p    0.026598  0.029737 -0.006953  0.063788   \n",
      "avg_cbcl_q40_p_cbcl_q70_p   -0.054602 -0.055378  0.222498  0.012704   \n",
      "avg_cbcl_q53_p_cbcl_q55_p   -0.056978 -0.021421 -0.081874 -0.007582   \n",
      "avg_cbcl_q56c_p_cbcl_q56f_p -0.019178  0.055137  0.029425  0.027449   \n",
      "avg_cbcl_q81_p_cbcl_q82_p    0.030524  0.007799  0.063038  0.063060   \n",
      "\n",
      "                             Factor 10  Factor 11  Factor 12  Factor 13  \\\n",
      "cbcl_q01_p                   -0.025733   0.038476   0.105795   0.230014   \n",
      "cbcl_q03_p                    0.090476   0.052660  -0.103383   0.036130   \n",
      "cbcl_q04_p                    0.119454   0.016021  -0.051581   0.028233   \n",
      "cbcl_q05_p                    0.090549   0.052506   0.084848  -0.112601   \n",
      "cbcl_q06_p                    0.023510  -0.001036  -0.002611  -0.021785   \n",
      "...                                ...        ...        ...        ...   \n",
      "avg_cbcl_q25_p_cbcl_q48_p     0.032268   0.044241   0.126943  -0.037628   \n",
      "avg_cbcl_q40_p_cbcl_q70_p     0.155627  -0.027468   0.141935   0.099658   \n",
      "avg_cbcl_q53_p_cbcl_q55_p    -0.017702   0.011805  -0.154187   0.048614   \n",
      "avg_cbcl_q56c_p_cbcl_q56f_p   0.029578  -0.022269  -0.040092  -0.012600   \n",
      "avg_cbcl_q81_p_cbcl_q82_p    -0.039128   0.039008   0.074715   0.066704   \n",
      "\n",
      "                             Factor 14  Factor 15  Factor 16  \n",
      "cbcl_q01_p                    0.263575  -0.083768   0.054505  \n",
      "cbcl_q03_p                    0.040839   0.009997   0.021058  \n",
      "cbcl_q04_p                    0.023553   0.033965   0.050972  \n",
      "cbcl_q05_p                    0.041886   0.235097  -0.264961  \n",
      "cbcl_q06_p                    0.010959  -0.030562  -0.147373  \n",
      "...                                ...        ...        ...  \n",
      "avg_cbcl_q25_p_cbcl_q48_p     0.557148   0.071217  -0.037586  \n",
      "avg_cbcl_q40_p_cbcl_q70_p    -0.046192   0.099780   0.067468  \n",
      "avg_cbcl_q53_p_cbcl_q55_p     0.140610   0.632542   0.126481  \n",
      "avg_cbcl_q56c_p_cbcl_q56f_p   0.000307   0.056562  -0.003889  \n",
      "avg_cbcl_q81_p_cbcl_q82_p    -0.055875  -0.012269   0.072156  \n",
      "\n",
      "[102 rows x 16 columns]\n",
      "Variance Explained:\n",
      " (array([3.67359709, 1.75560805, 3.21420174, 2.89985508, 2.31644997,\n",
      "       2.2742063 , 2.64637479, 1.73200009, 1.62380833, 2.00611448,\n",
      "       1.70086874, 1.81227118, 1.57741179, 1.93266923, 1.88994364,\n",
      "       1.52974663]), array([0.03601566, 0.01721184, 0.03151178, 0.02842995, 0.02271029,\n",
      "       0.02229614, 0.02594485, 0.01698039, 0.01591969, 0.01966779,\n",
      "       0.01667518, 0.01776736, 0.01546482, 0.01894774, 0.01852886,\n",
      "       0.01499752]), array([0.03601566, 0.0532275 , 0.08473928, 0.11316923, 0.13587953,\n",
      "       0.15817567, 0.18412052, 0.20110091, 0.2170206 , 0.23668839,\n",
      "       0.25336358, 0.27113094, 0.28659576, 0.3055435 , 0.32407236,\n",
      "       0.33906987]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Bartlett 和 KMO 测试\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(data_scaled)\n",
    "print(f\"Bartlett's Test Chi-square: {chi_square_value}, p-value: {p_value}\")\n",
    "kmo_all, kmo_model = calculate_kmo(data_scaled)\n",
    "print(f\"KMO Test Score: {kmo_model}\")\n",
    "\n",
    "# factor analysis\n",
    "fa = FactorAnalyzer(n_factors=16,  rotation= rotation, method = 'principal')\n",
    "# fa = FactorAnalyzer(n_factors=16, rotation=\"equamax\", method='principal')\n",
    "# fa.fit(data_cleaned)\n",
    "fa.fit(data_scaled)\n",
    "\n",
    "# factor loadings\n",
    "factor_loadings = fa.loadings_\n",
    "# factor_loadings_df = pd.DataFrame(factor_loadings, columns=[\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\", \"Factor 6\"])\n",
    "factor_loadings_df = pd.DataFrame(factor_loadings, columns=[f\"Factor {i}\" for i in range(1, 17)])\n",
    "# factor_loadings_df = pd.DataFrame(factor_loadings, columns=[\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\"])\n",
    "# factor_loadings_df.index = data_cleaned.columns\n",
    "factor_loadings_df.index = data_final.columns\n",
    "print(\"Factor Loadings DataFrame:\\n\", factor_loadings_df)\n",
    "\n",
    "# variance explained\n",
    "variance_explained = fa.get_factor_variance()\n",
    "print(\"Variance Explained:\\n\", variance_explained)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03245182, 0.01720842, 0.02686883, 0.01793263, 0.02223959,\n",
       "       0.02282064, 0.01686017, 0.0254489 , 0.01819661, 0.01726673,\n",
       "       0.02757612, 0.01682084, 0.02125505, 0.01760954, 0.01823394,\n",
       "       0.01747789])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_explained[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33626773081510103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_explained[2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor 1:\n",
      " cbcl_q95_p                               0.576580\n",
      "cbcl_q86_p                               0.562006\n",
      "cbcl_q03_p                               0.533706\n",
      "cbcl_q109_p                              0.519226\n",
      "cbcl_q87_p                               0.509411\n",
      "cbcl_q68_p                               0.483531\n",
      "cbcl_q88_p                               0.474791\n",
      "cbcl_q27_p                               0.386796\n",
      "cbcl_q14_p                               0.377646\n",
      "cbcl_q19_p                               0.361401\n",
      "cbcl_q33_p                               0.354315\n",
      "cbcl_q103_p                              0.268592\n",
      "avg_cbcl_q22_p_cbcl_q23_p_cbcl_q28_p     0.265540\n",
      "cbcl_q26_p                               0.244424\n",
      "cbcl_q104_p                              0.235022\n",
      "cbcl_q05_p                               0.181576\n",
      "cbcl_q43_p                               0.178289\n",
      "cbcl_q41_p                               0.166262\n",
      "cbcl_q89_p                               0.155802\n",
      "cbcl_q16_p                               0.149191\n",
      "cbcl_q83_p                               0.148031\n",
      "cbcl_q12_p                               0.144975\n",
      "cbcl_q01_p                               0.140634\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.136975\n",
      "cbcl_q04_p                               0.132776\n",
      "cbcl_q35_p                               0.130499\n",
      "cbcl_q07_p                               0.128027\n",
      "cbcl_q65_p                               0.118337\n",
      "avg_cbcl_q57_p_cbcl_q97_p                0.107472\n",
      "cbcl_q34_p                               0.102004\n",
      "cbcl_q94_p                               0.100273\n",
      "cbcl_q11_p                               0.100188\n",
      "Name: Factor 1, dtype: float64\n",
      "Factor 2:\n",
      " cbcl_q11_p     0.543186\n",
      "cbcl_q29_p     0.449552\n",
      "cbcl_q50_p     0.398570\n",
      "cbcl_q14_p     0.331921\n",
      "cbcl_q19_p     0.294296\n",
      "cbcl_q45_p     0.279018\n",
      "cbcl_q27_p     0.246548\n",
      "cbcl_q75_p     0.239847\n",
      "cbcl_q112_p    0.208093\n",
      "cbcl_q77_p     0.191310\n",
      "cbcl_q12_p     0.186215\n",
      "cbcl_q30_p     0.180539\n",
      "cbcl_q31_p     0.176981\n",
      "cbcl_q71_p     0.174352\n",
      "cbcl_q93_p     0.168793\n",
      "cbcl_q09_p     0.155282\n",
      "cbcl_q32_p     0.130228\n",
      "cbcl_q56d_p    0.127176\n",
      "cbcl_q01_p     0.123036\n",
      "cbcl_q46_p     0.117448\n",
      "cbcl_q64_p     0.117348\n",
      "cbcl_q56e_p    0.116584\n",
      "cbcl_q47_p     0.111428\n",
      "cbcl_q52_p     0.108013\n",
      "Name: Factor 2, dtype: float64\n",
      "Factor 3:\n",
      " avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p     0.620373\n",
      "cbcl_q61_p                               0.563955\n",
      "cbcl_q04_p                               0.559464\n",
      "cbcl_q17_p                               0.432809\n",
      "cbcl_q13_p                               0.423477\n",
      "cbcl_q80_p                               0.413497\n",
      "cbcl_q01_p                               0.370795\n",
      "cbcl_q41_p                               0.358645\n",
      "avg_cbcl_q22_p_cbcl_q23_p_cbcl_q28_p     0.348738\n",
      "cbcl_q43_p                               0.308499\n",
      "cbcl_q24_p                               0.250190\n",
      "cbcl_q26_p                               0.243927\n",
      "cbcl_q74_p                               0.239306\n",
      "cbcl_q09_p                               0.219182\n",
      "cbcl_q79_p                               0.208614\n",
      "cbcl_q05_p                               0.199410\n",
      "cbcl_q39_p                               0.169555\n",
      "cbcl_q64_p                               0.159150\n",
      "cbcl_q03_p                               0.156962\n",
      "cbcl_q35_p                               0.148305\n",
      "cbcl_q19_p                               0.137217\n",
      "cbcl_q62_p                               0.134806\n",
      "cbcl_q46_p                               0.128695\n",
      "cbcl_q11_p                               0.127419\n",
      "cbcl_q12_p                               0.109472\n",
      "cbcl_q72_p                               0.103633\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.100178\n",
      "Name: Factor 3, dtype: float64\n",
      "Factor 4:\n",
      " cbcl_q66_p                               0.478609\n",
      "cbcl_q46_p                               0.477776\n",
      "cbcl_q84_p                               0.389118\n",
      "cbcl_q85_p                               0.358588\n",
      "cbcl_q45_p                               0.328696\n",
      "cbcl_q09_p                               0.312271\n",
      "cbcl_q13_p                               0.304117\n",
      "cbcl_q80_p                               0.286350\n",
      "cbcl_q17_p                               0.236819\n",
      "cbcl_q50_p                               0.216307\n",
      "cbcl_q44_p                               0.195732\n",
      "cbcl_q89_p                               0.179435\n",
      "cbcl_q111_p                              0.175640\n",
      "cbcl_q104_p                              0.166449\n",
      "cbcl_q56h_p                              0.165431\n",
      "cbcl_q58_p                               0.150049\n",
      "cbcl_q112_p                              0.138653\n",
      "cbcl_q42_p                               0.134137\n",
      "cbcl_q83_p                               0.124152\n",
      "cbcl_q67_p                               0.120872\n",
      "cbcl_q05_p                               0.120356\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.106961\n",
      "avg_cbcl_q57_p_cbcl_q97_p                0.103992\n",
      "cbcl_q93_p                               0.101624\n",
      "cbcl_q87_p                               0.100586\n",
      "Name: Factor 4, dtype: float64\n",
      "Factor 5:\n",
      " avg_cbcl_q56c_p_cbcl_q56f_p    0.671208\n",
      "cbcl_q56b_p                    0.610904\n",
      "cbcl_q56a_p                    0.554242\n",
      "cbcl_q56g_p                    0.553294\n",
      "cbcl_q56h_p                    0.422173\n",
      "cbcl_q51_p                     0.407212\n",
      "cbcl_q56e_p                    0.323985\n",
      "cbcl_q56d_p                    0.293442\n",
      "cbcl_q49_p                     0.161501\n",
      "cbcl_q36_p                     0.151977\n",
      "cbcl_q30_p                     0.142850\n",
      "cbcl_q24_p                     0.123910\n",
      "cbcl_q47_p                     0.119262\n",
      "cbcl_q54_p                     0.114384\n",
      "cbcl_q112_p                    0.108377\n",
      "cbcl_q92_p                     0.102961\n",
      "Name: Factor 5, dtype: float64\n",
      "Factor 6:\n",
      " cbcl_q75_p     0.630346\n",
      "cbcl_q65_p     0.525288\n",
      "cbcl_q69_p     0.510136\n",
      "cbcl_q42_p     0.500382\n",
      "cbcl_q111_p    0.456136\n",
      "cbcl_q71_p     0.411863\n",
      "cbcl_q64_p     0.214251\n",
      "cbcl_q05_p     0.188719\n",
      "cbcl_q63_p     0.175561\n",
      "cbcl_q32_p     0.175018\n",
      "cbcl_q80_p     0.170421\n",
      "cbcl_q102_p    0.169801\n",
      "cbcl_q86_p     0.162424\n",
      "cbcl_q83_p     0.158160\n",
      "cbcl_q88_p     0.157136\n",
      "cbcl_q29_p     0.154891\n",
      "cbcl_q17_p     0.140455\n",
      "cbcl_q112_p    0.138921\n",
      "cbcl_q103_p    0.137359\n",
      "cbcl_q24_p     0.123618\n",
      "cbcl_q110_p    0.122197\n",
      "cbcl_q87_p     0.119936\n",
      "cbcl_q77_p     0.116565\n",
      "Name: Factor 6, dtype: float64\n",
      "Factor 7:\n",
      " cbcl_q60_p                   0.726504\n",
      "cbcl_q59_p                   0.709374\n",
      "cbcl_q96_p                   0.532834\n",
      "cbcl_q67_p                   0.278674\n",
      "cbcl_q110_p                  0.169785\n",
      "cbcl_q84_p                   0.159885\n",
      "avg_cbcl_q81_p_cbcl_q82_p    0.148425\n",
      "cbcl_q72_p                   0.137141\n",
      "cbcl_q56g_p                  0.115555\n",
      "cbcl_q15_p                   0.107969\n",
      "cbcl_q01_p                   0.107109\n",
      "cbcl_q64_p                   0.105044\n",
      "Name: Factor 7, dtype: float64\n",
      "Factor 8:\n",
      " cbcl_q35_p     0.594795\n",
      "cbcl_q52_p     0.552469\n",
      "cbcl_q31_p     0.520435\n",
      "cbcl_q32_p     0.486210\n",
      "cbcl_q18_p     0.450086\n",
      "cbcl_q91_p     0.438002\n",
      "cbcl_q33_p     0.392228\n",
      "cbcl_q112_p    0.338311\n",
      "cbcl_q103_p    0.315611\n",
      "cbcl_q34_p     0.300360\n",
      "cbcl_q71_p     0.267188\n",
      "cbcl_q50_p     0.263162\n",
      "cbcl_q30_p     0.256798\n",
      "cbcl_q45_p     0.201332\n",
      "cbcl_q12_p     0.200065\n",
      "cbcl_q67_p     0.190862\n",
      "cbcl_q88_p     0.164731\n",
      "cbcl_q09_p     0.135727\n",
      "cbcl_q15_p     0.129259\n",
      "cbcl_q17_p     0.105124\n",
      "cbcl_q72_p     0.101719\n",
      "cbcl_q87_p     0.100365\n",
      "Name: Factor 8, dtype: float64\n",
      "Factor 9:\n",
      " cbcl_q100_p                             0.687832\n",
      "cbcl_q76_p                              0.613924\n",
      "cbcl_q45_p                              0.316958\n",
      "cbcl_q47_p                              0.310779\n",
      "cbcl_q50_p                              0.289776\n",
      "cbcl_q24_p                              0.273696\n",
      "cbcl_q112_p                             0.227290\n",
      "cbcl_q44_p                              0.213904\n",
      "cbcl_q92_p                              0.193566\n",
      "avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p    0.158004\n",
      "cbcl_q49_p                              0.149044\n",
      "cbcl_q46_p                              0.142740\n",
      "cbcl_q09_p                              0.136880\n",
      "cbcl_q95_p                              0.136730\n",
      "cbcl_q41_p                              0.135627\n",
      "cbcl_q58_p                              0.133149\n",
      "cbcl_q103_p                             0.131863\n",
      "cbcl_q86_p                              0.128459\n",
      "cbcl_q29_p                              0.127652\n",
      "cbcl_q102_p                             0.124129\n",
      "cbcl_q42_p                              0.112911\n",
      "cbcl_q90_p                              0.108359\n",
      "cbcl_q104_p                             0.102741\n",
      "cbcl_q71_p                              0.102356\n",
      "Name: Factor 9, dtype: float64\n",
      "Factor 10:\n",
      " cbcl_q06_p                   0.676911\n",
      "cbcl_q108_p                  0.657440\n",
      "cbcl_q107_p                  0.631127\n",
      "cbcl_q49_p                   0.401699\n",
      "cbcl_q44_p                   0.174298\n",
      "cbcl_q58_p                   0.146819\n",
      "cbcl_q56d_p                  0.140439\n",
      "cbcl_q72_p                   0.138737\n",
      "cbcl_q109_p                  0.137590\n",
      "cbcl_q24_p                   0.118506\n",
      "cbcl_q30_p                   0.117198\n",
      "avg_cbcl_q81_p_cbcl_q82_p    0.115018\n",
      "cbcl_q45_p                   0.111385\n",
      "cbcl_q43_p                   0.106638\n",
      "Name: Factor 10, dtype: float64\n",
      "Factor 11:\n",
      " avg_cbcl_q57_p_cbcl_q97_p                0.576936\n",
      "cbcl_q37_p                               0.549355\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.492618\n",
      "cbcl_q16_p                               0.491314\n",
      "cbcl_q90_p                               0.443915\n",
      "avg_cbcl_q81_p_cbcl_q82_p                0.434601\n",
      "cbcl_q15_p                               0.372845\n",
      "cbcl_q39_p                               0.320676\n",
      "cbcl_q94_p                               0.301440\n",
      "cbcl_q26_p                               0.290615\n",
      "cbcl_q67_p                               0.289189\n",
      "avg_cbcl_q22_p_cbcl_q23_p_cbcl_q28_p     0.282005\n",
      "cbcl_q68_p                               0.241901\n",
      "cbcl_q89_p                               0.234553\n",
      "cbcl_q43_p                               0.234387\n",
      "cbcl_q72_p                               0.219517\n",
      "avg_cbcl_q25_p_cbcl_q48_p                0.179372\n",
      "cbcl_q95_p                               0.170329\n",
      "cbcl_q18_p                               0.163929\n",
      "cbcl_q96_p                               0.163836\n",
      "cbcl_q66_p                               0.148634\n",
      "cbcl_q41_p                               0.143027\n",
      "cbcl_q91_p                               0.138205\n",
      "cbcl_q34_p                               0.122292\n",
      "cbcl_q61_p                               0.122257\n",
      "cbcl_q65_p                               0.121842\n",
      "cbcl_q44_p                               0.115469\n",
      "cbcl_q69_p                               0.111514\n",
      "cbcl_q46_p                               0.102239\n",
      "Name: Factor 11, dtype: float64\n",
      "Factor 12:\n",
      " avg_cbcl_q25_p_cbcl_q48_p    0.540112\n",
      "cbcl_q38_p                   0.501708\n",
      "cbcl_q30_p                   0.355991\n",
      "cbcl_q34_p                   0.354786\n",
      "cbcl_q12_p                   0.309178\n",
      "cbcl_q37_p                   0.307000\n",
      "cbcl_q111_p                  0.261602\n",
      "cbcl_q42_p                   0.228257\n",
      "cbcl_q01_p                   0.201562\n",
      "cbcl_q33_p                   0.198643\n",
      "cbcl_q35_p                   0.177473\n",
      "cbcl_q05_p                   0.171541\n",
      "cbcl_q64_p                   0.166297\n",
      "cbcl_q103_p                  0.144972\n",
      "cbcl_q11_p                   0.124176\n",
      "avg_cbcl_q57_p_cbcl_q97_p    0.112559\n",
      "cbcl_q93_p                   0.110877\n",
      "cbcl_q09_p                   0.109516\n",
      "cbcl_q16_p                   0.106656\n",
      "cbcl_q19_p                   0.103665\n",
      "Name: Factor 12, dtype: float64\n",
      "Factor 13:\n",
      " cbcl_q07_p                              0.560800\n",
      "cbcl_q74_p                              0.511327\n",
      "cbcl_q63_p                              0.433183\n",
      "cbcl_q94_p                              0.376700\n",
      "cbcl_q93_p                              0.328225\n",
      "cbcl_q39_p                              0.260033\n",
      "cbcl_q58_p                              0.256986\n",
      "cbcl_q32_p                              0.227083\n",
      "cbcl_q43_p                              0.225034\n",
      "cbcl_q83_p                              0.220361\n",
      "cbcl_q16_p                              0.217383\n",
      "cbcl_q41_p                              0.209965\n",
      "cbcl_q104_p                             0.196188\n",
      "cbcl_q03_p                              0.195047\n",
      "avg_cbcl_q22_p_cbcl_q23_p_cbcl_q28_p    0.189067\n",
      "cbcl_q44_p                              0.178633\n",
      "cbcl_q27_p                              0.171968\n",
      "cbcl_q92_p                              0.165986\n",
      "cbcl_q09_p                              0.160321\n",
      "cbcl_q85_p                              0.160095\n",
      "cbcl_q19_p                              0.152485\n",
      "cbcl_q26_p                              0.151150\n",
      "cbcl_q71_p                              0.137255\n",
      "cbcl_q31_p                              0.128718\n",
      "avg_cbcl_q08_p_cbcl_q10_p_cbcl_q78_p    0.128510\n",
      "cbcl_q86_p                              0.125324\n",
      "cbcl_q69_p                              0.123710\n",
      "cbcl_q04_p                              0.120441\n",
      "cbcl_q112_p                             0.107418\n",
      "cbcl_q56e_p                             0.104758\n",
      "Name: Factor 13, dtype: float64\n",
      "Factor 14:\n",
      " cbcl_q36_p                               0.663200\n",
      "cbcl_q62_p                               0.600772\n",
      "cbcl_q92_p                               0.384184\n",
      "cbcl_q64_p                               0.261386\n",
      "cbcl_q38_p                               0.252518\n",
      "cbcl_q79_p                               0.218149\n",
      "cbcl_q58_p                               0.207581\n",
      "cbcl_q01_p                               0.192430\n",
      "cbcl_q93_p                               0.188367\n",
      "cbcl_q104_p                              0.166818\n",
      "cbcl_q14_p                               0.165527\n",
      "cbcl_q109_p                              0.156875\n",
      "cbcl_q17_p                               0.151597\n",
      "cbcl_q83_p                               0.147894\n",
      "cbcl_q47_p                               0.147167\n",
      "cbcl_q41_p                               0.138656\n",
      "cbcl_q66_p                               0.135421\n",
      "avg_cbcl_q81_p_cbcl_q82_p                0.126545\n",
      "cbcl_q71_p                               0.125869\n",
      "cbcl_q84_p                               0.119936\n",
      "cbcl_q98_p                               0.118783\n",
      "cbcl_q85_p                               0.107283\n",
      "cbcl_q56a_p                              0.107170\n",
      "cbcl_q80_p                               0.105754\n",
      "cbcl_q68_p                               0.105221\n",
      "cbcl_q44_p                               0.105175\n",
      "avg_cbcl_q106_p_cbcl_q20_p_cbcl_q21_p    0.102238\n",
      "cbcl_q37_p                               0.100825\n",
      "Name: Factor 14, dtype: float64\n",
      "Factor 15:\n",
      " avg_cbcl_q53_p_cbcl_q55_p    0.698567\n",
      "cbcl_q54_p                   0.613983\n",
      "cbcl_q102_p                  0.570556\n",
      "cbcl_q77_p                   0.340252\n",
      "cbcl_q89_p                   0.201638\n",
      "cbcl_q05_p                   0.196389\n",
      "cbcl_q13_p                   0.190324\n",
      "cbcl_q104_p                  0.184692\n",
      "cbcl_q51_p                   0.174982\n",
      "cbcl_q52_p                   0.162977\n",
      "cbcl_q103_p                  0.144714\n",
      "cbcl_q88_p                   0.140188\n",
      "cbcl_q93_p                   0.139091\n",
      "cbcl_q96_p                   0.137867\n",
      "cbcl_q12_p                   0.127688\n",
      "cbcl_q111_p                  0.116082\n",
      "cbcl_q62_p                   0.115492\n",
      "cbcl_q85_p                   0.112962\n",
      "cbcl_q63_p                   0.104060\n",
      "cbcl_q68_p                   0.101833\n",
      "cbcl_q80_p                   0.100797\n",
      "Name: Factor 15, dtype: float64\n",
      "Factor 16:\n",
      " cbcl_q40_p     0.757562\n",
      "cbcl_q70_p     0.751593\n",
      "cbcl_q85_p     0.323489\n",
      "cbcl_q84_p     0.277371\n",
      "cbcl_q47_p     0.261071\n",
      "cbcl_q56g_p    0.176614\n",
      "cbcl_q89_p     0.164558\n",
      "cbcl_q18_p     0.159322\n",
      "cbcl_q72_p     0.127424\n",
      "cbcl_q83_p     0.115054\n",
      "cbcl_q69_p     0.114225\n",
      "cbcl_q39_p     0.113762\n",
      "cbcl_q87_p     0.112470\n",
      "cbcl_q93_p     0.101033\n",
      "Name: Factor 16, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 17):\n",
    "    # all loading values of the factor greater than 0.1\n",
    "    factor_values = factor_loadings_df[f\"Factor {i}\"][factor_loadings_df[f\"Factor {i}\"] > 0.1]\n",
    "\n",
    "    # print(f\"Factor {i}:\\n\", factor_loadings_df[f\"Factor {i}\"].sort_values(ascending=False).head(20))\n",
    "\n",
    "    # descending order\n",
    "    print(f\"Factor {i}:\\n\", factor_values.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_columns = {}\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for i in range(1, 17):\n",
    "    # 筛选出符合条件的列名\n",
    "    factor_values = factor_loadings_df[f\"Factor {i}\"][factor_loadings_df[f\"Factor {i}\"] > 0.1]\n",
    "    # filtered_columns[f\"Factor {i}\"] = factor_values.index.tolist()  # 保存列名（或索引）\n",
    "        # 创建一个临时数据框保存因子名、列名和加载值\n",
    "    temp_df = pd.DataFrame({\n",
    "        f\"Factor {i} Variable\": factor_values.index,     # 存储列名\n",
    "        f\"Factor {i} Loading\": factor_values.values      # 存储加载值\n",
    "    })\n",
    "    \n",
    "    # 将临时数据框合并到结果数据框\n",
    "    result_df = pd.concat([result_df, temp_df], axis=1)\n",
    "\n",
    "\n",
    "# # 保存为CSV文件\n",
    "# filtered_columns_df.to_csv(\"filtered_factor_columns.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit and translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from googletrans import Translator\n",
    "import re\n",
    "import time\n",
    "\n",
    "# 指定源语言和目标语言\n",
    "translator = Translator()\n",
    "\n",
    "# 解析 element.html 文件以获取列名和详细信息\n",
    "with open(\"data/element.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# 创建一个字典来存储列名和对应的详细信息\n",
    "column_details = {}\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# 提取 cbcl_q 列名的正则表达式\n",
    "cbcl_pattern = re.compile(r\"(cbcl_q\\d+[a-z]*_p)\")\n",
    "\n",
    "for i in range(1, 17):\n",
    "    # 筛选出符合条件的加载值\n",
    "    factor_values = factor_loadings_df[f\"Factor {i}\"][factor_loadings_df[f\"Factor {i}\"] > 0.1]\n",
    "    \n",
    "    original_text = []\n",
    "    translated_text = []\n",
    "    for column_name in factor_values.index:\n",
    "        # 查找 column_name 中的所有 cbcl_q 字段\n",
    "        cbcl_items = cbcl_pattern.findall(column_name)  # 提取所有符合 cbcl_qXX_p 或 cbcl_qXXh_p 格式的子串\n",
    "\n",
    "        # 初始化存储每个 cbcl 字段详细信息的列表\n",
    "        original = []\n",
    "        details = []\n",
    "        for cbcl_item in cbcl_items:\n",
    "            # 获取每个 cbcl 字段的详细信息\n",
    "            target = soup.find(lambda tag: tag.name == \"td\" and cbcl_item in tag.get_text(strip=True))\n",
    "            if target:\n",
    "                detail_info = target.find_next(\"td\").get_text(strip=True)\n",
    "                # 保存原始详细信息\n",
    "                original.append(detail_info)\n",
    "                \n",
    "                # 翻译详细信息并添加到结果\n",
    "                try:\n",
    "                    translated_detail = translator.translate(detail_info, src='es', dest='en').text\n",
    "                except AttributeError as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    translated_detail = detail_info\n",
    "                details.append(translated_detail)\n",
    "                time.sleep(0.25)\n",
    "\n",
    "        # 将所有细节合并为单个字符串，并添加到列表中\n",
    "        original_text.append(\"; \".join(original) if original else \"N/A\")\n",
    "        translated_text.append(\"; \".join(details) if details else \"N/A\")\n",
    "\n",
    "    # 创建一个临时数据框保存因子名、列名、加载值和详细信息\n",
    "    temp_df = pd.DataFrame({\n",
    "        f\"Factor {i} Variable\": factor_values.index,  # 存储列名\n",
    "        f\"Factor {i} Loading\": factor_values.values,  # 存储加载值\n",
    "        f\"Factor {i} Detail\": original_text,  # 映射详细信息\n",
    "        f\"Factor {i} Translated_Detail\": translated_text  # 映射翻译后详细信息\n",
    "    })\n",
    "\n",
    "    # 按加载值降序排序\n",
    "    sorted_df = temp_df.sort_values(by=f\"Factor {i} Loading\", ascending=False).reset_index(drop=True)\n",
    "    # 将临时数据框合并到结果数据框\n",
    "    result_df = pd.concat([result_df.reset_index(drop=True), sorted_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 保存结果为CSV文件\n",
    "result_df.to_csv(output_dir + \"/interpretable_information_EN.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from googletrans import Translator\n",
    "import re\n",
    "import time\n",
    "\n",
    "# 指定源语言和目标语言\n",
    "translator = Translator()\n",
    "\n",
    "# 解析 element.html 文件以获取列名和详细信息\n",
    "with open(\"data/element.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "# 创建一个字典来存储列名和对应的详细信息\n",
    "column_details = {}\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# 提取 cbcl_q 列名的正则表达式\n",
    "cbcl_pattern = re.compile(r\"(cbcl_q\\d+[a-z]*_p)\")\n",
    "\n",
    "for i in range(1, 17):\n",
    "    # 筛选出符合条件的加载值\n",
    "    factor_values = factor_loadings_df[f\"Factor {i}\"][factor_loadings_df[f\"Factor {i}\"] > 0.1]\n",
    "    \n",
    "    original_text = []\n",
    "    translated_text = []\n",
    "    for column_name in factor_values.index:\n",
    "        # 查找 column_name 中的所有 cbcl_q 字段\n",
    "        cbcl_items = cbcl_pattern.findall(column_name)  # 提取所有符合 cbcl_qXX_p 或 cbcl_qXXh_p 格式的子串\n",
    "\n",
    "        # 初始化存储每个 cbcl 字段详细信息的列表\n",
    "        original = []\n",
    "        details = []\n",
    "        for cbcl_item in cbcl_items:\n",
    "            # 获取每个 cbcl 字段的详细信息\n",
    "            target = soup.find(lambda tag: tag.name == \"td\" and cbcl_item in tag.get_text(strip=True))\n",
    "            if target:\n",
    "                detail_info = target.find_next(\"td\").get_text(strip=True)\n",
    "                # 保存原始详细信息\n",
    "                original.append(detail_info)\n",
    "                \n",
    "                # 翻译详细信息并添加到结果\n",
    "                try:\n",
    "                    translated_detail = translator.translate(detail_info, src='es', dest='zh-cn').text\n",
    "                except AttributeError as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    translated_detail = detail_info\n",
    "                details.append(translated_detail)\n",
    "                time.sleep(0.25)\n",
    "\n",
    "        # 将所有细节合并为单个字符串，并添加到列表中\n",
    "        original_text.append(\"; \".join(original) if original else \"N/A\")\n",
    "        translated_text.append(\"; \".join(details) if details else \"N/A\")\n",
    "\n",
    "    # 创建一个临时数据框保存因子名、列名、加载值和详细信息\n",
    "    temp_df = pd.DataFrame({\n",
    "        f\"Factor {i} Variable\": factor_values.index,  # 存储列名\n",
    "        f\"Factor {i} Loading\": factor_values.values,  # 存储加载值\n",
    "        f\"Factor {i} Detail\": original_text,  # 映射详细信息\n",
    "        f\"Factor {i} Translated_Detail\": translated_text  # 映射翻译后详细信息\n",
    "    })\n",
    "\n",
    "    # 按加载值降序排序\n",
    "    sorted_df = temp_df.sort_values(by=f\"Factor {i} Loading\", ascending=False).reset_index(drop=True)\n",
    "    # 将临时数据框合并到结果数据框\n",
    "    result_df_CN = pd.concat([result_df.reset_index(drop=True), sorted_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 保存结果为CSV文件\n",
    "result_df_CN.to_csv(output_dir + \"/interpretable_information_CN.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_pattern = re.compile(r\"Factor \\d+ Translated_Detail\")\n",
    "selected_columns = [col for col in result_df.columns if EN_pattern.match(col)]\n",
    "\n",
    "result_df[selected_columns].to_csv(output_dir + \"/details.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate labels with the factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAsso\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "import seaborn as sns\n",
    "from scipy.stats.distributions import halfcauchy\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "#import lasso\n",
    "from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qns = pd.read_csv(r'G:\\ABCD\\script\\trail/trail_tsne_RF/factor analysis/output/NA/data_cleaned.csv')\n",
    "scores = pd.read_csv(r'G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\EFA.csv')\n",
    "\n",
    "clf = Lasso()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PA1</th>\n",
       "      <th>PA2</th>\n",
       "      <th>PA3</th>\n",
       "      <th>PA5</th>\n",
       "      <th>PA4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.590732</td>\n",
       "      <td>-0.675405</td>\n",
       "      <td>-0.734731</td>\n",
       "      <td>-0.707320</td>\n",
       "      <td>0.474278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.463630</td>\n",
       "      <td>-0.569176</td>\n",
       "      <td>-0.943524</td>\n",
       "      <td>-0.853836</td>\n",
       "      <td>0.226067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.561360</td>\n",
       "      <td>0.088093</td>\n",
       "      <td>0.267510</td>\n",
       "      <td>1.555214</td>\n",
       "      <td>0.320209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.393168</td>\n",
       "      <td>0.241235</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>-0.462533</td>\n",
       "      <td>-0.893611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.441081</td>\n",
       "      <td>0.258618</td>\n",
       "      <td>0.827516</td>\n",
       "      <td>-1.152136</td>\n",
       "      <td>-1.017326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>11158</td>\n",
       "      <td>0.704346</td>\n",
       "      <td>2.204722</td>\n",
       "      <td>1.285274</td>\n",
       "      <td>1.950706</td>\n",
       "      <td>1.057855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>11159</td>\n",
       "      <td>0.570672</td>\n",
       "      <td>0.403511</td>\n",
       "      <td>-0.676810</td>\n",
       "      <td>-0.893910</td>\n",
       "      <td>-0.799992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>11160</td>\n",
       "      <td>-0.555718</td>\n",
       "      <td>-0.763623</td>\n",
       "      <td>-0.856959</td>\n",
       "      <td>-0.452341</td>\n",
       "      <td>0.132974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>11161</td>\n",
       "      <td>-0.232042</td>\n",
       "      <td>0.089243</td>\n",
       "      <td>-0.821274</td>\n",
       "      <td>-0.141745</td>\n",
       "      <td>-0.697024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>11162</td>\n",
       "      <td>-0.214842</td>\n",
       "      <td>-0.530197</td>\n",
       "      <td>0.101790</td>\n",
       "      <td>-0.787224</td>\n",
       "      <td>-0.200235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       PA1       PA2       PA3       PA5       PA4\n",
       "0               1 -0.590732 -0.675405 -0.734731 -0.707320  0.474278\n",
       "1               2 -0.463630 -0.569176 -0.943524 -0.853836  0.226067\n",
       "2               3 -0.561360  0.088093  0.267510  1.555214  0.320209\n",
       "3               4 -0.393168  0.241235  0.337544 -0.462533 -0.893611\n",
       "4               5  2.441081  0.258618  0.827516 -1.152136 -1.017326\n",
       "...           ...       ...       ...       ...       ...       ...\n",
       "10647       11158  0.704346  2.204722  1.285274  1.950706  1.057855\n",
       "10648       11159  0.570672  0.403511 -0.676810 -0.893910 -0.799992\n",
       "10649       11160 -0.555718 -0.763623 -0.856959 -0.452341  0.132974\n",
       "10650       11161 -0.232042  0.089243 -0.821274 -0.141745 -0.697024\n",
       "10651       11162 -0.214842 -0.530197  0.101790 -0.787224 -0.200235\n",
       "\n",
       "[10652 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>cbcl_q01_p</th>\n",
       "      <th>cbcl_q03_p</th>\n",
       "      <th>cbcl_q04_p</th>\n",
       "      <th>cbcl_q05_p</th>\n",
       "      <th>cbcl_q06_p</th>\n",
       "      <th>cbcl_q07_p</th>\n",
       "      <th>cbcl_q08_p</th>\n",
       "      <th>cbcl_q09_p</th>\n",
       "      <th>...</th>\n",
       "      <th>cbcl_q102_p</th>\n",
       "      <th>cbcl_q103_p</th>\n",
       "      <th>cbcl_q104_p</th>\n",
       "      <th>cbcl_q106_p</th>\n",
       "      <th>cbcl_q107_p</th>\n",
       "      <th>cbcl_q108_p</th>\n",
       "      <th>cbcl_q109_p</th>\n",
       "      <th>cbcl_q110_p</th>\n",
       "      <th>cbcl_q111_p</th>\n",
       "      <th>cbcl_q112_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>11158</td>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>11159</td>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>11160</td>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>11161</td>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>11162</td>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    src_subject_id  cbcl_q01_p  cbcl_q03_p  cbcl_q04_p  \\\n",
       "0               1  NDAR_INV003RTV85           0           0           0   \n",
       "1               2  NDAR_INV005V6D2C           0           0           0   \n",
       "2               3  NDAR_INV007W6H7B           0           1           0   \n",
       "3               4  NDAR_INV00BD7VDC           0           1           1   \n",
       "4               5  NDAR_INV00CY2MDM           1           2           1   \n",
       "...           ...               ...         ...         ...         ...   \n",
       "10647       11158  NDAR_INVZZLZCKAY           0           2           1   \n",
       "10648       11159  NDAR_INVZZNX6W2P           0           1           0   \n",
       "10649       11160  NDAR_INVZZPKBDAC           0           0           0   \n",
       "10650       11161  NDAR_INVZZZ2ALR6           0           1           0   \n",
       "10651       11162  NDAR_INVZZZNB0XC           1           0           1   \n",
       "\n",
       "       cbcl_q05_p  cbcl_q06_p  cbcl_q07_p  cbcl_q08_p  cbcl_q09_p  ...  \\\n",
       "0               0           0           0           0           0  ...   \n",
       "1               0           0           0           0           0  ...   \n",
       "2               0           0           1           0           1  ...   \n",
       "3               0           0           1           1           1  ...   \n",
       "4               0           0           1           2           0  ...   \n",
       "...           ...         ...         ...         ...         ...  ...   \n",
       "10647           0           0           0           1           1  ...   \n",
       "10648           1           0           0           0           0  ...   \n",
       "10649           0           0           0           0           0  ...   \n",
       "10650           0           0           0           0           0  ...   \n",
       "10651           0           0           0           0           1  ...   \n",
       "\n",
       "       cbcl_q102_p  cbcl_q103_p  cbcl_q104_p  cbcl_q106_p  cbcl_q107_p  \\\n",
       "0                0            0            0            0            0   \n",
       "1                0            0            0            0            0   \n",
       "2                0            0            1            0            0   \n",
       "3                0            0            1            0            0   \n",
       "4                0            1            0            0            0   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10647            1            0            1            0            0   \n",
       "10648            0            1            0            0            0   \n",
       "10649            0            0            0            0            0   \n",
       "10650            0            0            0            0            0   \n",
       "10651            0            0            0            0            0   \n",
       "\n",
       "       cbcl_q108_p  cbcl_q109_p  cbcl_q110_p  cbcl_q111_p  cbcl_q112_p  \n",
       "0                1            0            0            0            0  \n",
       "1                0            0            0            0            0  \n",
       "2                1            0            0            0            1  \n",
       "3                0            0            0            0            1  \n",
       "4                0            0            0            0            0  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "10647            0            1            0            1            1  \n",
       "10648            0            1            0            0            0  \n",
       "10649            0            0            0            0            0  \n",
       "10650            0            0            0            0            0  \n",
       "10651            0            0            0            0            0  \n",
       "\n",
       "[10652 rows x 116 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_col</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PA1</th>\n",
       "      <th>PA2</th>\n",
       "      <th>PA3</th>\n",
       "      <th>PA5</th>\n",
       "      <th>PA4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.590732</td>\n",
       "      <td>-0.675405</td>\n",
       "      <td>-0.734731</td>\n",
       "      <td>-0.707320</td>\n",
       "      <td>0.474278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.463630</td>\n",
       "      <td>-0.569176</td>\n",
       "      <td>-0.943524</td>\n",
       "      <td>-0.853836</td>\n",
       "      <td>0.226067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.561360</td>\n",
       "      <td>0.088093</td>\n",
       "      <td>0.267510</td>\n",
       "      <td>1.555214</td>\n",
       "      <td>0.320209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.393168</td>\n",
       "      <td>0.241235</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>-0.462533</td>\n",
       "      <td>-0.893611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2.441081</td>\n",
       "      <td>0.258618</td>\n",
       "      <td>0.827516</td>\n",
       "      <td>-1.152136</td>\n",
       "      <td>-1.017326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   new_col  Unnamed: 0       PA1       PA2       PA3       PA5       PA4\n",
       "0      NaN           1 -0.590732 -0.675405 -0.734731 -0.707320  0.474278\n",
       "1      NaN           2 -0.463630 -0.569176 -0.943524 -0.853836  0.226067\n",
       "2      NaN           3 -0.561360  0.088093  0.267510  1.555214  0.320209\n",
       "3      NaN           4 -0.393168  0.241235  0.337544 -0.462533 -0.893611\n",
       "4      NaN           5  2.441081  0.258618  0.827516 -1.152136 -1.017326"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores = encoded_df\n",
    "scores.insert(0, 'new_col',np.nan)\n",
    "# scores.insert(1, 'new_col1',np.nan)\n",
    "scores.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.001, 0.01, 0.05, 0.075, 0.1, 0.125, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:06<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "r2_values = np.empty((5, len(alpha_values)))\n",
    "n_items = np.empty(len(alpha_values))\n",
    "\n",
    "for n, alpha in enumerate(tqdm(alpha_values)):\n",
    "    clf = Lasso(alpha=alpha)\n",
    "    clf.fit(qns.iloc[:, 2:], scores.iloc[:, 2:])\n",
    "    pred = cross_val_predict(clf, qns.iloc[:, 2:], scores.iloc[:, 2:], cv=5)\n",
    "    for i in range(5):\n",
    "        r2_values[i, n] = r2_score(scores.iloc[:, i+2], pred[:, i])\n",
    "    n_items[n] = np.any(clf.coef_.T != 0, axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = matplotlib.font_manager.FontProperties(fname=\"c:\\\\windows\\\\fonts\\\\nunitosans-light.ttf\")\n",
    "# matplotlib.rcParams['font.family'] = prop.get_name()\n",
    "matplotlib.rcParams['font.weight'] = 'light'\n",
    "matplotlib.rcParams['axes.facecolor'] = '#fbfbfb'\n",
    "\n",
    "pal = ['#4f4f4f', '#B80044', '#0e79b2', '#f9a800', '#00a087']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_2816\\1587163178.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax2.set_xticklabels(alpha_values)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(dpi=100, facecolor='white')\n",
    "for i in range(3):\n",
    "    ax.plot(n_items, r2_values[i, :], label='Factor {0}'.format(i+1), color=pal[i])\n",
    "ax.set_xlabel(\"Number of items\")\n",
    "ax.set_ylabel(\"$R^2$\")\n",
    "ax.legend()\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xticklabels(alpha_values)\n",
    "ax2.set_xticks(n_items)\n",
    "ax.axvline(63, color='#8c8a8a', linestyle=':')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('../figures/number_of_questions.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Lasso(alpha=0.125)\n",
    "pred = cross_val_predict(clf, qns.iloc[:, 2:], scores.iloc[:, 2:], cv=5)\n",
    "clf.fit(qns.iloc[:, 2:], scores.iloc[:, 2:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x350 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 5, figsize=(16, 3.5), dpi=100, facecolor='white')\n",
    "\n",
    "factors = ['V1', 'V2', 'V3','V4' ,'V5']\n",
    "\n",
    "for i in range(5):\n",
    "    sns.regplot(x = scores.iloc[:, i+2], y =  pred[:, i], ax=ax[i], color=pal[i], scatter_kws={'alpha': 0.5})\n",
    "    ax[i].set_title(factors[i] + '\\n$R^2$ = {0}'.format(np.round(r2_score(scores.iloc[:, i+2], pred[:, i]), 5)), fontweight='light')\n",
    "    ax[i].set_xlabel('True score')\n",
    "    ax[i].set_ylabel('Predicted score')\n",
    "    \n",
    "plt.tight_layout()\n",
    "# plt.savefig('../figures/factor_prediction.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(83.22222222222221, 0.5, 'Factor')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x150 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(dpi=100, figsize=(9, 1.5), facecolor='white')\n",
    "sns.heatmap(clf.coef_, cmap='Blues', yticklabels=['V1', 'V2', 'V3', 'V4', 'V5'])\n",
    "plt.xlabel(\"Question number\")\n",
    "plt.ylabel(\"Factor\")\n",
    "# plt.savefig('../figures/all_question_coefs.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(clf.coef_.T != 0, axis=1).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1920x240 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(24, 3), dpi=80, facecolor='white')\n",
    "coefs = clf.coef_.T\n",
    "sns.heatmap(coefs[np.any(coefs != 0, axis=1)].T, cmap=\"Blues\", xticklabels=qns.columns[2:][np.any(coefs != 0, axis=1)],\n",
    "           yticklabels=['Anxiety/depression', 'Compulsivity', 'Social withdrawal'], linewidths=.5);\n",
    "plt.xticks(rotation=45);\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../figures/retained_questions.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparse autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbcl_q01_p</th>\n",
       "      <th>cbcl_q03_p</th>\n",
       "      <th>cbcl_q04_p</th>\n",
       "      <th>cbcl_q05_p</th>\n",
       "      <th>cbcl_q06_p</th>\n",
       "      <th>cbcl_q07_p</th>\n",
       "      <th>cbcl_q08_p</th>\n",
       "      <th>cbcl_q09_p</th>\n",
       "      <th>cbcl_q10_p</th>\n",
       "      <th>cbcl_q11_p</th>\n",
       "      <th>...</th>\n",
       "      <th>cbcl_q102_p</th>\n",
       "      <th>cbcl_q103_p</th>\n",
       "      <th>cbcl_q104_p</th>\n",
       "      <th>cbcl_q106_p</th>\n",
       "      <th>cbcl_q107_p</th>\n",
       "      <th>cbcl_q108_p</th>\n",
       "      <th>cbcl_q109_p</th>\n",
       "      <th>cbcl_q110_p</th>\n",
       "      <th>cbcl_q111_p</th>\n",
       "      <th>cbcl_q112_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8942</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8943</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8944</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8945</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8946</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8947 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cbcl_q01_p  cbcl_q03_p  cbcl_q04_p  cbcl_q05_p  cbcl_q06_p  cbcl_q07_p  \\\n",
       "0              0           0           0           1           0           0   \n",
       "1              0           0           0           0           0           0   \n",
       "2              0           0           0           0           0           0   \n",
       "3              1           0           1           0           0           1   \n",
       "4              0           0           0           0           0           0   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "8942           0           1           0           0           0           1   \n",
       "8943           1           1           1           0           0           0   \n",
       "8944           0           0           1           0           0           0   \n",
       "8945           0           0           0           0           0           0   \n",
       "8946           0           0           0           0           0           0   \n",
       "\n",
       "      cbcl_q08_p  cbcl_q09_p  cbcl_q10_p  cbcl_q11_p  ...  cbcl_q102_p  \\\n",
       "0              0           0           1           0  ...            0   \n",
       "1              0           0           0           0  ...            0   \n",
       "2              0           0           0           0  ...            0   \n",
       "3              0           1           0           0  ...            0   \n",
       "4              1           0           2           0  ...            0   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "8942           0           1           0           1  ...            0   \n",
       "8943           1           0           1           1  ...            1   \n",
       "8944           0           0           1           0  ...            0   \n",
       "8945           0           0           0           0  ...            0   \n",
       "8946           0           0           0           0  ...            0   \n",
       "\n",
       "      cbcl_q103_p  cbcl_q104_p  cbcl_q106_p  cbcl_q107_p  cbcl_q108_p  \\\n",
       "0               0            0            0            0            1   \n",
       "1               0            0            0            0            0   \n",
       "2               0            0            0            0            0   \n",
       "3               0            0            0            0            0   \n",
       "4               0            0            0            0            0   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "8942            0            0            0            0            1   \n",
       "8943            1            0            0            0            0   \n",
       "8944            0            0            0            0            0   \n",
       "8945            0            0            0            0            0   \n",
       "8946            0            0            0            0            0   \n",
       "\n",
       "      cbcl_q109_p  cbcl_q110_p  cbcl_q111_p  cbcl_q112_p  \n",
       "0               0            0            0            0  \n",
       "1               0            0            0            0  \n",
       "2               0            0            0            0  \n",
       "3               0            0            0            0  \n",
       "4               0            0            0            0  \n",
       "...           ...          ...          ...          ...  \n",
       "8942            0            0            0            0  \n",
       "8943            1            0            0            1  \n",
       "8944            1            0            0            1  \n",
       "8945            0            0            0            0  \n",
       "8946            0            0            0            0  \n",
       "\n",
       "[8947 rows x 114 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qns.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0509 - mae: 0.1158 - val_loss: 0.0328 - val_mae: 0.1147\n",
      "Epoch 2/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 0.0361 - mae: 0.1183 - val_loss: 0.0308 - val_mae: 0.1029\n",
      "Epoch 3/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.0347 - mae: 0.1109 - val_loss: 0.0304 - val_mae: 0.1009\n",
      "Epoch 4/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.0342 - mae: 0.1094 - val_loss: 0.0303 - val_mae: 0.0999\n",
      "Epoch 5/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0341 - mae: 0.1087 - val_loss: 0.0302 - val_mae: 0.1003\n",
      "Epoch 6/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.0347 - mae: 0.1105 - val_loss: 0.0301 - val_mae: 0.1005\n",
      "Epoch 7/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 0.0341 - mae: 0.1095 - val_loss: 0.0301 - val_mae: 0.1006\n",
      "Epoch 8/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.0338 - mae: 0.1088 - val_loss: 0.0299 - val_mae: 0.0997\n",
      "Epoch 9/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0334 - mae: 0.1076 - val_loss: 0.0292 - val_mae: 0.0982\n",
      "Epoch 10/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 0.0327 - mae: 0.1063 - val_loss: 0.0289 - val_mae: 0.0972\n",
      "Epoch 11/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 0.0327 - mae: 0.1063 - val_loss: 0.0286 - val_mae: 0.0961\n",
      "Epoch 12/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.0320 - mae: 0.1044 - val_loss: 0.0282 - val_mae: 0.0952\n",
      "Epoch 13/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 0.0313 - mae: 0.1027 - val_loss: 0.0278 - val_mae: 0.0947\n",
      "Epoch 14/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.0314 - mae: 0.1027 - val_loss: 0.0276 - val_mae: 0.0938\n",
      "Epoch 15/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0311 - mae: 0.1020 - val_loss: 0.0274 - val_mae: 0.0939\n",
      "Epoch 16/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.0302 - mae: 0.1001 - val_loss: 0.0273 - val_mae: 0.0931\n",
      "Epoch 17/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.0307 - mae: 0.1012 - val_loss: 0.0272 - val_mae: 0.0928\n",
      "Epoch 18/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.0306 - mae: 0.1013 - val_loss: 0.0271 - val_mae: 0.0929\n",
      "Epoch 19/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0306 - mae: 0.1013 - val_loss: 0.0270 - val_mae: 0.0927\n",
      "Epoch 20/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.0306 - mae: 0.1012 - val_loss: 0.0269 - val_mae: 0.0926\n",
      "Epoch 21/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 0.0302 - mae: 0.1003 - val_loss: 0.0267 - val_mae: 0.0922\n",
      "Epoch 22/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 0.0295 - mae: 0.0984 - val_loss: 0.0266 - val_mae: 0.0919\n",
      "Epoch 23/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 0.0296 - mae: 0.0985 - val_loss: 0.0265 - val_mae: 0.0917\n",
      "Epoch 24/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.0297 - mae: 0.0988 - val_loss: 0.0264 - val_mae: 0.0916\n",
      "Epoch 25/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0291 - mae: 0.0979 - val_loss: 0.0264 - val_mae: 0.0916\n",
      "Epoch 26/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 0.0296 - mae: 0.0988 - val_loss: 0.0263 - val_mae: 0.0912\n",
      "Epoch 27/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.0290 - mae: 0.0975 - val_loss: 0.0263 - val_mae: 0.0909\n",
      "Epoch 28/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.0295 - mae: 0.0988 - val_loss: 0.0262 - val_mae: 0.0911\n",
      "Epoch 29/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0292 - mae: 0.0981 - val_loss: 0.0262 - val_mae: 0.0909\n",
      "Epoch 30/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 0.0296 - mae: 0.0991 - val_loss: 0.0262 - val_mae: 0.0908\n",
      "Epoch 31/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 0.0292 - mae: 0.0985 - val_loss: 0.0261 - val_mae: 0.0905\n",
      "Epoch 32/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 0.0288 - mae: 0.0972 - val_loss: 0.0261 - val_mae: 0.0907\n",
      "Epoch 33/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.0294 - mae: 0.0986 - val_loss: 0.0261 - val_mae: 0.0905\n",
      "Epoch 34/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 0.0295 - mae: 0.0993 - val_loss: 0.0260 - val_mae: 0.0910\n",
      "Epoch 35/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0288 - mae: 0.0975 - val_loss: 0.0260 - val_mae: 0.0909\n",
      "Epoch 36/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0289 - mae: 0.0979 - val_loss: 0.0260 - val_mae: 0.0909\n",
      "Epoch 37/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 0.0289 - mae: 0.0980 - val_loss: 0.0260 - val_mae: 0.0909\n",
      "Epoch 38/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0286 - mae: 0.0970 - val_loss: 0.0260 - val_mae: 0.0902\n",
      "Epoch 39/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 0.0287 - mae: 0.0972 - val_loss: 0.0259 - val_mae: 0.0909\n",
      "Epoch 40/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.0288 - mae: 0.0975 - val_loss: 0.0259 - val_mae: 0.0911\n",
      "Epoch 41/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.0293 - mae: 0.0987 - val_loss: 0.0259 - val_mae: 0.0910\n",
      "Epoch 42/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.0295 - mae: 0.0992 - val_loss: 0.0259 - val_mae: 0.0907\n",
      "Epoch 43/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 0.0286 - mae: 0.0975 - val_loss: 0.0258 - val_mae: 0.0913\n",
      "Epoch 44/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0294 - mae: 0.0993 - val_loss: 0.0258 - val_mae: 0.0905\n",
      "Epoch 45/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.0294 - mae: 0.0994 - val_loss: 0.0258 - val_mae: 0.0902\n",
      "Epoch 46/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - loss: 0.0292 - mae: 0.0987 - val_loss: 0.0258 - val_mae: 0.0915\n",
      "Epoch 47/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0285 - mae: 0.0972 - val_loss: 0.0258 - val_mae: 0.0912\n",
      "Epoch 48/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0290 - mae: 0.0987 - val_loss: 0.0258 - val_mae: 0.0910\n",
      "Epoch 49/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 0.0285 - mae: 0.0974 - val_loss: 0.0257 - val_mae: 0.0910\n",
      "Epoch 50/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0284 - mae: 0.0970 - val_loss: 0.0257 - val_mae: 0.0908\n",
      "Epoch 51/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.0291 - mae: 0.0988 - val_loss: 0.0257 - val_mae: 0.0907\n",
      "Epoch 52/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 0.0289 - mae: 0.0985 - val_loss: 0.0257 - val_mae: 0.0908\n",
      "Epoch 53/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0284 - mae: 0.0972 - val_loss: 0.0257 - val_mae: 0.0906\n",
      "Epoch 54/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0287 - mae: 0.0983 - val_loss: 0.0257 - val_mae: 0.0908\n",
      "Epoch 55/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0286 - mae: 0.0979 - val_loss: 0.0257 - val_mae: 0.0905\n",
      "Epoch 56/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 0.0291 - mae: 0.0991 - val_loss: 0.0257 - val_mae: 0.0908\n",
      "Epoch 57/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0286 - mae: 0.0979 - val_loss: 0.0256 - val_mae: 0.0905\n",
      "Epoch 58/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0288 - mae: 0.0985 - val_loss: 0.0256 - val_mae: 0.0910\n",
      "Epoch 59/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0287 - mae: 0.0983 - val_loss: 0.0256 - val_mae: 0.0909\n",
      "Epoch 60/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0288 - mae: 0.0986 - val_loss: 0.0256 - val_mae: 0.0906\n",
      "Epoch 61/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 0.0285 - mae: 0.0981 - val_loss: 0.0256 - val_mae: 0.0910\n",
      "Epoch 62/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0285 - mae: 0.0980 - val_loss: 0.0256 - val_mae: 0.0907\n",
      "Epoch 63/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 0.0284 - mae: 0.0972 - val_loss: 0.0256 - val_mae: 0.0910\n",
      "Epoch 64/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 0.0289 - mae: 0.0987 - val_loss: 0.0256 - val_mae: 0.0910\n",
      "Epoch 65/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.0285 - mae: 0.0980 - val_loss: 0.0256 - val_mae: 0.0915\n",
      "Epoch 66/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0290 - mae: 0.0989 - val_loss: 0.0255 - val_mae: 0.0908\n",
      "Epoch 67/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0279 - mae: 0.0964 - val_loss: 0.0255 - val_mae: 0.0909\n",
      "Epoch 68/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 0.0288 - mae: 0.0989 - val_loss: 0.0255 - val_mae: 0.0909\n",
      "Epoch 69/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 0.0287 - mae: 0.0986 - val_loss: 0.0255 - val_mae: 0.0910\n",
      "Epoch 70/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0287 - mae: 0.0985 - val_loss: 0.0255 - val_mae: 0.0910\n",
      "Epoch 71/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0285 - mae: 0.0981 - val_loss: 0.0255 - val_mae: 0.0904\n",
      "Epoch 72/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0291 - mae: 0.0998 - val_loss: 0.0255 - val_mae: 0.0909\n",
      "Epoch 73/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0283 - mae: 0.0973 - val_loss: 0.0255 - val_mae: 0.0906\n",
      "Epoch 74/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0284 - mae: 0.0974 - val_loss: 0.0255 - val_mae: 0.0907\n",
      "Epoch 75/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.0280 - mae: 0.0969 - val_loss: 0.0255 - val_mae: 0.0909\n",
      "Epoch 76/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.0283 - mae: 0.0977 - val_loss: 0.0254 - val_mae: 0.0908\n",
      "Epoch 77/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0282 - mae: 0.0974 - val_loss: 0.0254 - val_mae: 0.0907\n",
      "Epoch 78/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 0.0282 - mae: 0.0974 - val_loss: 0.0254 - val_mae: 0.0905\n",
      "Epoch 79/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.0287 - mae: 0.0987 - val_loss: 0.0254 - val_mae: 0.0910\n",
      "Epoch 80/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 0.0287 - mae: 0.0986 - val_loss: 0.0254 - val_mae: 0.0909\n",
      "Epoch 81/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.0287 - mae: 0.0990 - val_loss: 0.0254 - val_mae: 0.0907\n",
      "Epoch 82/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0282 - mae: 0.0974 - val_loss: 0.0254 - val_mae: 0.0901\n",
      "Epoch 83/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.0283 - mae: 0.0979 - val_loss: 0.0254 - val_mae: 0.0905\n",
      "Epoch 84/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.0289 - mae: 0.0994 - val_loss: 0.0254 - val_mae: 0.0904\n",
      "Epoch 85/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 0.0286 - mae: 0.0984 - val_loss: 0.0254 - val_mae: 0.0912\n",
      "Epoch 86/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0289 - mae: 0.0994 - val_loss: 0.0254 - val_mae: 0.0905\n",
      "Epoch 87/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.0280 - mae: 0.0970 - val_loss: 0.0254 - val_mae: 0.0912\n",
      "Epoch 88/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 0.0286 - mae: 0.0984 - val_loss: 0.0254 - val_mae: 0.0906\n",
      "Epoch 89/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.0279 - mae: 0.0970 - val_loss: 0.0254 - val_mae: 0.0910\n",
      "Epoch 90/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0278 - mae: 0.0965 - val_loss: 0.0253 - val_mae: 0.0910\n",
      "Epoch 91/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.0286 - mae: 0.0989 - val_loss: 0.0253 - val_mae: 0.0913\n",
      "Epoch 92/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0287 - mae: 0.0990 - val_loss: 0.0253 - val_mae: 0.0909\n",
      "Epoch 93/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0284 - mae: 0.0978 - val_loss: 0.0253 - val_mae: 0.0907\n",
      "Epoch 94/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0284 - mae: 0.0981 - val_loss: 0.0253 - val_mae: 0.0915\n",
      "Epoch 95/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.0286 - mae: 0.0991 - val_loss: 0.0253 - val_mae: 0.0914\n",
      "Epoch 96/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0279 - mae: 0.0973 - val_loss: 0.0253 - val_mae: 0.0911\n",
      "Epoch 97/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 0.0288 - mae: 0.0993 - val_loss: 0.0253 - val_mae: 0.0897\n",
      "Epoch 98/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0282 - mae: 0.0978 - val_loss: 0.0253 - val_mae: 0.0910\n",
      "Epoch 99/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0281 - mae: 0.0979 - val_loss: 0.0253 - val_mae: 0.0907\n",
      "Epoch 100/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.0281 - mae: 0.0976 - val_loss: 0.0253 - val_mae: 0.0902\n",
      "Epoch 101/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.0282 - mae: 0.0978 - val_loss: 0.0253 - val_mae: 0.0910\n",
      "Epoch 102/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0285 - mae: 0.0988 - val_loss: 0.0252 - val_mae: 0.0909\n",
      "Epoch 103/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.0283 - mae: 0.0977 - val_loss: 0.0253 - val_mae: 0.0901\n",
      "Epoch 104/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.0288 - mae: 0.0991 - val_loss: 0.0253 - val_mae: 0.0912\n",
      "Epoch 105/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 0.0283 - mae: 0.0984 - val_loss: 0.0252 - val_mae: 0.0901\n",
      "Epoch 106/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0282 - mae: 0.0979 - val_loss: 0.0253 - val_mae: 0.0917\n",
      "Epoch 107/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 0.0283 - mae: 0.0986 - val_loss: 0.0252 - val_mae: 0.0910\n",
      "Epoch 108/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0282 - mae: 0.0976 - val_loss: 0.0252 - val_mae: 0.0907\n",
      "Epoch 109/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.0287 - mae: 0.0994 - val_loss: 0.0252 - val_mae: 0.0905\n",
      "Epoch 110/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0287 - mae: 0.0993 - val_loss: 0.0252 - val_mae: 0.0908\n",
      "Epoch 111/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.0282 - mae: 0.0981 - val_loss: 0.0253 - val_mae: 0.0916\n",
      "Epoch 112/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.0284 - mae: 0.0986 - val_loss: 0.0252 - val_mae: 0.0907\n",
      "Epoch 113/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.0284 - mae: 0.0985 - val_loss: 0.0252 - val_mae: 0.0906\n",
      "Epoch 114/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.0282 - mae: 0.0982 - val_loss: 0.0252 - val_mae: 0.0909\n",
      "Epoch 115/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 0.0278 - mae: 0.0973 - val_loss: 0.0252 - val_mae: 0.0900\n",
      "Epoch 116/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 0.0282 - mae: 0.0980 - val_loss: 0.0252 - val_mae: 0.0899\n",
      "Epoch 117/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0284 - mae: 0.0985 - val_loss: 0.0252 - val_mae: 0.0906\n",
      "Epoch 118/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.0284 - mae: 0.0986 - val_loss: 0.0251 - val_mae: 0.0906\n",
      "Epoch 119/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.0283 - mae: 0.0981 - val_loss: 0.0252 - val_mae: 0.0906\n",
      "Epoch 120/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 0.0287 - mae: 0.0992 - val_loss: 0.0252 - val_mae: 0.0910\n",
      "Epoch 121/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 0.0283 - mae: 0.0986 - val_loss: 0.0252 - val_mae: 0.0911\n",
      "Epoch 122/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.0281 - mae: 0.0980 - val_loss: 0.0252 - val_mae: 0.0916\n",
      "Epoch 123/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.0281 - mae: 0.0980 - val_loss: 0.0251 - val_mae: 0.0907\n",
      "Epoch 124/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0283 - mae: 0.0982 - val_loss: 0.0251 - val_mae: 0.0903\n",
      "Epoch 125/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 0.0282 - mae: 0.0986 - val_loss: 0.0251 - val_mae: 0.0909\n",
      "Epoch 126/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0282 - mae: 0.0983 - val_loss: 0.0251 - val_mae: 0.0910\n",
      "Epoch 127/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 0.0277 - mae: 0.0973 - val_loss: 0.0251 - val_mae: 0.0913\n",
      "Epoch 128/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.0287 - mae: 0.0997 - val_loss: 0.0251 - val_mae: 0.0907\n",
      "Epoch 129/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0277 - mae: 0.0972 - val_loss: 0.0251 - val_mae: 0.0908\n",
      "Epoch 130/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0286 - mae: 0.0991 - val_loss: 0.0252 - val_mae: 0.0891\n",
      "Epoch 131/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0285 - mae: 0.0988 - val_loss: 0.0251 - val_mae: 0.0903\n",
      "Epoch 132/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0279 - mae: 0.0974 - val_loss: 0.0251 - val_mae: 0.0907\n",
      "Epoch 133/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.0283 - mae: 0.0985 - val_loss: 0.0251 - val_mae: 0.0906\n",
      "Epoch 134/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0283 - mae: 0.0986 - val_loss: 0.0251 - val_mae: 0.0910\n",
      "Epoch 135/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.0281 - mae: 0.0980 - val_loss: 0.0251 - val_mae: 0.0901\n",
      "Epoch 136/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0283 - mae: 0.0986 - val_loss: 0.0251 - val_mae: 0.0907\n",
      "Epoch 137/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 0.0283 - mae: 0.0991 - val_loss: 0.0251 - val_mae: 0.0906\n",
      "Epoch 138/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0282 - mae: 0.0982 - val_loss: 0.0251 - val_mae: 0.0906\n",
      "Epoch 139/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0283 - mae: 0.0984 - val_loss: 0.0250 - val_mae: 0.0902\n",
      "Epoch 140/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.0278 - mae: 0.0970 - val_loss: 0.0251 - val_mae: 0.0902\n",
      "Epoch 141/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.0285 - mae: 0.0991 - val_loss: 0.0251 - val_mae: 0.0901\n",
      "Epoch 142/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0277 - mae: 0.0967 - val_loss: 0.0251 - val_mae: 0.0909\n",
      "Epoch 143/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0281 - mae: 0.0982 - val_loss: 0.0253 - val_mae: 0.0900\n",
      "Epoch 144/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 0.0283 - mae: 0.0976 - val_loss: 0.0250 - val_mae: 0.0906\n",
      "Epoch 145/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 0.0274 - mae: 0.0963 - val_loss: 0.0250 - val_mae: 0.0905\n",
      "Epoch 146/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0279 - mae: 0.0977 - val_loss: 0.0250 - val_mae: 0.0905\n",
      "Epoch 147/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 0.0280 - mae: 0.0975 - val_loss: 0.0250 - val_mae: 0.0904\n",
      "Epoch 148/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0281 - mae: 0.0981 - val_loss: 0.0250 - val_mae: 0.0907\n",
      "Epoch 149/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 0.0280 - mae: 0.0976 - val_loss: 0.0250 - val_mae: 0.0904\n",
      "Epoch 150/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0278 - mae: 0.0974 - val_loss: 0.0250 - val_mae: 0.0902\n",
      "Epoch 151/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.0280 - mae: 0.0978 - val_loss: 0.0250 - val_mae: 0.0906\n",
      "Epoch 152/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.0286 - mae: 0.0998 - val_loss: 0.0253 - val_mae: 0.0910\n",
      "Epoch 153/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0283 - mae: 0.0982 - val_loss: 0.0250 - val_mae: 0.0903\n",
      "Epoch 154/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0282 - mae: 0.0981 - val_loss: 0.0250 - val_mae: 0.0906\n",
      "Epoch 155/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0279 - mae: 0.0979 - val_loss: 0.0250 - val_mae: 0.0902\n",
      "Epoch 156/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 0.0283 - mae: 0.0983 - val_loss: 0.0250 - val_mae: 0.0907\n",
      "Epoch 157/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.0284 - mae: 0.0991 - val_loss: 0.0250 - val_mae: 0.0904\n",
      "Epoch 158/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0278 - mae: 0.0973 - val_loss: 0.0250 - val_mae: 0.0900\n",
      "Epoch 159/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0276 - mae: 0.0969 - val_loss: 0.0250 - val_mae: 0.0891\n",
      "Epoch 160/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.0276 - mae: 0.0965 - val_loss: 0.0250 - val_mae: 0.0909\n",
      "Epoch 161/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.0281 - mae: 0.0982 - val_loss: 0.0250 - val_mae: 0.0904\n",
      "Epoch 162/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0277 - mae: 0.0973 - val_loss: 0.0250 - val_mae: 0.0903\n",
      "Epoch 163/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.0285 - mae: 0.0991 - val_loss: 0.0250 - val_mae: 0.0904\n",
      "Epoch 164/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0278 - mae: 0.0977 - val_loss: 0.0250 - val_mae: 0.0902\n",
      "Epoch 165/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0281 - mae: 0.0980 - val_loss: 0.0250 - val_mae: 0.0908\n",
      "Epoch 166/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0287 - mae: 0.0995 - val_loss: 0.0250 - val_mae: 0.0910\n",
      "Epoch 167/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 0.0281 - mae: 0.0983 - val_loss: 0.0250 - val_mae: 0.0905\n",
      "Epoch 168/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.0280 - mae: 0.0983 - val_loss: 0.0250 - val_mae: 0.0908\n",
      "Epoch 169/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.0280 - mae: 0.0978 - val_loss: 0.0250 - val_mae: 0.0896\n",
      "Epoch 170/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0280 - mae: 0.0977 - val_loss: 0.0250 - val_mae: 0.0913\n",
      "Epoch 171/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0281 - mae: 0.0981 - val_loss: 0.0250 - val_mae: 0.0897\n",
      "Epoch 172/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - loss: 0.0283 - mae: 0.0986 - val_loss: 0.0249 - val_mae: 0.0908\n",
      "Epoch 173/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 0.0283 - mae: 0.0987 - val_loss: 0.0249 - val_mae: 0.0904\n",
      "Epoch 174/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 0.0283 - mae: 0.0985 - val_loss: 0.0250 - val_mae: 0.0908\n",
      "Epoch 175/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0284 - mae: 0.0987 - val_loss: 0.0252 - val_mae: 0.0930\n",
      "Epoch 176/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0286 - mae: 0.0989 - val_loss: 0.0249 - val_mae: 0.0903\n",
      "Epoch 177/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0280 - mae: 0.0976 - val_loss: 0.0249 - val_mae: 0.0904\n",
      "Epoch 178/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.0280 - mae: 0.0980 - val_loss: 0.0249 - val_mae: 0.0904\n",
      "Epoch 179/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0282 - mae: 0.0983 - val_loss: 0.0249 - val_mae: 0.0905\n",
      "Epoch 180/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0280 - mae: 0.0982 - val_loss: 0.0250 - val_mae: 0.0890\n",
      "Epoch 181/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 0.0277 - mae: 0.0974 - val_loss: 0.0249 - val_mae: 0.0911\n",
      "Epoch 182/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 0.0281 - mae: 0.0981 - val_loss: 0.0249 - val_mae: 0.0903\n",
      "Epoch 183/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0282 - mae: 0.0989 - val_loss: 0.0249 - val_mae: 0.0896\n",
      "Epoch 184/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 0.0283 - mae: 0.0984 - val_loss: 0.0249 - val_mae: 0.0911\n",
      "Epoch 185/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0282 - mae: 0.0987 - val_loss: 0.0252 - val_mae: 0.0899\n",
      "Epoch 186/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.0280 - mae: 0.0976 - val_loss: 0.0251 - val_mae: 0.0887\n",
      "Epoch 187/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 0.0281 - mae: 0.0980 - val_loss: 0.0249 - val_mae: 0.0903\n",
      "Epoch 188/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.0282 - mae: 0.0982 - val_loss: 0.0249 - val_mae: 0.0904\n",
      "Epoch 189/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.0279 - mae: 0.0976 - val_loss: 0.0249 - val_mae: 0.0905\n",
      "Epoch 190/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.0284 - mae: 0.0990 - val_loss: 0.0249 - val_mae: 0.0902\n",
      "Epoch 191/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.0281 - mae: 0.0979 - val_loss: 0.0250 - val_mae: 0.0914\n",
      "Epoch 192/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.0282 - mae: 0.0983 - val_loss: 0.0249 - val_mae: 0.0902\n",
      "Epoch 193/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.0282 - mae: 0.0984 - val_loss: 0.0249 - val_mae: 0.0904\n",
      "Epoch 194/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 0.0284 - mae: 0.0993 - val_loss: 0.0249 - val_mae: 0.0905\n",
      "Epoch 195/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.0281 - mae: 0.0982 - val_loss: 0.0249 - val_mae: 0.0902\n",
      "Epoch 196/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 0.0278 - mae: 0.0971 - val_loss: 0.0251 - val_mae: 0.0881\n",
      "Epoch 197/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.0278 - mae: 0.0969 - val_loss: 0.0249 - val_mae: 0.0903\n",
      "Epoch 198/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 0.0285 - mae: 0.0992 - val_loss: 0.0249 - val_mae: 0.0901\n",
      "Epoch 199/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.0277 - mae: 0.0969 - val_loss: 0.0249 - val_mae: 0.0902\n",
      "Epoch 200/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.0279 - mae: 0.0978 - val_loss: 0.0249 - val_mae: 0.0903\n",
      "Restoring model weights from the end of the best epoch: 198.\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step\n",
      "        PC1       PC2       PC3       PC4       PC5\n",
      "0  0.004024  0.033268  0.015003  0.054835  0.008215\n",
      "1  0.036652  0.026473  0.000000  0.000000  0.000000\n",
      "2  0.036495  0.000000  0.000000  0.000839  0.000000\n",
      "3  0.046388  0.014393  0.054044  0.036450  0.038265\n",
      "4  0.000000  0.052144  0.009681  0.081652  0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense,ActivityRegularization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Data Normalization\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(qns.iloc[:, 2:])\n",
    "\n",
    "# 2. Define a Simplified Autoencoder Structure\n",
    "input_dim = qns.iloc[:, 2:].shape[1]\n",
    "encoding_dim = 5  # Capturing five main features\n",
    "\n",
    "# Input Layer\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# Encoding Layer\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "encoded = ActivityRegularization(l1=1e-4)(encoded)\n",
    "\n",
    "# Decoding Layer\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "# Construct Autoencoder Model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "# Encoder Model (to extract features)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# 3. Compile and Train the Model\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Use EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(data_scaled, data_scaled, epochs=200, batch_size=32, shuffle=True, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# 4. Extract Main Components\n",
    "encoded_data = encoder.predict(data_scaled)\n",
    "\n",
    "# Convert the extracted components to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=[f'PC{i+1}' for i in range(encoding_dim)])\n",
    "\n",
    "# View the extracted components\n",
    "print(encoded_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "Variance Explained: 0.3426\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 用训练好的Autoencoder对原始数据进行重构\n",
    "reconstructed_data = autoencoder.predict(data_scaled)\n",
    "\n",
    "# 计算每个特征的总方差\n",
    "total_variance = np.var(data_scaled, axis=0)\n",
    "\n",
    "# 计算重构误差 (MSE)\n",
    "reconstruction_error = mean_squared_error(data_scaled, reconstructed_data)\n",
    "\n",
    "# 计算解释的方差比例\n",
    "variance_explained_total = 1 - (reconstruction_error / np.mean(total_variance))\n",
    "\n",
    "print(f'Variance Explained: {variance_explained_total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.015003</td>\n",
       "      <td>0.054835</td>\n",
       "      <td>0.008215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036652</td>\n",
       "      <td>0.026473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.046388</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>0.054044</td>\n",
       "      <td>0.036450</td>\n",
       "      <td>0.038265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052144</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.081652</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8942</th>\n",
       "      <td>0.007805</td>\n",
       "      <td>0.070670</td>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>0.069377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8943</th>\n",
       "      <td>0.077333</td>\n",
       "      <td>0.092783</td>\n",
       "      <td>0.116401</td>\n",
       "      <td>0.076105</td>\n",
       "      <td>0.105912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8944</th>\n",
       "      <td>0.057946</td>\n",
       "      <td>0.034327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.038471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8945</th>\n",
       "      <td>0.010644</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.004877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8946</th>\n",
       "      <td>0.035342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8947 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PC1       PC2       PC3       PC4       PC5\n",
       "0     0.004024  0.033268  0.015003  0.054835  0.008215\n",
       "1     0.036652  0.026473  0.000000  0.000000  0.000000\n",
       "2     0.036495  0.000000  0.000000  0.000839  0.000000\n",
       "3     0.046388  0.014393  0.054044  0.036450  0.038265\n",
       "4     0.000000  0.052144  0.009681  0.081652  0.000000\n",
       "...        ...       ...       ...       ...       ...\n",
       "8942  0.007805  0.070670  0.006713  0.008951  0.069377\n",
       "8943  0.077333  0.092783  0.116401  0.076105  0.105912\n",
       "8944  0.057946  0.034327  0.000000  0.012800  0.038471\n",
       "8945  0.010644  0.003564  0.000000  0.001124  0.004877\n",
       "8946  0.035342  0.000000  0.005415  0.000000  0.000000\n",
       "\n",
       "[8947 rows x 5 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data and drop the first column and the subject id\n",
    "data = pd.read_csv(r\"G:/ABCD/data/mri_y_rsfmr_cor_gp_aseg.csv\")\n",
    "# only keep eventname = baseline_year_1_arm_1\n",
    "data = data[data['eventname'] == 'baseline_year_1_arm_1'].drop(columns=['eventname'])\n",
    "\n",
    "labels = pd.read_csv(r'G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\nmf_W.csv').iloc[:, 1:]\n",
    "\n",
    "# only keep the subjects that have labels\n",
    "data = data[data['src_subject_id'].isin(labels[\"src_subject_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_6936\\3108171495.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(columns=['src_subject_id'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data.drop(columns=['src_subject_id'], inplace=True)\n",
    "labels.drop(columns=['src_subject_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[369], line 22\u001b[0m\n\u001b[0;32m     10\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     11\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[0;32m     12\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestRegressor(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     ))\n\u001b[0;32m     19\u001b[0m ])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Perform 10-fold cross-validation\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Report the mean R2 score from cross-validation to NNI\u001b[39;00m\n\u001b[0;32m     25\u001b[0m mean_cv_r2 \u001b[38;5;241m=\u001b[39m cv_scores\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    730\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 732\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "#import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestRegressor(\n",
    "        # n_estimators=params['n_estimators'],\n",
    "        # max_depth=params['max_depth'],\n",
    "        # min_samples_split=params['min_samples_split'],\n",
    "        # min_samples_leaf=params['min_samples_leaf'],\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores = cross_val_score(pipeline, data, labels, cv=2, scoring='r2')\n",
    "\n",
    "# Report the mean R2 score from cross-validation to NNI\n",
    "mean_cv_r2 = cv_scores.mean()\n",
    "\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(f'10-Fold Cross-Validation R2 Scores: {cv_scores}')\n",
    "print(f'Mean R2 Score from 10-Fold Cross-Validation: {mean_cv_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853/853 - 1s - 2ms/step - loss: 0.9912 - mae: 0.7254 - val_loss: 0.9704 - val_mae: 0.6975\n",
      "Epoch 2/100\n",
      "853/853 - 1s - 799us/step - loss: 0.8245 - mae: 0.6764 - val_loss: 0.9641 - val_mae: 0.7336\n",
      "Epoch 3/100\n",
      "853/853 - 1s - 869us/step - loss: 0.7642 - mae: 0.6519 - val_loss: 1.0132 - val_mae: 0.7600\n",
      "Epoch 4/100\n",
      "853/853 - 1s - 800us/step - loss: 0.7115 - mae: 0.6328 - val_loss: 1.0228 - val_mae: 0.7254\n",
      "Epoch 5/100\n",
      "853/853 - 1s - 953us/step - loss: 0.6598 - mae: 0.6079 - val_loss: 1.0440 - val_mae: 0.7672\n",
      "Epoch 6/100\n",
      "853/853 - 1s - 884us/step - loss: 0.5991 - mae: 0.5819 - val_loss: 1.0900 - val_mae: 0.7710\n",
      "Epoch 7/100\n",
      "853/853 - 1s - 875us/step - loss: 0.5474 - mae: 0.5530 - val_loss: 1.1460 - val_mae: 0.7898\n",
      "Epoch 8/100\n",
      "853/853 - 1s - 882us/step - loss: 0.4990 - mae: 0.5268 - val_loss: 1.2186 - val_mae: 0.8249\n",
      "Epoch 9/100\n",
      "853/853 - 1s - 907us/step - loss: 0.4727 - mae: 0.5089 - val_loss: 1.2220 - val_mae: 0.8153\n",
      "Epoch 10/100\n",
      "853/853 - 1s - 891us/step - loss: 0.4323 - mae: 0.4862 - val_loss: 1.2469 - val_mae: 0.8409\n",
      "Epoch 11/100\n",
      "853/853 - 1s - 890us/step - loss: 0.3614 - mae: 0.4526 - val_loss: 1.3846 - val_mae: 0.8768\n",
      "Epoch 12/100\n",
      "853/853 - 1s - 876us/step - loss: 0.3553 - mae: 0.4429 - val_loss: 1.3489 - val_mae: 0.8467\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "R^2 score: -0.06281963386220268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个函数来构建神经网络模型\n",
    "def create_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# 一次性划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 数据归一化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 创建和编译模型\n",
    "model = create_nn_model(input_dim=X_train.shape[1])\n",
    "\n",
    "# 定义 EarlyStopping 回调函数\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# 训练模型并使用 EarlyStopping\n",
    "model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    validation_data=(X_test_scaled, y_test),  # 添加验证数据以监控 val_loss\n",
    "    epochs=100, \n",
    "    batch_size=10, \n",
    "    verbose=2, \n",
    "    callbacks=[early_stopping]  # 添加 EarlyStopping 回调\n",
    ")\n",
    "\n",
    "# 进行预测\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 计算 R^2 分数\n",
    "score = r2_score(y_test, y_pred)\n",
    "\n",
    "# 输出 R^2 分数\n",
    "print(\"R^2 score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# load data and drop the first column and the subject id\n",
    "data = pd.read_csv(r\"G:/ABCD/data/mri_y_rsfmr_cor_gp_aseg.csv\")\n",
    "# only keep eventname = baseline_year_1_arm_1\n",
    "data = data[data['eventname'] == 'baseline_year_1_arm_1'].drop(columns=['eventname']).dropna()\n",
    "\n",
    "labels = pd.read_csv(r'G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\EFA.csv').iloc[:, 1:]\n",
    "# labels = pd.read_csv(r'G:/ABCD/data/mh_p_cbcl.csv')\n",
    "# # only keep the  column.('cbcl_scr_syn_anxdep_t')\n",
    "# labels = labels[labels['eventname'] == 'baseline_year_1_arm_1'].drop(columns=['eventname'])\n",
    "# labels = labels[['src_subject_id', 'cbcl_scr_syn_anxdep_t']].dropna()\n",
    "\n",
    "# only keep the subjects that have labels\n",
    "data = data[data['src_subject_id'].isin(labels[\"src_subject_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>PA1</th>\n",
       "      <th>PA2</th>\n",
       "      <th>PA3</th>\n",
       "      <th>PA5</th>\n",
       "      <th>PA4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>-0.590732</td>\n",
       "      <td>-0.675405</td>\n",
       "      <td>-0.734731</td>\n",
       "      <td>-0.707320</td>\n",
       "      <td>0.474278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>-0.463630</td>\n",
       "      <td>-0.569176</td>\n",
       "      <td>-0.943524</td>\n",
       "      <td>-0.853836</td>\n",
       "      <td>0.226067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>-0.561360</td>\n",
       "      <td>0.088093</td>\n",
       "      <td>0.267510</td>\n",
       "      <td>1.555214</td>\n",
       "      <td>0.320209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>-0.393168</td>\n",
       "      <td>0.241235</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>-0.462533</td>\n",
       "      <td>-0.893611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>2.441081</td>\n",
       "      <td>0.258618</td>\n",
       "      <td>0.827516</td>\n",
       "      <td>-1.152136</td>\n",
       "      <td>-1.017326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>0.704346</td>\n",
       "      <td>2.204722</td>\n",
       "      <td>1.285274</td>\n",
       "      <td>1.950706</td>\n",
       "      <td>1.057855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>0.570672</td>\n",
       "      <td>0.403511</td>\n",
       "      <td>-0.676810</td>\n",
       "      <td>-0.893910</td>\n",
       "      <td>-0.799992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>-0.555718</td>\n",
       "      <td>-0.763623</td>\n",
       "      <td>-0.856959</td>\n",
       "      <td>-0.452341</td>\n",
       "      <td>0.132974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>-0.232042</td>\n",
       "      <td>0.089243</td>\n",
       "      <td>-0.821274</td>\n",
       "      <td>-0.141745</td>\n",
       "      <td>-0.697024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>-0.214842</td>\n",
       "      <td>-0.530197</td>\n",
       "      <td>0.101790</td>\n",
       "      <td>-0.787224</td>\n",
       "      <td>-0.200235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id       PA1       PA2       PA3       PA5       PA4\n",
       "0      NDAR_INV003RTV85 -0.590732 -0.675405 -0.734731 -0.707320  0.474278\n",
       "1      NDAR_INV005V6D2C -0.463630 -0.569176 -0.943524 -0.853836  0.226067\n",
       "2      NDAR_INV007W6H7B -0.561360  0.088093  0.267510  1.555214  0.320209\n",
       "3      NDAR_INV00BD7VDC -0.393168  0.241235  0.337544 -0.462533 -0.893611\n",
       "4      NDAR_INV00CY2MDM  2.441081  0.258618  0.827516 -1.152136 -1.017326\n",
       "...                 ...       ...       ...       ...       ...       ...\n",
       "10647  NDAR_INVZZLZCKAY  0.704346  2.204722  1.285274  1.950706  1.057855\n",
       "10648  NDAR_INVZZNX6W2P  0.570672  0.403511 -0.676810 -0.893910 -0.799992\n",
       "10649  NDAR_INVZZPKBDAC -0.555718 -0.763623 -0.856959 -0.452341  0.132974\n",
       "10650  NDAR_INVZZZ2ALR6 -0.232042  0.089243 -0.821274 -0.141745 -0.697024\n",
       "10651  NDAR_INVZZZNB0XC -0.214842 -0.530197  0.101790 -0.787224 -0.200235\n",
       "\n",
       "[10652 rows x 6 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aalh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aarh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aglh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_agrh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_bs</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cdelh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cderh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_crcxlh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_crcxrh</th>\n",
       "      <th>...</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_hplh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_hprh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_pllh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_plrh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_ptlh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_ptrh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_thplh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_thprh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_vtdclh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_vtdcrh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>-0.090258</td>\n",
       "      <td>-0.083909</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.048167</td>\n",
       "      <td>0.036012</td>\n",
       "      <td>0.013843</td>\n",
       "      <td>-0.020096</td>\n",
       "      <td>-0.081816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106619</td>\n",
       "      <td>0.112692</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>-0.106608</td>\n",
       "      <td>0.074385</td>\n",
       "      <td>0.119522</td>\n",
       "      <td>-0.135962</td>\n",
       "      <td>-0.060436</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>0.113327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>-0.098593</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>-0.170801</td>\n",
       "      <td>-0.134319</td>\n",
       "      <td>-0.022124</td>\n",
       "      <td>0.092985</td>\n",
       "      <td>0.061854</td>\n",
       "      <td>0.011913</td>\n",
       "      <td>0.013176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026733</td>\n",
       "      <td>-0.061707</td>\n",
       "      <td>0.127464</td>\n",
       "      <td>-0.135847</td>\n",
       "      <td>0.097308</td>\n",
       "      <td>0.104559</td>\n",
       "      <td>-0.060583</td>\n",
       "      <td>-0.037027</td>\n",
       "      <td>0.021549</td>\n",
       "      <td>-0.014602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>-0.065449</td>\n",
       "      <td>0.122893</td>\n",
       "      <td>-0.006246</td>\n",
       "      <td>-0.118878</td>\n",
       "      <td>-0.027118</td>\n",
       "      <td>-0.026523</td>\n",
       "      <td>-0.045971</td>\n",
       "      <td>-0.065351</td>\n",
       "      <td>-0.036497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076594</td>\n",
       "      <td>0.110142</td>\n",
       "      <td>-0.077766</td>\n",
       "      <td>-0.020942</td>\n",
       "      <td>0.120677</td>\n",
       "      <td>0.135012</td>\n",
       "      <td>0.020034</td>\n",
       "      <td>-0.017153</td>\n",
       "      <td>-0.043255</td>\n",
       "      <td>-0.072356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>-0.148170</td>\n",
       "      <td>-0.049085</td>\n",
       "      <td>-0.026814</td>\n",
       "      <td>-0.028232</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>-0.153769</td>\n",
       "      <td>-0.073198</td>\n",
       "      <td>-0.138927</td>\n",
       "      <td>0.032385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099686</td>\n",
       "      <td>0.077307</td>\n",
       "      <td>0.019302</td>\n",
       "      <td>0.064816</td>\n",
       "      <td>0.141727</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>-0.038964</td>\n",
       "      <td>0.030539</td>\n",
       "      <td>-0.019395</td>\n",
       "      <td>-0.060836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>-0.148117</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>-0.104190</td>\n",
       "      <td>-0.037308</td>\n",
       "      <td>0.121178</td>\n",
       "      <td>0.034533</td>\n",
       "      <td>-0.183537</td>\n",
       "      <td>-0.258663</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036267</td>\n",
       "      <td>-0.058694</td>\n",
       "      <td>-0.152607</td>\n",
       "      <td>-0.076361</td>\n",
       "      <td>0.058996</td>\n",
       "      <td>0.226045</td>\n",
       "      <td>-0.028446</td>\n",
       "      <td>-0.064175</td>\n",
       "      <td>-0.066788</td>\n",
       "      <td>0.010962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>0.030591</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>-0.138875</td>\n",
       "      <td>-0.096117</td>\n",
       "      <td>-0.008607</td>\n",
       "      <td>-0.111538</td>\n",
       "      <td>-0.111535</td>\n",
       "      <td>-0.161508</td>\n",
       "      <td>-0.102951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097498</td>\n",
       "      <td>0.043633</td>\n",
       "      <td>0.055380</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>0.149435</td>\n",
       "      <td>0.025411</td>\n",
       "      <td>-0.097254</td>\n",
       "      <td>0.030673</td>\n",
       "      <td>-0.084058</td>\n",
       "      <td>-0.002552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>-0.089590</td>\n",
       "      <td>-0.002218</td>\n",
       "      <td>-0.150480</td>\n",
       "      <td>0.052007</td>\n",
       "      <td>0.119924</td>\n",
       "      <td>-0.258660</td>\n",
       "      <td>-0.164504</td>\n",
       "      <td>-0.068733</td>\n",
       "      <td>-0.057475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016435</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>0.084433</td>\n",
       "      <td>-0.091015</td>\n",
       "      <td>0.185989</td>\n",
       "      <td>0.038151</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.060238</td>\n",
       "      <td>-0.132834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>-0.112247</td>\n",
       "      <td>-0.010195</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>-0.112359</td>\n",
       "      <td>-0.043313</td>\n",
       "      <td>0.045530</td>\n",
       "      <td>-0.188547</td>\n",
       "      <td>-0.048601</td>\n",
       "      <td>-0.165866</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023279</td>\n",
       "      <td>0.049299</td>\n",
       "      <td>0.093359</td>\n",
       "      <td>-0.069631</td>\n",
       "      <td>0.228659</td>\n",
       "      <td>0.213006</td>\n",
       "      <td>-0.077175</td>\n",
       "      <td>-0.049607</td>\n",
       "      <td>0.041590</td>\n",
       "      <td>-0.035196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>0.051633</td>\n",
       "      <td>-0.127992</td>\n",
       "      <td>-0.172488</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>-0.014129</td>\n",
       "      <td>-0.236400</td>\n",
       "      <td>-0.056846</td>\n",
       "      <td>0.076282</td>\n",
       "      <td>-0.055248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062011</td>\n",
       "      <td>-0.025998</td>\n",
       "      <td>0.016382</td>\n",
       "      <td>-0.024677</td>\n",
       "      <td>0.084724</td>\n",
       "      <td>0.093898</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>-0.202033</td>\n",
       "      <td>0.027023</td>\n",
       "      <td>0.074749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>-0.018827</td>\n",
       "      <td>-0.116764</td>\n",
       "      <td>-0.085335</td>\n",
       "      <td>0.049574</td>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.172729</td>\n",
       "      <td>-0.047047</td>\n",
       "      <td>-0.034302</td>\n",
       "      <td>-0.117179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058867</td>\n",
       "      <td>0.065065</td>\n",
       "      <td>0.018944</td>\n",
       "      <td>-0.033643</td>\n",
       "      <td>0.238980</td>\n",
       "      <td>0.100577</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>-0.027085</td>\n",
       "      <td>0.030794</td>\n",
       "      <td>0.053762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id  rsfmri_cor_ngd_au_scs_aalh  \\\n",
       "0      NDAR_INV003RTV85                   -0.090258   \n",
       "1      NDAR_INV005V6D2C                   -0.098593   \n",
       "2      NDAR_INV007W6H7B                   -0.065449   \n",
       "3      NDAR_INV00BD7VDC                   -0.148170   \n",
       "6      NDAR_INV00CY2MDM                   -0.148117   \n",
       "...                 ...                         ...   \n",
       "22117  NDAR_INVZZLZCKAY                    0.030591   \n",
       "22120  NDAR_INVZZNX6W2P                   -0.089590   \n",
       "22123  NDAR_INVZZPKBDAC                   -0.112247   \n",
       "22125  NDAR_INVZZZ2ALR6                    0.051633   \n",
       "22127  NDAR_INVZZZNB0XC                   -0.018827   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_aarh  rsfmri_cor_ngd_au_scs_aglh  \\\n",
       "0                       -0.083909                    0.004247   \n",
       "1                        0.093085                   -0.170801   \n",
       "2                        0.122893                   -0.006246   \n",
       "3                       -0.049085                   -0.026814   \n",
       "6                        0.037320                   -0.104190   \n",
       "...                           ...                         ...   \n",
       "22117                    0.003116                   -0.138875   \n",
       "22120                   -0.002218                   -0.150480   \n",
       "22123                   -0.010195                    0.038551   \n",
       "22125                   -0.127992                   -0.172488   \n",
       "22127                   -0.116764                   -0.085335   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_agrh  rsfmri_cor_ngd_au_scs_bs  \\\n",
       "0                       -0.054861                 -0.048167   \n",
       "1                       -0.134319                 -0.022124   \n",
       "2                       -0.118878                 -0.027118   \n",
       "3                       -0.028232                  0.008625   \n",
       "6                       -0.037308                  0.121178   \n",
       "...                           ...                       ...   \n",
       "22117                   -0.096117                 -0.008607   \n",
       "22120                    0.052007                  0.119924   \n",
       "22123                   -0.112359                 -0.043313   \n",
       "22125                    0.013249                 -0.014129   \n",
       "22127                    0.049574                  0.160182   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_cdelh  rsfmri_cor_ngd_au_scs_cderh  \\\n",
       "0                         0.036012                     0.013843   \n",
       "1                         0.092985                     0.061854   \n",
       "2                        -0.026523                    -0.045971   \n",
       "3                        -0.153769                    -0.073198   \n",
       "6                         0.034533                    -0.183537   \n",
       "...                            ...                          ...   \n",
       "22117                    -0.111538                    -0.111535   \n",
       "22120                    -0.258660                    -0.164504   \n",
       "22123                     0.045530                    -0.188547   \n",
       "22125                    -0.236400                    -0.056846   \n",
       "22127                     0.172729                    -0.047047   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_crcxlh  rsfmri_cor_ngd_au_scs_crcxrh  ...  \\\n",
       "0                         -0.020096                     -0.081816  ...   \n",
       "1                          0.011913                      0.013176  ...   \n",
       "2                         -0.065351                     -0.036497  ...   \n",
       "3                         -0.138927                      0.032385  ...   \n",
       "6                         -0.258663                      0.003149  ...   \n",
       "...                             ...                           ...  ...   \n",
       "22117                     -0.161508                     -0.102951  ...   \n",
       "22120                     -0.068733                     -0.057475  ...   \n",
       "22123                     -0.048601                     -0.165866  ...   \n",
       "22125                      0.076282                     -0.055248  ...   \n",
       "22127                     -0.034302                     -0.117179  ...   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_hplh  rsfmri_cor_ngd_vs_scs_hprh  \\\n",
       "0                        0.106619                    0.112692   \n",
       "1                       -0.026733                   -0.061707   \n",
       "2                        0.076594                    0.110142   \n",
       "3                        0.099686                    0.077307   \n",
       "6                       -0.036267                   -0.058694   \n",
       "...                           ...                         ...   \n",
       "22117                    0.097498                    0.043633   \n",
       "22120                    0.016435                    0.048579   \n",
       "22123                   -0.023279                    0.049299   \n",
       "22125                    0.062011                   -0.025998   \n",
       "22127                    0.058867                    0.065065   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_pllh  rsfmri_cor_ngd_vs_scs_plrh  \\\n",
       "0                        0.013191                   -0.106608   \n",
       "1                        0.127464                   -0.135847   \n",
       "2                       -0.077766                   -0.020942   \n",
       "3                        0.019302                    0.064816   \n",
       "6                       -0.152607                   -0.076361   \n",
       "...                           ...                         ...   \n",
       "22117                    0.055380                    0.102299   \n",
       "22120                    0.084433                   -0.091015   \n",
       "22123                    0.093359                   -0.069631   \n",
       "22125                    0.016382                   -0.024677   \n",
       "22127                    0.018944                   -0.033643   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_ptlh  rsfmri_cor_ngd_vs_scs_ptrh  \\\n",
       "0                        0.074385                    0.119522   \n",
       "1                        0.097308                    0.104559   \n",
       "2                        0.120677                    0.135012   \n",
       "3                        0.141727                    0.010190   \n",
       "6                        0.058996                    0.226045   \n",
       "...                           ...                         ...   \n",
       "22117                    0.149435                    0.025411   \n",
       "22120                    0.185989                    0.038151   \n",
       "22123                    0.228659                    0.213006   \n",
       "22125                    0.084724                    0.093898   \n",
       "22127                    0.238980                    0.100577   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_thplh  rsfmri_cor_ngd_vs_scs_thprh  \\\n",
       "0                        -0.135962                    -0.060436   \n",
       "1                        -0.060583                    -0.037027   \n",
       "2                         0.020034                    -0.017153   \n",
       "3                        -0.038964                     0.030539   \n",
       "6                        -0.028446                    -0.064175   \n",
       "...                            ...                          ...   \n",
       "22117                    -0.097254                     0.030673   \n",
       "22120                     0.006236                     0.012520   \n",
       "22123                    -0.077175                    -0.049607   \n",
       "22125                     0.000896                    -0.202033   \n",
       "22127                    -0.009229                    -0.027085   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_vtdclh  rsfmri_cor_ngd_vs_scs_vtdcrh  \n",
       "0                         -0.000929                      0.113327  \n",
       "1                          0.021549                     -0.014602  \n",
       "2                         -0.043255                     -0.072356  \n",
       "3                         -0.019395                     -0.060836  \n",
       "6                         -0.066788                      0.010962  \n",
       "...                             ...                           ...  \n",
       "22117                     -0.084058                     -0.002552  \n",
       "22120                      0.060238                     -0.132834  \n",
       "22123                      0.041590                     -0.035196  \n",
       "22125                      0.027023                      0.074749  \n",
       "22127                      0.030794                      0.053762  \n",
       "\n",
       "[10652 rows x 248 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['src_subject_id'])\n",
    "labels = labels.drop(columns=['src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PA2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.675405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.569176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.241235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.258618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>2.204722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>0.403511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>-0.763623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>0.089243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>-0.530197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PA2\n",
       "0     -0.675405\n",
       "1     -0.569176\n",
       "2      0.088093\n",
       "3      0.241235\n",
       "4      0.258618\n",
       "...         ...\n",
       "10647  2.204722\n",
       "10648  0.403511\n",
       "10649 -0.763623\n",
       "10650  0.089243\n",
       "10651 -0.530197\n",
       "\n",
       "[10652 rows x 1 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.iloc[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -0.09530539862210774\n",
      "均方误差 (MSE): 0.9913688966571422\n",
      "R^2 评分: -0.09530539862210774\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "#import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels.iloc[:,[1]], test_size=0.3, random_state=42)\n",
    "\n",
    "# 使用支持向量回归\n",
    "regressor = svm.SVR()  # 确保使用 SVR 而不是 SVC\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = regressor.predict(X_test)\n",
    "# accuracy \n",
    "# 评估模型性能\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"均方误差 (MSE):\", mse)\n",
    "print(\"R^2 评分:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.drop(columns=['src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均方误差 (MSE): 0.9943679438196448\n",
      "R^2 评分: -0.01415056228921241\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 将数据划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels.iloc[:, [0]], test_size=0.2)\n",
    "\n",
    "# 使用线性回归\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# 评估模型性能\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"均方误差 (MSE):\", mse)\n",
    "print(\"R^2 评分:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均方误差 (MSE): 0.8409004247621553\n",
      "R^2 评分: -0.00017185603033462727\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 将数据划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "\n",
    "# 使用线性回归\n",
    "regressor1 = MultiTaskElasticNet()\n",
    "regressor1.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = regressor1.predict(X_test)\n",
    "\n",
    "# 评估模型性能\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"均方误差 (MSE):\", mse)\n",
    "print(\"R^2 评分:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(r'G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\data_cleaned.csv').drop(columns=['Unnamed: 0', 'src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.to_csv(r'G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\target.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(r'G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor_1</th>\n",
       "      <th>Factor_2</th>\n",
       "      <th>Factor_3</th>\n",
       "      <th>Factor_4</th>\n",
       "      <th>Factor_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.225962</td>\n",
       "      <td>-0.987701</td>\n",
       "      <td>1.428785</td>\n",
       "      <td>0.196275</td>\n",
       "      <td>2.294005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.270909</td>\n",
       "      <td>-1.172184</td>\n",
       "      <td>1.699451</td>\n",
       "      <td>1.889652</td>\n",
       "      <td>2.205972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170160</td>\n",
       "      <td>0.610330</td>\n",
       "      <td>-0.446073</td>\n",
       "      <td>2.320928</td>\n",
       "      <td>1.705356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.410402</td>\n",
       "      <td>-0.158501</td>\n",
       "      <td>0.223055</td>\n",
       "      <td>3.264879</td>\n",
       "      <td>1.840801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.462913</td>\n",
       "      <td>-5.130525</td>\n",
       "      <td>1.558750</td>\n",
       "      <td>5.146125</td>\n",
       "      <td>-4.048456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>-6.752625</td>\n",
       "      <td>5.188299</td>\n",
       "      <td>0.805525</td>\n",
       "      <td>-0.362465</td>\n",
       "      <td>-2.794127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>0.047847</td>\n",
       "      <td>-2.206021</td>\n",
       "      <td>1.290669</td>\n",
       "      <td>3.946332</td>\n",
       "      <td>-0.416121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>-0.721727</td>\n",
       "      <td>-0.715168</td>\n",
       "      <td>1.275204</td>\n",
       "      <td>0.842071</td>\n",
       "      <td>2.017429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>-0.617627</td>\n",
       "      <td>-1.584424</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>1.639837</td>\n",
       "      <td>0.536771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>1.174780</td>\n",
       "      <td>-0.877488</td>\n",
       "      <td>1.137938</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.846687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
       "0      0.225962 -0.987701  1.428785  0.196275  2.294005\n",
       "1     -0.270909 -1.172184  1.699451  1.889652  2.205972\n",
       "2     -0.170160  0.610330 -0.446073  2.320928  1.705356\n",
       "3      0.410402 -0.158501  0.223055  3.264879  1.840801\n",
       "4      3.462913 -5.130525  1.558750  5.146125 -4.048456\n",
       "...         ...       ...       ...       ...       ...\n",
       "10647 -6.752625  5.188299  0.805525 -0.362465 -2.794127\n",
       "10648  0.047847 -2.206021  1.290669  3.946332 -0.416121\n",
       "10649 -0.721727 -0.715168  1.275204  0.842071  2.017429\n",
       "10650 -0.617627 -1.584424  0.179293  1.639837  0.536771\n",
       "10651  1.174780 -0.877488  1.137938  0.925061  0.846687\n",
       "\n",
       "[10652 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target.iloc[:,1], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均方误差 (MSE): 12.757518307536543\n",
      "R^2 评分: -0.5547964281276063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#标准化数据\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model1 = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000)\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# 评估模型性能\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"均方误差 (MSE):\", mse)\n",
    "print(\"R^2 评分:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 6.9431, Val Loss: 4.8581\n",
      "Epoch [2/100], Train Loss: 6.7767, Val Loss: 4.9208\n",
      "Epoch [3/100], Train Loss: 6.6072, Val Loss: 4.9533\n",
      "Epoch [4/100], Train Loss: 6.3278, Val Loss: 5.1843\n",
      "Epoch [5/100], Train Loss: 5.8956, Val Loss: 5.3937\n",
      "Epoch [6/100], Train Loss: 5.3739, Val Loss: 5.6553\n",
      "Epoch [7/100], Train Loss: 4.7916, Val Loss: 5.7848\n",
      "Epoch [8/100], Train Loss: 4.2999, Val Loss: 6.0064\n",
      "Epoch [9/100], Train Loss: 3.7938, Val Loss: 5.7991\n",
      "Epoch [10/100], Train Loss: 3.3754, Val Loss: 6.0572\n",
      "Epoch [11/100], Train Loss: 3.0624, Val Loss: 6.4718\n",
      "Early stopping at epoch 11\n",
      "R² score: -0.3518291493573309\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个函数来构建神经网络模型\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 准备数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target.iloc[:, 1], test_size=0.2, random_state=42)\n",
    "\n",
    "# 数据归一化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# 创建模型\n",
    "input_dim = X_train.shape[1]\n",
    "model = NeuralNetwork(input_dim)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Early Stopping 参数\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        val_loss = criterion(y_pred, y_test_tensor).item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # 检查 Early Stopping 条件\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# 预测和计算 R² 分数\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"R² score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For mono-task outputs, use ElasticNet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiTaskElasticNet\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiTaskElasticNet()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 评估模型性能\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\mne\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\mne\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:2386\u001b[0m, in \u001b[0;36mMultiTaskElasticNet.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2384\u001b[0m     model_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLasso\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 2386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor mono-task outputs, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m model_str)\n\u001b[0;32m   2388\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   2389\u001b[0m n_targets \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: For mono-task outputs, use ElasticNet"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "\n",
    "\n",
    "model = MultiTaskElasticNet()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 评估模型性能\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"均方误差 (MSE):\", mse)\n",
    "print(\"R^2 评分:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model with scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5408\\2238401008.py:10: DtypeWarning: Columns (124,128,132,136,140,144,148,152,156,160,164,168,172,176,180,184,188,192,196,200) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  labels = pd.read_csv(r'G:/ABCD/data/mh_p_cbcl.csv')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# load data and drop the first column and the subject id\n",
    "data = pd.read_csv(r\"G:/ABCD/data/mri_y_rsfmr_cor_gp_aseg.csv\")\n",
    "# only keep eventname = baseline_year_1_arm_1\n",
    "data = data[data['eventname'] == 'baseline_year_1_arm_1'].drop(columns=['eventname']).dropna()\n",
    "\n",
    "# labels = pd.read_csv(r'G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\EFA.csv').iloc[:, 1:]\n",
    "labels = pd.read_csv(r'G:/ABCD/data/mh_p_cbcl.csv')\n",
    "# only keep the  column.('cbcl_scr_syn_anxdep_t')\n",
    "labels = labels[labels['eventname'] == 'baseline_year_1_arm_1'].drop(columns=['eventname'])\n",
    "labels = labels[['src_subject_id', 'cbcl_scr_syn_anxdep_t']].dropna()\n",
    "\n",
    "# only keep the subjects that have labels\n",
    "data = data[data['src_subject_id'].isin(labels[\"src_subject_id\"])]\n",
    "labels = labels[labels['src_subject_id'].isin(data[\"src_subject_id\"])]\n",
    "\n",
    "data = data.drop(columns=['src_subject_id'])\n",
    "labels = labels.drop(columns=['src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbcl_scr_syn_anxdep_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48710</th>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48715</th>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48720</th>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48725</th>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48730</th>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11180 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cbcl_scr_syn_anxdep_t\n",
       "0                       50.0\n",
       "4                       50.0\n",
       "8                       53.0\n",
       "11                      59.0\n",
       "14                      53.0\n",
       "...                      ...\n",
       "48710                   65.0\n",
       "48715                   51.0\n",
       "48720                   50.0\n",
       "48725                   52.0\n",
       "48730                   50.0\n",
       "\n",
       "[11180 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aalh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aarh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aglh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_agrh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_bs</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cdelh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cderh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_crcxlh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_crcxrh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_hplh</th>\n",
       "      <th>...</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_hplh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_hprh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_pllh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_plrh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_ptlh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_ptrh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_thplh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_thprh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_vtdclh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_vtdcrh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.090258</td>\n",
       "      <td>-0.083909</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.048167</td>\n",
       "      <td>0.036012</td>\n",
       "      <td>0.013843</td>\n",
       "      <td>-0.020096</td>\n",
       "      <td>-0.081816</td>\n",
       "      <td>-0.043832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106619</td>\n",
       "      <td>0.112692</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>-0.106608</td>\n",
       "      <td>0.074385</td>\n",
       "      <td>0.119522</td>\n",
       "      <td>-0.135962</td>\n",
       "      <td>-0.060436</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>0.113327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.098593</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>-0.170801</td>\n",
       "      <td>-0.134319</td>\n",
       "      <td>-0.022124</td>\n",
       "      <td>0.092985</td>\n",
       "      <td>0.061854</td>\n",
       "      <td>0.011913</td>\n",
       "      <td>0.013176</td>\n",
       "      <td>-0.010548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026733</td>\n",
       "      <td>-0.061707</td>\n",
       "      <td>0.127464</td>\n",
       "      <td>-0.135847</td>\n",
       "      <td>0.097308</td>\n",
       "      <td>0.104559</td>\n",
       "      <td>-0.060583</td>\n",
       "      <td>-0.037027</td>\n",
       "      <td>0.021549</td>\n",
       "      <td>-0.014602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.065449</td>\n",
       "      <td>0.122893</td>\n",
       "      <td>-0.006246</td>\n",
       "      <td>-0.118878</td>\n",
       "      <td>-0.027118</td>\n",
       "      <td>-0.026523</td>\n",
       "      <td>-0.045971</td>\n",
       "      <td>-0.065351</td>\n",
       "      <td>-0.036497</td>\n",
       "      <td>-0.085342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076594</td>\n",
       "      <td>0.110142</td>\n",
       "      <td>-0.077766</td>\n",
       "      <td>-0.020942</td>\n",
       "      <td>0.120677</td>\n",
       "      <td>0.135012</td>\n",
       "      <td>0.020034</td>\n",
       "      <td>-0.017153</td>\n",
       "      <td>-0.043255</td>\n",
       "      <td>-0.072356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.148170</td>\n",
       "      <td>-0.049085</td>\n",
       "      <td>-0.026814</td>\n",
       "      <td>-0.028232</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>-0.153769</td>\n",
       "      <td>-0.073198</td>\n",
       "      <td>-0.138927</td>\n",
       "      <td>0.032385</td>\n",
       "      <td>-0.063008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099686</td>\n",
       "      <td>0.077307</td>\n",
       "      <td>0.019302</td>\n",
       "      <td>0.064816</td>\n",
       "      <td>0.141727</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>-0.038964</td>\n",
       "      <td>0.030539</td>\n",
       "      <td>-0.019395</td>\n",
       "      <td>-0.060836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.148117</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>-0.104190</td>\n",
       "      <td>-0.037308</td>\n",
       "      <td>0.121178</td>\n",
       "      <td>0.034533</td>\n",
       "      <td>-0.183537</td>\n",
       "      <td>-0.258663</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>-0.051349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036267</td>\n",
       "      <td>-0.058694</td>\n",
       "      <td>-0.152607</td>\n",
       "      <td>-0.076361</td>\n",
       "      <td>0.058996</td>\n",
       "      <td>0.226045</td>\n",
       "      <td>-0.028446</td>\n",
       "      <td>-0.064175</td>\n",
       "      <td>-0.066788</td>\n",
       "      <td>0.010962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>0.030591</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>-0.138875</td>\n",
       "      <td>-0.096117</td>\n",
       "      <td>-0.008607</td>\n",
       "      <td>-0.111538</td>\n",
       "      <td>-0.111535</td>\n",
       "      <td>-0.161508</td>\n",
       "      <td>-0.102951</td>\n",
       "      <td>-0.041270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097498</td>\n",
       "      <td>0.043633</td>\n",
       "      <td>0.055380</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>0.149435</td>\n",
       "      <td>0.025411</td>\n",
       "      <td>-0.097254</td>\n",
       "      <td>0.030673</td>\n",
       "      <td>-0.084058</td>\n",
       "      <td>-0.002552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>-0.089590</td>\n",
       "      <td>-0.002218</td>\n",
       "      <td>-0.150480</td>\n",
       "      <td>0.052007</td>\n",
       "      <td>0.119924</td>\n",
       "      <td>-0.258660</td>\n",
       "      <td>-0.164504</td>\n",
       "      <td>-0.068733</td>\n",
       "      <td>-0.057475</td>\n",
       "      <td>-0.058599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016435</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>0.084433</td>\n",
       "      <td>-0.091015</td>\n",
       "      <td>0.185989</td>\n",
       "      <td>0.038151</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.060238</td>\n",
       "      <td>-0.132834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>-0.112247</td>\n",
       "      <td>-0.010195</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>-0.112359</td>\n",
       "      <td>-0.043313</td>\n",
       "      <td>0.045530</td>\n",
       "      <td>-0.188547</td>\n",
       "      <td>-0.048601</td>\n",
       "      <td>-0.165866</td>\n",
       "      <td>-0.003135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023279</td>\n",
       "      <td>0.049299</td>\n",
       "      <td>0.093359</td>\n",
       "      <td>-0.069631</td>\n",
       "      <td>0.228659</td>\n",
       "      <td>0.213006</td>\n",
       "      <td>-0.077175</td>\n",
       "      <td>-0.049607</td>\n",
       "      <td>0.041590</td>\n",
       "      <td>-0.035196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>0.051633</td>\n",
       "      <td>-0.127992</td>\n",
       "      <td>-0.172488</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>-0.014129</td>\n",
       "      <td>-0.236400</td>\n",
       "      <td>-0.056846</td>\n",
       "      <td>0.076282</td>\n",
       "      <td>-0.055248</td>\n",
       "      <td>-0.004072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062011</td>\n",
       "      <td>-0.025998</td>\n",
       "      <td>0.016382</td>\n",
       "      <td>-0.024677</td>\n",
       "      <td>0.084724</td>\n",
       "      <td>0.093898</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>-0.202033</td>\n",
       "      <td>0.027023</td>\n",
       "      <td>0.074749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>-0.018827</td>\n",
       "      <td>-0.116764</td>\n",
       "      <td>-0.085335</td>\n",
       "      <td>0.049574</td>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.172729</td>\n",
       "      <td>-0.047047</td>\n",
       "      <td>-0.034302</td>\n",
       "      <td>-0.117179</td>\n",
       "      <td>-0.025159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058867</td>\n",
       "      <td>0.065065</td>\n",
       "      <td>0.018944</td>\n",
       "      <td>-0.033643</td>\n",
       "      <td>0.238980</td>\n",
       "      <td>0.100577</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>-0.027085</td>\n",
       "      <td>0.030794</td>\n",
       "      <td>0.053762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11180 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rsfmri_cor_ngd_au_scs_aalh  rsfmri_cor_ngd_au_scs_aarh  \\\n",
       "0                       -0.090258                   -0.083909   \n",
       "1                       -0.098593                    0.093085   \n",
       "2                       -0.065449                    0.122893   \n",
       "3                       -0.148170                   -0.049085   \n",
       "6                       -0.148117                    0.037320   \n",
       "...                           ...                         ...   \n",
       "22117                    0.030591                    0.003116   \n",
       "22120                   -0.089590                   -0.002218   \n",
       "22123                   -0.112247                   -0.010195   \n",
       "22125                    0.051633                   -0.127992   \n",
       "22127                   -0.018827                   -0.116764   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_aglh  rsfmri_cor_ngd_au_scs_agrh  \\\n",
       "0                        0.004247                   -0.054861   \n",
       "1                       -0.170801                   -0.134319   \n",
       "2                       -0.006246                   -0.118878   \n",
       "3                       -0.026814                   -0.028232   \n",
       "6                       -0.104190                   -0.037308   \n",
       "...                           ...                         ...   \n",
       "22117                   -0.138875                   -0.096117   \n",
       "22120                   -0.150480                    0.052007   \n",
       "22123                    0.038551                   -0.112359   \n",
       "22125                   -0.172488                    0.013249   \n",
       "22127                   -0.085335                    0.049574   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_bs  rsfmri_cor_ngd_au_scs_cdelh  \\\n",
       "0                     -0.048167                     0.036012   \n",
       "1                     -0.022124                     0.092985   \n",
       "2                     -0.027118                    -0.026523   \n",
       "3                      0.008625                    -0.153769   \n",
       "6                      0.121178                     0.034533   \n",
       "...                         ...                          ...   \n",
       "22117                 -0.008607                    -0.111538   \n",
       "22120                  0.119924                    -0.258660   \n",
       "22123                 -0.043313                     0.045530   \n",
       "22125                 -0.014129                    -0.236400   \n",
       "22127                  0.160182                     0.172729   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_cderh  rsfmri_cor_ngd_au_scs_crcxlh  \\\n",
       "0                         0.013843                     -0.020096   \n",
       "1                         0.061854                      0.011913   \n",
       "2                        -0.045971                     -0.065351   \n",
       "3                        -0.073198                     -0.138927   \n",
       "6                        -0.183537                     -0.258663   \n",
       "...                            ...                           ...   \n",
       "22117                    -0.111535                     -0.161508   \n",
       "22120                    -0.164504                     -0.068733   \n",
       "22123                    -0.188547                     -0.048601   \n",
       "22125                    -0.056846                      0.076282   \n",
       "22127                    -0.047047                     -0.034302   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_crcxrh  rsfmri_cor_ngd_au_scs_hplh  ...  \\\n",
       "0                         -0.081816                   -0.043832  ...   \n",
       "1                          0.013176                   -0.010548  ...   \n",
       "2                         -0.036497                   -0.085342  ...   \n",
       "3                          0.032385                   -0.063008  ...   \n",
       "6                          0.003149                   -0.051349  ...   \n",
       "...                             ...                         ...  ...   \n",
       "22117                     -0.102951                   -0.041270  ...   \n",
       "22120                     -0.057475                   -0.058599  ...   \n",
       "22123                     -0.165866                   -0.003135  ...   \n",
       "22125                     -0.055248                   -0.004072  ...   \n",
       "22127                     -0.117179                   -0.025159  ...   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_hplh  rsfmri_cor_ngd_vs_scs_hprh  \\\n",
       "0                        0.106619                    0.112692   \n",
       "1                       -0.026733                   -0.061707   \n",
       "2                        0.076594                    0.110142   \n",
       "3                        0.099686                    0.077307   \n",
       "6                       -0.036267                   -0.058694   \n",
       "...                           ...                         ...   \n",
       "22117                    0.097498                    0.043633   \n",
       "22120                    0.016435                    0.048579   \n",
       "22123                   -0.023279                    0.049299   \n",
       "22125                    0.062011                   -0.025998   \n",
       "22127                    0.058867                    0.065065   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_pllh  rsfmri_cor_ngd_vs_scs_plrh  \\\n",
       "0                        0.013191                   -0.106608   \n",
       "1                        0.127464                   -0.135847   \n",
       "2                       -0.077766                   -0.020942   \n",
       "3                        0.019302                    0.064816   \n",
       "6                       -0.152607                   -0.076361   \n",
       "...                           ...                         ...   \n",
       "22117                    0.055380                    0.102299   \n",
       "22120                    0.084433                   -0.091015   \n",
       "22123                    0.093359                   -0.069631   \n",
       "22125                    0.016382                   -0.024677   \n",
       "22127                    0.018944                   -0.033643   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_ptlh  rsfmri_cor_ngd_vs_scs_ptrh  \\\n",
       "0                        0.074385                    0.119522   \n",
       "1                        0.097308                    0.104559   \n",
       "2                        0.120677                    0.135012   \n",
       "3                        0.141727                    0.010190   \n",
       "6                        0.058996                    0.226045   \n",
       "...                           ...                         ...   \n",
       "22117                    0.149435                    0.025411   \n",
       "22120                    0.185989                    0.038151   \n",
       "22123                    0.228659                    0.213006   \n",
       "22125                    0.084724                    0.093898   \n",
       "22127                    0.238980                    0.100577   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_thplh  rsfmri_cor_ngd_vs_scs_thprh  \\\n",
       "0                        -0.135962                    -0.060436   \n",
       "1                        -0.060583                    -0.037027   \n",
       "2                         0.020034                    -0.017153   \n",
       "3                        -0.038964                     0.030539   \n",
       "6                        -0.028446                    -0.064175   \n",
       "...                            ...                          ...   \n",
       "22117                    -0.097254                     0.030673   \n",
       "22120                     0.006236                     0.012520   \n",
       "22123                    -0.077175                    -0.049607   \n",
       "22125                     0.000896                    -0.202033   \n",
       "22127                    -0.009229                    -0.027085   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_vtdclh  rsfmri_cor_ngd_vs_scs_vtdcrh  \n",
       "0                         -0.000929                      0.113327  \n",
       "1                          0.021549                     -0.014602  \n",
       "2                         -0.043255                     -0.072356  \n",
       "3                         -0.019395                     -0.060836  \n",
       "6                         -0.066788                      0.010962  \n",
       "...                             ...                           ...  \n",
       "22117                     -0.084058                     -0.002552  \n",
       "22120                      0.060238                     -0.132834  \n",
       "22123                      0.041590                     -0.035196  \n",
       "22125                      0.027023                      0.074749  \n",
       "22127                      0.030794                      0.053762  \n",
       "\n",
       "[11180 rows x 247 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用条件语句将 'cbcl_scr_syn_anxdep_t' 列的值重新分类\n",
    "labels['cbcl_scr_syn_anxdep_t'] = labels['cbcl_scr_syn_anxdep_t'].apply(\n",
    "    lambda x: 0 if x <= 50 else (1 if x <= 64 else 2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aalh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aarh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_aglh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_agrh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_bs</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cdelh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_cderh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_crcxlh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_crcxrh</th>\n",
       "      <th>rsfmri_cor_ngd_au_scs_hplh</th>\n",
       "      <th>...</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_hplh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_hprh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_pllh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_plrh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_ptlh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_ptrh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_thplh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_thprh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_vtdclh</th>\n",
       "      <th>rsfmri_cor_ngd_vs_scs_vtdcrh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.090258</td>\n",
       "      <td>-0.083909</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>-0.054861</td>\n",
       "      <td>-0.048167</td>\n",
       "      <td>0.036012</td>\n",
       "      <td>0.013843</td>\n",
       "      <td>-0.020096</td>\n",
       "      <td>-0.081816</td>\n",
       "      <td>-0.043832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106619</td>\n",
       "      <td>0.112692</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>-0.106608</td>\n",
       "      <td>0.074385</td>\n",
       "      <td>0.119522</td>\n",
       "      <td>-0.135962</td>\n",
       "      <td>-0.060436</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>0.113327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.098593</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>-0.170801</td>\n",
       "      <td>-0.134319</td>\n",
       "      <td>-0.022124</td>\n",
       "      <td>0.092985</td>\n",
       "      <td>0.061854</td>\n",
       "      <td>0.011913</td>\n",
       "      <td>0.013176</td>\n",
       "      <td>-0.010548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026733</td>\n",
       "      <td>-0.061707</td>\n",
       "      <td>0.127464</td>\n",
       "      <td>-0.135847</td>\n",
       "      <td>0.097308</td>\n",
       "      <td>0.104559</td>\n",
       "      <td>-0.060583</td>\n",
       "      <td>-0.037027</td>\n",
       "      <td>0.021549</td>\n",
       "      <td>-0.014602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.065449</td>\n",
       "      <td>0.122893</td>\n",
       "      <td>-0.006246</td>\n",
       "      <td>-0.118878</td>\n",
       "      <td>-0.027118</td>\n",
       "      <td>-0.026523</td>\n",
       "      <td>-0.045971</td>\n",
       "      <td>-0.065351</td>\n",
       "      <td>-0.036497</td>\n",
       "      <td>-0.085342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076594</td>\n",
       "      <td>0.110142</td>\n",
       "      <td>-0.077766</td>\n",
       "      <td>-0.020942</td>\n",
       "      <td>0.120677</td>\n",
       "      <td>0.135012</td>\n",
       "      <td>0.020034</td>\n",
       "      <td>-0.017153</td>\n",
       "      <td>-0.043255</td>\n",
       "      <td>-0.072356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.148170</td>\n",
       "      <td>-0.049085</td>\n",
       "      <td>-0.026814</td>\n",
       "      <td>-0.028232</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>-0.153769</td>\n",
       "      <td>-0.073198</td>\n",
       "      <td>-0.138927</td>\n",
       "      <td>0.032385</td>\n",
       "      <td>-0.063008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099686</td>\n",
       "      <td>0.077307</td>\n",
       "      <td>0.019302</td>\n",
       "      <td>0.064816</td>\n",
       "      <td>0.141727</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>-0.038964</td>\n",
       "      <td>0.030539</td>\n",
       "      <td>-0.019395</td>\n",
       "      <td>-0.060836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.148117</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>-0.104190</td>\n",
       "      <td>-0.037308</td>\n",
       "      <td>0.121178</td>\n",
       "      <td>0.034533</td>\n",
       "      <td>-0.183537</td>\n",
       "      <td>-0.258663</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>-0.051349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036267</td>\n",
       "      <td>-0.058694</td>\n",
       "      <td>-0.152607</td>\n",
       "      <td>-0.076361</td>\n",
       "      <td>0.058996</td>\n",
       "      <td>0.226045</td>\n",
       "      <td>-0.028446</td>\n",
       "      <td>-0.064175</td>\n",
       "      <td>-0.066788</td>\n",
       "      <td>0.010962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>0.030591</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>-0.138875</td>\n",
       "      <td>-0.096117</td>\n",
       "      <td>-0.008607</td>\n",
       "      <td>-0.111538</td>\n",
       "      <td>-0.111535</td>\n",
       "      <td>-0.161508</td>\n",
       "      <td>-0.102951</td>\n",
       "      <td>-0.041270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097498</td>\n",
       "      <td>0.043633</td>\n",
       "      <td>0.055380</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>0.149435</td>\n",
       "      <td>0.025411</td>\n",
       "      <td>-0.097254</td>\n",
       "      <td>0.030673</td>\n",
       "      <td>-0.084058</td>\n",
       "      <td>-0.002552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>-0.089590</td>\n",
       "      <td>-0.002218</td>\n",
       "      <td>-0.150480</td>\n",
       "      <td>0.052007</td>\n",
       "      <td>0.119924</td>\n",
       "      <td>-0.258660</td>\n",
       "      <td>-0.164504</td>\n",
       "      <td>-0.068733</td>\n",
       "      <td>-0.057475</td>\n",
       "      <td>-0.058599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016435</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>0.084433</td>\n",
       "      <td>-0.091015</td>\n",
       "      <td>0.185989</td>\n",
       "      <td>0.038151</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.060238</td>\n",
       "      <td>-0.132834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>-0.112247</td>\n",
       "      <td>-0.010195</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>-0.112359</td>\n",
       "      <td>-0.043313</td>\n",
       "      <td>0.045530</td>\n",
       "      <td>-0.188547</td>\n",
       "      <td>-0.048601</td>\n",
       "      <td>-0.165866</td>\n",
       "      <td>-0.003135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023279</td>\n",
       "      <td>0.049299</td>\n",
       "      <td>0.093359</td>\n",
       "      <td>-0.069631</td>\n",
       "      <td>0.228659</td>\n",
       "      <td>0.213006</td>\n",
       "      <td>-0.077175</td>\n",
       "      <td>-0.049607</td>\n",
       "      <td>0.041590</td>\n",
       "      <td>-0.035196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>0.051633</td>\n",
       "      <td>-0.127992</td>\n",
       "      <td>-0.172488</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>-0.014129</td>\n",
       "      <td>-0.236400</td>\n",
       "      <td>-0.056846</td>\n",
       "      <td>0.076282</td>\n",
       "      <td>-0.055248</td>\n",
       "      <td>-0.004072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062011</td>\n",
       "      <td>-0.025998</td>\n",
       "      <td>0.016382</td>\n",
       "      <td>-0.024677</td>\n",
       "      <td>0.084724</td>\n",
       "      <td>0.093898</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>-0.202033</td>\n",
       "      <td>0.027023</td>\n",
       "      <td>0.074749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>-0.018827</td>\n",
       "      <td>-0.116764</td>\n",
       "      <td>-0.085335</td>\n",
       "      <td>0.049574</td>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.172729</td>\n",
       "      <td>-0.047047</td>\n",
       "      <td>-0.034302</td>\n",
       "      <td>-0.117179</td>\n",
       "      <td>-0.025159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058867</td>\n",
       "      <td>0.065065</td>\n",
       "      <td>0.018944</td>\n",
       "      <td>-0.033643</td>\n",
       "      <td>0.238980</td>\n",
       "      <td>0.100577</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>-0.027085</td>\n",
       "      <td>0.030794</td>\n",
       "      <td>0.053762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11180 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rsfmri_cor_ngd_au_scs_aalh  rsfmri_cor_ngd_au_scs_aarh  \\\n",
       "0                       -0.090258                   -0.083909   \n",
       "1                       -0.098593                    0.093085   \n",
       "2                       -0.065449                    0.122893   \n",
       "3                       -0.148170                   -0.049085   \n",
       "6                       -0.148117                    0.037320   \n",
       "...                           ...                         ...   \n",
       "22117                    0.030591                    0.003116   \n",
       "22120                   -0.089590                   -0.002218   \n",
       "22123                   -0.112247                   -0.010195   \n",
       "22125                    0.051633                   -0.127992   \n",
       "22127                   -0.018827                   -0.116764   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_aglh  rsfmri_cor_ngd_au_scs_agrh  \\\n",
       "0                        0.004247                   -0.054861   \n",
       "1                       -0.170801                   -0.134319   \n",
       "2                       -0.006246                   -0.118878   \n",
       "3                       -0.026814                   -0.028232   \n",
       "6                       -0.104190                   -0.037308   \n",
       "...                           ...                         ...   \n",
       "22117                   -0.138875                   -0.096117   \n",
       "22120                   -0.150480                    0.052007   \n",
       "22123                    0.038551                   -0.112359   \n",
       "22125                   -0.172488                    0.013249   \n",
       "22127                   -0.085335                    0.049574   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_bs  rsfmri_cor_ngd_au_scs_cdelh  \\\n",
       "0                     -0.048167                     0.036012   \n",
       "1                     -0.022124                     0.092985   \n",
       "2                     -0.027118                    -0.026523   \n",
       "3                      0.008625                    -0.153769   \n",
       "6                      0.121178                     0.034533   \n",
       "...                         ...                          ...   \n",
       "22117                 -0.008607                    -0.111538   \n",
       "22120                  0.119924                    -0.258660   \n",
       "22123                 -0.043313                     0.045530   \n",
       "22125                 -0.014129                    -0.236400   \n",
       "22127                  0.160182                     0.172729   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_cderh  rsfmri_cor_ngd_au_scs_crcxlh  \\\n",
       "0                         0.013843                     -0.020096   \n",
       "1                         0.061854                      0.011913   \n",
       "2                        -0.045971                     -0.065351   \n",
       "3                        -0.073198                     -0.138927   \n",
       "6                        -0.183537                     -0.258663   \n",
       "...                            ...                           ...   \n",
       "22117                    -0.111535                     -0.161508   \n",
       "22120                    -0.164504                     -0.068733   \n",
       "22123                    -0.188547                     -0.048601   \n",
       "22125                    -0.056846                      0.076282   \n",
       "22127                    -0.047047                     -0.034302   \n",
       "\n",
       "       rsfmri_cor_ngd_au_scs_crcxrh  rsfmri_cor_ngd_au_scs_hplh  ...  \\\n",
       "0                         -0.081816                   -0.043832  ...   \n",
       "1                          0.013176                   -0.010548  ...   \n",
       "2                         -0.036497                   -0.085342  ...   \n",
       "3                          0.032385                   -0.063008  ...   \n",
       "6                          0.003149                   -0.051349  ...   \n",
       "...                             ...                         ...  ...   \n",
       "22117                     -0.102951                   -0.041270  ...   \n",
       "22120                     -0.057475                   -0.058599  ...   \n",
       "22123                     -0.165866                   -0.003135  ...   \n",
       "22125                     -0.055248                   -0.004072  ...   \n",
       "22127                     -0.117179                   -0.025159  ...   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_hplh  rsfmri_cor_ngd_vs_scs_hprh  \\\n",
       "0                        0.106619                    0.112692   \n",
       "1                       -0.026733                   -0.061707   \n",
       "2                        0.076594                    0.110142   \n",
       "3                        0.099686                    0.077307   \n",
       "6                       -0.036267                   -0.058694   \n",
       "...                           ...                         ...   \n",
       "22117                    0.097498                    0.043633   \n",
       "22120                    0.016435                    0.048579   \n",
       "22123                   -0.023279                    0.049299   \n",
       "22125                    0.062011                   -0.025998   \n",
       "22127                    0.058867                    0.065065   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_pllh  rsfmri_cor_ngd_vs_scs_plrh  \\\n",
       "0                        0.013191                   -0.106608   \n",
       "1                        0.127464                   -0.135847   \n",
       "2                       -0.077766                   -0.020942   \n",
       "3                        0.019302                    0.064816   \n",
       "6                       -0.152607                   -0.076361   \n",
       "...                           ...                         ...   \n",
       "22117                    0.055380                    0.102299   \n",
       "22120                    0.084433                   -0.091015   \n",
       "22123                    0.093359                   -0.069631   \n",
       "22125                    0.016382                   -0.024677   \n",
       "22127                    0.018944                   -0.033643   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_ptlh  rsfmri_cor_ngd_vs_scs_ptrh  \\\n",
       "0                        0.074385                    0.119522   \n",
       "1                        0.097308                    0.104559   \n",
       "2                        0.120677                    0.135012   \n",
       "3                        0.141727                    0.010190   \n",
       "6                        0.058996                    0.226045   \n",
       "...                           ...                         ...   \n",
       "22117                    0.149435                    0.025411   \n",
       "22120                    0.185989                    0.038151   \n",
       "22123                    0.228659                    0.213006   \n",
       "22125                    0.084724                    0.093898   \n",
       "22127                    0.238980                    0.100577   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_thplh  rsfmri_cor_ngd_vs_scs_thprh  \\\n",
       "0                        -0.135962                    -0.060436   \n",
       "1                        -0.060583                    -0.037027   \n",
       "2                         0.020034                    -0.017153   \n",
       "3                        -0.038964                     0.030539   \n",
       "6                        -0.028446                    -0.064175   \n",
       "...                            ...                          ...   \n",
       "22117                    -0.097254                     0.030673   \n",
       "22120                     0.006236                     0.012520   \n",
       "22123                    -0.077175                    -0.049607   \n",
       "22125                     0.000896                    -0.202033   \n",
       "22127                    -0.009229                    -0.027085   \n",
       "\n",
       "       rsfmri_cor_ngd_vs_scs_vtdclh  rsfmri_cor_ngd_vs_scs_vtdcrh  \n",
       "0                         -0.000929                      0.113327  \n",
       "1                          0.021549                     -0.014602  \n",
       "2                         -0.043255                     -0.072356  \n",
       "3                         -0.019395                     -0.060836  \n",
       "6                         -0.066788                      0.010962  \n",
       "...                             ...                           ...  \n",
       "22117                     -0.084058                     -0.002552  \n",
       "22120                      0.060238                     -0.132834  \n",
       "22123                      0.041590                     -0.035196  \n",
       "22125                      0.027023                      0.074749  \n",
       "22127                      0.030794                      0.053762  \n",
       "\n",
       "[11180 rows x 247 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.data import sampler\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 49\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 30\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "class Trim(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :28, :28]  # 裁剪到指定尺寸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Reshape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m---> 32\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mAutoEncoder.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      5\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m),     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      6\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m3136\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m     16\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3136\u001b[39m),\n\u001b[1;32m---> 17\u001b[0m         \u001b[43mReshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m),\n\u001b[0;32m     18\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConvTranspose2d(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     19\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.01\u001b[39m),\n\u001b[0;32m     20\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConvTranspose2d(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     21\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.01\u001b[39m),\n\u001b[0;32m     22\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConvTranspose2d(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m, stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     23\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.01\u001b[39m),\n\u001b[0;32m     24\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConvTranspose2d(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m, stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     25\u001b[0m         Trim(),  \u001b[38;5;66;03m# 1x29x29 -> 1x28x28\u001b[39;00m\n\u001b[0;32m     26\u001b[0m         nn\u001b[38;5;241m.\u001b[39mSigmoid()\n\u001b[0;32m     27\u001b[0m         )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Reshape' is not defined"
     ]
    }
   ],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3),     padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(3136, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "                torch.nn.Linear(2, 3136),\n",
    "                Reshape(-1, 64, 7, 7),\n",
    "                nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0),\n",
    "                Trim(),  # 1x29x29 -> 1x28x28\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "model = AutoEncoder()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(num_epochs, model, optimizer,\n",
    "                         train_loader, loss_fn=None,\n",
    "                         logging_interval=100,\n",
    "                         skip_epoch_stats=False,\n",
    "                         save_model=None):\n",
    "    log_dict = {'train_loss_per_batch': [],\n",
    "                'train_loss_per_epoch': []}\n",
    "    if loss_fn is None:\n",
    "        loss_fn = F.mse_loss\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (features, _) in enumerate(train_loader):\n",
    "            # FORWARD AND BACK PROP\n",
    "            logits = model(features)\n",
    "            loss = loss_fn(logits, features)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "            # LOGGING\n",
    "            log_dict['train_loss_per_batch'].append(loss.item())\n",
    "            if not batch_idx % logging_interval:\n",
    "                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
    "                      % (epoch+1, num_epochs, batch_idx,\n",
    "                          len(train_loader), loss))\n",
    "        if not skip_epoch_stats:\n",
    "            model.eval()\n",
    "            with torch.set_grad_enabled(False):  # save memory during inference\n",
    "                train_loss = compute_epoch_loss_autoencoder(\n",
    "                    model, train_loader, loss_fn)\n",
    "                print('***Epoch: %03d/%03d | Loss: %.3f' % (\n",
    "                      epoch+1, num_epochs, train_loss))\n",
    "                log_dict['train_loss_per_epoch'].append(train_loss.item())\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "    if save_model is not None:\n",
    "        torch.save(model.state_dict(), save_model)\n",
    "    return log_dict\n",
    "log_dict = train_autoencoder(num_epochs=NUM_EPOCHS, model=model,\n",
    "                                optimizer=optimizer,\n",
    "                                train_loader=train_loader,\n",
    "                                skip_epoch_stats=True,\n",
    "                                logging_interval=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4610 - mse: 0.4610 - val_loss: 0.4071 - val_mse: 0.4073\n",
      "Epoch 2/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.3982 - mse: 0.3982 - val_loss: 0.3991 - val_mse: 0.3993\n",
      "Epoch 3/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 0.3918 - mse: 0.3918 - val_loss: 0.3970 - val_mse: 0.3973\n",
      "Epoch 4/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 0.4005 - mse: 0.4005 - val_loss: 0.3968 - val_mse: 0.3970\n",
      "Epoch 5/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.3983 - mse: 0.3983 - val_loss: 0.3991 - val_mse: 0.3993\n",
      "Epoch 6/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.4041 - mse: 0.4041 - val_loss: 0.3976 - val_mse: 0.3978\n",
      "Epoch 7/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 0.4005 - mse: 0.4005 - val_loss: 0.3992 - val_mse: 0.3994\n",
      "Epoch 8/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.3882 - mse: 0.3882 - val_loss: 0.4004 - val_mse: 0.4006\n",
      "Epoch 9/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.4108 - mse: 0.4108 - val_loss: 0.3968 - val_mse: 0.3970\n",
      "Epoch 10/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.4016 - mse: 0.4016 - val_loss: 0.4000 - val_mse: 0.4002\n",
      "Epoch 11/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.4024 - mse: 0.4024 - val_loss: 0.3985 - val_mse: 0.3987\n",
      "Epoch 12/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 0.3955 - mse: 0.3955 - val_loss: 0.4127 - val_mse: 0.4128\n",
      "Epoch 13/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 0.3995 - mse: 0.3995 - val_loss: 0.3965 - val_mse: 0.3967\n",
      "Epoch 14/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 0.4053 - mse: 0.4053 - val_loss: 0.3978 - val_mse: 0.3980\n",
      "Epoch 15/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.4014 - mse: 0.4014 - val_loss: 0.3965 - val_mse: 0.3967\n",
      "Epoch 16/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.4072 - mse: 0.4072 - val_loss: 0.3968 - val_mse: 0.3970\n",
      "Epoch 17/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 0.3999 - mse: 0.3999 - val_loss: 0.3978 - val_mse: 0.3980\n",
      "Epoch 18/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.3955 - mse: 0.3955 - val_loss: 0.3993 - val_mse: 0.3995\n",
      "Epoch 19/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 0.3865 - mse: 0.3865 - val_loss: 0.3988 - val_mse: 0.3991\n",
      "Epoch 20/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 0.4008 - mse: 0.4008 - val_loss: 0.4012 - val_mse: 0.4014\n",
      "Epoch 21/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.3997 - mse: 0.3997 - val_loss: 0.3981 - val_mse: 0.3984\n",
      "Epoch 22/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.4016 - mse: 0.4016 - val_loss: 0.3982 - val_mse: 0.3984\n",
      "Epoch 23/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 0.3958 - mse: 0.3958 - val_loss: 0.3968 - val_mse: 0.3970\n",
      "Epoch 24/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.4001 - mse: 0.4001 - val_loss: 0.3964 - val_mse: 0.3966\n",
      "Epoch 25/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 0.3828 - mse: 0.3828 - val_loss: 0.3969 - val_mse: 0.3971\n",
      "Epoch 26/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 0.3882 - mse: 0.3882 - val_loss: 0.3979 - val_mse: 0.3981\n",
      "Epoch 27/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 0.3996 - mse: 0.3996 - val_loss: 0.3965 - val_mse: 0.3967\n",
      "Epoch 28/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.3928 - mse: 0.3928 - val_loss: 0.3989 - val_mse: 0.3991\n",
      "Epoch 29/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.3841 - mse: 0.3841 - val_loss: 0.3965 - val_mse: 0.3968\n",
      "Epoch 30/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.4020 - mse: 0.4020 - val_loss: 0.3966 - val_mse: 0.3968\n",
      "Epoch 31/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 0.3922 - mse: 0.3922 - val_loss: 0.3963 - val_mse: 0.3965\n",
      "Epoch 32/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 0.3983 - mse: 0.3983 - val_loss: 0.3964 - val_mse: 0.3966\n",
      "Epoch 33/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.3935 - mse: 0.3935 - val_loss: 0.3968 - val_mse: 0.3970\n",
      "Epoch 34/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.4097 - mse: 0.4097 - val_loss: 0.3965 - val_mse: 0.3967\n",
      "Epoch 35/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 0.3848 - mse: 0.3848 - val_loss: 0.3968 - val_mse: 0.3970\n",
      "Epoch 36/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.3890 - mse: 0.3890 - val_loss: 0.3962 - val_mse: 0.3964\n",
      "Epoch 37/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.3904 - mse: 0.3904 - val_loss: 0.3963 - val_mse: 0.3965\n",
      "Epoch 38/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 0.3971 - mse: 0.3971 - val_loss: 0.3966 - val_mse: 0.3968\n",
      "Epoch 39/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 0.4000 - mse: 0.4000 - val_loss: 0.3971 - val_mse: 0.3972\n",
      "Epoch 40/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 0.3895 - mse: 0.3895 - val_loss: 0.3965 - val_mse: 0.3967\n",
      "Epoch 41/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 0.3893 - mse: 0.3893 - val_loss: 0.3963 - val_mse: 0.3965\n",
      "Epoch 42/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 0.3932 - mse: 0.3932 - val_loss: 0.3965 - val_mse: 0.3967\n",
      "Epoch 43/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.3866 - mse: 0.3866 - val_loss: 0.3961 - val_mse: 0.3963\n",
      "Epoch 44/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - loss: 0.3871 - mse: 0.3871 - val_loss: 0.4040 - val_mse: 0.4041\n",
      "Epoch 45/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 0.4017 - mse: 0.4017 - val_loss: 0.3976 - val_mse: 0.3978\n",
      "Epoch 46/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4035 - mse: 0.4035 - val_loss: 0.3961 - val_mse: 0.3963\n",
      "Epoch 47/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 0.3870 - mse: 0.3870 - val_loss: 0.3969 - val_mse: 0.3971\n",
      "Epoch 48/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.3969 - mse: 0.3969 - val_loss: 0.3983 - val_mse: 0.3985\n",
      "Epoch 49/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.3918 - mse: 0.3918 - val_loss: 0.3973 - val_mse: 0.3975\n",
      "Epoch 50/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 0.3995 - mse: 0.3995 - val_loss: 0.3969 - val_mse: 0.3971\n",
      "Epoch 51/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 0.3870 - mse: 0.3870 - val_loss: 0.3967 - val_mse: 0.3969\n",
      "Epoch 52/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 0.3874 - mse: 0.3874 - val_loss: 0.3966 - val_mse: 0.3968\n",
      "Epoch 53/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.4006 - mse: 0.4006 - val_loss: 0.3966 - val_mse: 0.3968\n",
      "Epoch 54/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 0.3901 - mse: 0.3901 - val_loss: 0.3966 - val_mse: 0.3968\n",
      "Epoch 55/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 0.3939 - mse: 0.3939 - val_loss: 0.3991 - val_mse: 0.3993\n",
      "Epoch 56/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3889 - mse: 0.3889 - val_loss: 0.3963 - val_mse: 0.3966\n",
      "Epoch 57/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 0.4011 - mse: 0.4011 - val_loss: 0.3963 - val_mse: 0.3965\n",
      "Epoch 58/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 0.3925 - mse: 0.3925 - val_loss: 0.3987 - val_mse: 0.3989\n",
      "Epoch 59/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 0.4037 - mse: 0.4037 - val_loss: 0.3970 - val_mse: 0.3972\n",
      "Epoch 60/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 0.3805 - mse: 0.3805 - val_loss: 0.3967 - val_mse: 0.3969\n",
      "Epoch 61/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 0.3931 - mse: 0.3931 - val_loss: 0.3965 - val_mse: 0.3967\n",
      "Epoch 62/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 0.3889 - mse: 0.3889 - val_loss: 0.3966 - val_mse: 0.3968\n",
      "Epoch 63/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 0.3861 - mse: 0.3861 - val_loss: 0.4021 - val_mse: 0.4022\n",
      "Epoch 64/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3988 - mse: 0.3988 - val_loss: 0.3971 - val_mse: 0.3973\n",
      "Epoch 65/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.4016 - mse: 0.4016 - val_loss: 0.3974 - val_mse: 0.3976\n",
      "Epoch 66/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 0.3947 - mse: 0.3947 - val_loss: 0.3971 - val_mse: 0.3974\n",
      "Epoch 67/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3983 - mse: 0.3983 - val_loss: 0.3981 - val_mse: 0.3983\n",
      "Epoch 68/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 0.3889 - mse: 0.3889 - val_loss: 0.3993 - val_mse: 0.3995\n",
      "Epoch 69/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4007 - mse: 0.4007 - val_loss: 0.3975 - val_mse: 0.3977\n",
      "Epoch 70/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 0.3880 - mse: 0.3880 - val_loss: 0.3973 - val_mse: 0.3975\n",
      "Epoch 71/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 0.3910 - mse: 0.3910 - val_loss: 0.3999 - val_mse: 0.4001\n",
      "Epoch 72/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3979 - mse: 0.3979 - val_loss: 0.3979 - val_mse: 0.3982\n",
      "Epoch 73/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3843 - mse: 0.3843 - val_loss: 0.3985 - val_mse: 0.3987\n",
      "Epoch 74/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.3886 - mse: 0.3886 - val_loss: 0.3996 - val_mse: 0.3997\n",
      "Epoch 75/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 0.3970 - mse: 0.3970 - val_loss: 0.3973 - val_mse: 0.3975\n",
      "Epoch 76/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3846 - mse: 0.3846 - val_loss: 0.3972 - val_mse: 0.3974\n",
      "Epoch 77/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 0.3958 - mse: 0.3958 - val_loss: 0.3969 - val_mse: 0.3971\n",
      "Epoch 78/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.3975 - mse: 0.3975 - val_loss: 0.3972 - val_mse: 0.3974\n",
      "Epoch 79/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.3913 - mse: 0.3913 - val_loss: 0.3978 - val_mse: 0.3981\n",
      "Epoch 80/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.3936 - mse: 0.3936 - val_loss: 0.3979 - val_mse: 0.3980\n",
      "Epoch 81/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3904 - mse: 0.3904 - val_loss: 0.3977 - val_mse: 0.3978\n",
      "Epoch 82/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.3901 - mse: 0.3901 - val_loss: 0.3969 - val_mse: 0.3971\n",
      "Epoch 83/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.3973 - mse: 0.3973 - val_loss: 0.4004 - val_mse: 0.4007\n",
      "Epoch 84/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3886 - mse: 0.3886 - val_loss: 0.3971 - val_mse: 0.3973\n",
      "Epoch 85/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.3909 - mse: 0.3909 - val_loss: 0.3975 - val_mse: 0.3977\n",
      "Epoch 86/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.3890 - mse: 0.3890 - val_loss: 0.3979 - val_mse: 0.3981\n",
      "Epoch 87/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.3972 - mse: 0.3972 - val_loss: 0.4001 - val_mse: 0.4002\n",
      "Epoch 88/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.3878 - mse: 0.3878 - val_loss: 0.4132 - val_mse: 0.4133\n",
      "Epoch 89/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 0.3968 - mse: 0.3968 - val_loss: 0.3978 - val_mse: 0.3980\n",
      "Epoch 90/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 0.3780 - mse: 0.3780 - val_loss: 0.3987 - val_mse: 0.3989\n",
      "Epoch 91/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 0.3938 - mse: 0.3938 - val_loss: 0.3985 - val_mse: 0.3987\n",
      "Epoch 92/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3955 - mse: 0.3955 - val_loss: 0.3983 - val_mse: 0.3984\n",
      "Epoch 93/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 0.3954 - mse: 0.3954 - val_loss: 0.3982 - val_mse: 0.3984\n",
      "Epoch 94/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 0.3863 - mse: 0.3863 - val_loss: 0.3994 - val_mse: 0.3996\n",
      "Epoch 95/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.3894 - mse: 0.3894 - val_loss: 0.4018 - val_mse: 0.4019\n",
      "Epoch 96/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 0.3905 - mse: 0.3905 - val_loss: 0.3989 - val_mse: 0.3991\n",
      "Epoch 97/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.3911 - mse: 0.3911 - val_loss: 0.3977 - val_mse: 0.3979\n",
      "Epoch 98/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.3842 - mse: 0.3842 - val_loss: 0.3979 - val_mse: 0.3981\n",
      "Epoch 99/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.3958 - mse: 0.3958 - val_loss: 0.4013 - val_mse: 0.4015\n",
      "Epoch 100/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 0.3969 - mse: 0.3969 - val_loss: 0.3990 - val_mse: 0.3992\n",
      "Epoch 101/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 0.3973 - mse: 0.3973 - val_loss: 0.4026 - val_mse: 0.4028\n",
      "Epoch 102/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 0.3834 - mse: 0.3834 - val_loss: 0.4052 - val_mse: 0.4053\n",
      "Epoch 103/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 0.3884 - mse: 0.3884 - val_loss: 0.4008 - val_mse: 0.4010\n",
      "Epoch 104/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 0.3871 - mse: 0.3871 - val_loss: 0.3988 - val_mse: 0.3990\n",
      "Epoch 105/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.3835 - mse: 0.3835 - val_loss: 0.3990 - val_mse: 0.3992\n",
      "Epoch 106/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.3884 - mse: 0.3884 - val_loss: 0.3985 - val_mse: 0.3987\n",
      "Epoch 107/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 0.3847 - mse: 0.3847 - val_loss: 0.3982 - val_mse: 0.3984\n",
      "Epoch 108/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 0.4054 - mse: 0.4054 - val_loss: 0.3984 - val_mse: 0.3986\n",
      "Epoch 109/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 0.3896 - mse: 0.3896 - val_loss: 0.3982 - val_mse: 0.3984\n",
      "Epoch 110/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.3829 - mse: 0.3829 - val_loss: 0.4053 - val_mse: 0.4054\n",
      "Epoch 111/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.3902 - mse: 0.3902 - val_loss: 0.3983 - val_mse: 0.3985\n",
      "Epoch 112/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3842 - mse: 0.3842 - val_loss: 0.4004 - val_mse: 0.4005\n",
      "Epoch 113/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.3952 - mse: 0.3952 - val_loss: 0.3972 - val_mse: 0.3974\n",
      "Epoch 114/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3992 - mse: 0.3992 - val_loss: 0.3989 - val_mse: 0.3991\n",
      "Epoch 115/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 0.3863 - mse: 0.3863 - val_loss: 0.3982 - val_mse: 0.3985\n",
      "Epoch 116/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3858 - mse: 0.3858 - val_loss: 0.3986 - val_mse: 0.3988\n",
      "Epoch 117/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3892 - mse: 0.3892 - val_loss: 0.3989 - val_mse: 0.3991\n",
      "Epoch 118/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - loss: 0.3919 - mse: 0.3919 - val_loss: 0.4008 - val_mse: 0.4010\n",
      "Epoch 119/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 0.3927 - mse: 0.3927 - val_loss: 0.3989 - val_mse: 0.3991\n",
      "Epoch 120/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 0.3978 - mse: 0.3978 - val_loss: 0.3988 - val_mse: 0.3990\n",
      "Epoch 121/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.3967 - mse: 0.3967 - val_loss: 0.3991 - val_mse: 0.3993\n",
      "Epoch 122/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 0.3879 - mse: 0.3879 - val_loss: 0.4004 - val_mse: 0.4006\n",
      "Epoch 123/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 0.4010 - mse: 0.4010 - val_loss: 0.3986 - val_mse: 0.3988\n",
      "Epoch 124/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 0.3933 - mse: 0.3933 - val_loss: 0.3995 - val_mse: 0.3997\n",
      "Epoch 125/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.3978 - mse: 0.3978 - val_loss: 0.4029 - val_mse: 0.4031\n",
      "Epoch 126/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 0.3901 - mse: 0.3901 - val_loss: 0.4011 - val_mse: 0.4012\n",
      "Epoch 127/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.3929 - mse: 0.3929 - val_loss: 0.3997 - val_mse: 0.3999\n",
      "Epoch 128/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 0.3889 - mse: 0.3889 - val_loss: 0.4034 - val_mse: 0.4035\n",
      "Epoch 129/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3850 - mse: 0.3850 - val_loss: 0.3988 - val_mse: 0.3990\n",
      "Epoch 130/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.3827 - mse: 0.3827 - val_loss: 0.4017 - val_mse: 0.4018\n",
      "Epoch 131/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3887 - mse: 0.3887 - val_loss: 0.3992 - val_mse: 0.3993\n",
      "Epoch 132/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 0.3908 - mse: 0.3908 - val_loss: 0.3994 - val_mse: 0.3996\n",
      "Epoch 133/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 0.3804 - mse: 0.3804 - val_loss: 0.4073 - val_mse: 0.4074\n",
      "Epoch 134/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 0.3930 - mse: 0.3930 - val_loss: 0.4014 - val_mse: 0.4015\n",
      "Epoch 135/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 0.3953 - mse: 0.3953 - val_loss: 0.3992 - val_mse: 0.3994\n",
      "Epoch 136/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 0.3865 - mse: 0.3865 - val_loss: 0.4122 - val_mse: 0.4123\n",
      "Epoch 137/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 0.3924 - mse: 0.3924 - val_loss: 0.4006 - val_mse: 0.4008\n",
      "Epoch 138/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 0.3935 - mse: 0.3935 - val_loss: 0.4014 - val_mse: 0.4016\n",
      "Epoch 139/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 0.3814 - mse: 0.3814 - val_loss: 0.3996 - val_mse: 0.3998\n",
      "Epoch 140/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - loss: 0.3908 - mse: 0.3908 - val_loss: 0.3997 - val_mse: 0.3998\n",
      "Epoch 141/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.3999 - mse: 0.3999 - val_loss: 0.3999 - val_mse: 0.4000\n",
      "Epoch 142/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 0.3977 - mse: 0.3977 - val_loss: 0.4010 - val_mse: 0.4012\n",
      "Epoch 143/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.3896 - mse: 0.3896 - val_loss: 0.4042 - val_mse: 0.4043\n",
      "Epoch 144/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.3912 - mse: 0.3912 - val_loss: 0.4007 - val_mse: 0.4009\n",
      "Epoch 145/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3808 - mse: 0.3808 - val_loss: 0.4053 - val_mse: 0.4055\n",
      "Epoch 146/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.3873 - mse: 0.3873 - val_loss: 0.4023 - val_mse: 0.4024\n",
      "Epoch 147/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3854 - mse: 0.3854 - val_loss: 0.4079 - val_mse: 0.4080\n",
      "Epoch 148/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 0.3874 - mse: 0.3874 - val_loss: 0.3999 - val_mse: 0.4001\n",
      "Epoch 149/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.3853 - mse: 0.3853 - val_loss: 0.4007 - val_mse: 0.4008\n",
      "Epoch 150/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 0.3954 - mse: 0.3954 - val_loss: 0.4019 - val_mse: 0.4020\n",
      "Epoch 151/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 0.3878 - mse: 0.3878 - val_loss: 0.4007 - val_mse: 0.4009\n",
      "Epoch 152/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 0.3908 - mse: 0.3908 - val_loss: 0.4023 - val_mse: 0.4024\n",
      "Epoch 153/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.3908 - mse: 0.3908 - val_loss: 0.4020 - val_mse: 0.4021\n",
      "Epoch 154/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 0.3927 - mse: 0.3927 - val_loss: 0.4060 - val_mse: 0.4062\n",
      "Epoch 155/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3878 - mse: 0.3878 - val_loss: 0.4002 - val_mse: 0.4003\n",
      "Epoch 156/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3940 - mse: 0.3940 - val_loss: 0.4021 - val_mse: 0.4023\n",
      "Epoch 157/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 0.3878 - mse: 0.3878 - val_loss: 0.4009 - val_mse: 0.4011\n",
      "Epoch 158/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step - loss: 0.3934 - mse: 0.3934 - val_loss: 0.4007 - val_mse: 0.4009\n",
      "Epoch 159/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 0.3798 - mse: 0.3798 - val_loss: 0.4154 - val_mse: 0.4155\n",
      "Epoch 160/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.3882 - mse: 0.3882 - val_loss: 0.4006 - val_mse: 0.4008\n",
      "Epoch 161/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3957 - mse: 0.3957 - val_loss: 0.4007 - val_mse: 0.4009\n",
      "Epoch 162/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3890 - mse: 0.3890 - val_loss: 0.3998 - val_mse: 0.4000\n",
      "Epoch 163/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3869 - mse: 0.3869 - val_loss: 0.4007 - val_mse: 0.4009\n",
      "Epoch 164/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 0.3854 - mse: 0.3854 - val_loss: 0.4073 - val_mse: 0.4075\n",
      "Epoch 165/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3838 - mse: 0.3838 - val_loss: 0.4005 - val_mse: 0.4007\n",
      "Epoch 166/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 0.3912 - mse: 0.3912 - val_loss: 0.4051 - val_mse: 0.4053\n",
      "Epoch 167/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 0.3924 - mse: 0.3924 - val_loss: 0.4012 - val_mse: 0.4014\n",
      "Epoch 168/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3956 - mse: 0.3956 - val_loss: 0.4009 - val_mse: 0.4011\n",
      "Epoch 169/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3873 - mse: 0.3873 - val_loss: 0.4025 - val_mse: 0.4027\n",
      "Epoch 170/200\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 0.3874 - mse: 0.3874 - val_loss: 0.4133 - val_mse: 0.4134\n",
      "Epoch 171/200\n",
      "\u001b[1m150/224\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.4003 - mse: 0.4003"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 预测\u001b[39;00m\n\u001b[0;32m     30\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:326\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    324\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m    325\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m--> 326\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m(\n\u001b[0;32m    327\u001b[0m     step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    328\u001b[0m )\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 将数据划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels.iloc[:, [0]], test_size=0.2)\n",
    "\n",
    "# 将标签转换为 numpy 数组并进行形状调整\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 构建神经网络模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # 输出层，回归问题使用线性激活\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 评估模型性能\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"均方误差 (MSE):\", mse)\n",
    "print(\"R^2 评分:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_154\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_154\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_133 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">247</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_114 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_115 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_147 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_133 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m247\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_114 (\u001b[38;5;33mSequential\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m15,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_115 (\u001b[38;5;33mSequential\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_116 (\u001b[38;5;33mSequential\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_147 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │        \u001b[38;5;34m13,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,192</span> (145.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,192\u001b[0m (145.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,192</span> (145.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,192\u001b[0m (145.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 定义一个 MLP Block\n",
    "def MLPBlock(units, dropout_rate):\n",
    "    return keras.Sequential([\n",
    "        layers.Dense(units, activation='relu'),  # 线性层 + ReLU 激活函数\n",
    "        layers.Dropout(dropout_rate)             # Dropout 层\n",
    "    ])\n",
    "\n",
    "# 构建完整的 MLP 模型\n",
    "def build_mlp_model(input_shape, num_blocks, units_per_block, dropout_rate):\n",
    "    inputs = keras.Input(shape=(input_shape,))\n",
    "    x = inputs\n",
    "\n",
    "    # 堆叠多个 MLP Block\n",
    "    for _ in range(num_blocks):\n",
    "        x = MLPBlock(units_per_block, dropout_rate)(x)\n",
    "\n",
    "    # 输出层，三分类问题使用 softmax 激活函数\n",
    "        outputs = layers.Dense(200)(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 示例：构建模型\n",
    "input_shape = 247  # 输入特征数\n",
    "num_blocks = 3  # 堆叠 3 个 MLP Block\n",
    "units_per_block = 64  # 每个 MLP Block 的神经元数\n",
    "dropout_rate = 0.3  # Dropout 概率\n",
    "\n",
    "model = build_mlp_model(input_shape, num_blocks, units_per_block, dropout_rate)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# 查看模型结构\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4079 - loss: 2.1209 - val_accuracy: 0.5067 - val_loss: 0.9556\n",
      "Epoch 2/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4990 - loss: 1.0486 - val_accuracy: 0.5040 - val_loss: 0.9487\n",
      "Epoch 3/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4990 - loss: 1.0095 - val_accuracy: 0.5036 - val_loss: 0.9284\n",
      "Epoch 4/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4969 - loss: 0.9802 - val_accuracy: 0.5067 - val_loss: 0.9128\n",
      "Epoch 5/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5005 - loss: 0.9694 - val_accuracy: 0.5067 - val_loss: 0.9081\n",
      "Epoch 6/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4941 - loss: 0.9564 - val_accuracy: 0.5067 - val_loss: 0.9065\n",
      "Epoch 7/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4952 - loss: 0.9262 - val_accuracy: 0.5067 - val_loss: 0.9060\n",
      "Epoch 8/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5083 - loss: 0.9273 - val_accuracy: 0.5072 - val_loss: 0.9062\n",
      "Epoch 9/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5028 - loss: 0.9178 - val_accuracy: 0.5067 - val_loss: 0.9068\n",
      "Epoch 10/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5049 - loss: 0.9210 - val_accuracy: 0.5067 - val_loss: 0.9058\n",
      "Epoch 11/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4997 - loss: 0.9072 - val_accuracy: 0.5067 - val_loss: 0.9066\n",
      "Epoch 12/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5132 - loss: 0.9006 - val_accuracy: 0.5067 - val_loss: 0.9052\n",
      "Epoch 13/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5060 - loss: 0.9027 - val_accuracy: 0.5067 - val_loss: 0.9060\n",
      "Epoch 14/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4955 - loss: 0.9133 - val_accuracy: 0.5067 - val_loss: 0.9061\n",
      "Epoch 15/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5077 - loss: 0.9016 - val_accuracy: 0.5067 - val_loss: 0.9077\n",
      "Epoch 16/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5074 - loss: 0.8963 - val_accuracy: 0.5067 - val_loss: 0.9081\n",
      "Epoch 17/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5035 - loss: 0.8972 - val_accuracy: 0.5067 - val_loss: 0.9063\n",
      "Epoch 18/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5029 - loss: 0.8928 - val_accuracy: 0.5067 - val_loss: 0.9104\n",
      "Epoch 19/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5085 - loss: 0.8915 - val_accuracy: 0.5067 - val_loss: 0.9076\n",
      "Epoch 20/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5066 - loss: 0.8887 - val_accuracy: 0.5067 - val_loss: 0.9074\n",
      "Epoch 21/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5096 - loss: 0.8851 - val_accuracy: 0.5067 - val_loss: 0.9039\n",
      "Epoch 22/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4947 - loss: 0.8922 - val_accuracy: 0.5067 - val_loss: 0.9124\n",
      "Epoch 23/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4976 - loss: 0.8929 - val_accuracy: 0.5067 - val_loss: 0.9164\n",
      "Epoch 24/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5057 - loss: 0.8907 - val_accuracy: 0.5067 - val_loss: 0.9173\n",
      "Epoch 25/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5056 - loss: 0.8831 - val_accuracy: 0.5067 - val_loss: 0.9157\n",
      "Epoch 26/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4986 - loss: 0.8698 - val_accuracy: 0.5067 - val_loss: 0.9161\n",
      "Epoch 27/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4978 - loss: 0.8811 - val_accuracy: 0.5067 - val_loss: 0.9228\n",
      "Epoch 28/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5035 - loss: 0.8688 - val_accuracy: 0.5067 - val_loss: 0.9239\n",
      "Epoch 29/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5089 - loss: 0.8613 - val_accuracy: 0.5067 - val_loss: 0.9365\n",
      "Epoch 30/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5006 - loss: 0.8641 - val_accuracy: 0.5072 - val_loss: 0.9355\n",
      "Epoch 31/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5102 - loss: 0.8589 - val_accuracy: 0.5072 - val_loss: 0.9291\n",
      "Epoch 32/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5153 - loss: 0.8539 - val_accuracy: 0.5072 - val_loss: 0.9274\n",
      "Epoch 33/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5105 - loss: 0.8578 - val_accuracy: 0.5067 - val_loss: 0.9278\n",
      "Epoch 34/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5017 - loss: 0.8477 - val_accuracy: 0.5067 - val_loss: 0.9278\n",
      "Epoch 35/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5026 - loss: 0.8546 - val_accuracy: 0.5067 - val_loss: 0.9435\n",
      "Epoch 36/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5008 - loss: 0.8419 - val_accuracy: 0.5067 - val_loss: 0.9533\n",
      "Epoch 37/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5080 - loss: 0.8484 - val_accuracy: 0.5067 - val_loss: 0.9429\n",
      "Epoch 38/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5091 - loss: 0.8454 - val_accuracy: 0.5072 - val_loss: 0.9543\n",
      "Epoch 39/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5168 - loss: 0.8318 - val_accuracy: 0.5072 - val_loss: 0.9556\n",
      "Epoch 40/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5042 - loss: 0.8407 - val_accuracy: 0.5067 - val_loss: 0.9587\n",
      "Epoch 41/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5110 - loss: 0.8363 - val_accuracy: 0.5067 - val_loss: 0.9629\n",
      "Epoch 42/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5143 - loss: 0.8322 - val_accuracy: 0.5067 - val_loss: 0.9509\n",
      "Epoch 43/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5086 - loss: 0.8320 - val_accuracy: 0.5067 - val_loss: 0.9621\n",
      "Epoch 44/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5044 - loss: 0.8295 - val_accuracy: 0.5067 - val_loss: 0.9875\n",
      "Epoch 45/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5112 - loss: 0.8315 - val_accuracy: 0.5072 - val_loss: 1.0584\n",
      "Epoch 46/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5132 - loss: 0.8353 - val_accuracy: 0.5076 - val_loss: 0.9670\n",
      "Epoch 47/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5011 - loss: 0.8207 - val_accuracy: 0.5067 - val_loss: 1.0317\n",
      "Epoch 48/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5049 - loss: 0.8457 - val_accuracy: 0.5058 - val_loss: 0.9644\n",
      "Epoch 49/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5173 - loss: 0.8347 - val_accuracy: 0.5063 - val_loss: 1.0066\n",
      "Epoch 50/50\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5092 - loss: 0.8242 - val_accuracy: 0.5054 - val_loss: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ed0119ad00>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 假设你有 X_train, y_train, X_test, y_test 数据\n",
    "# 将数据划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels.iloc[:, [0]], test_size=0.2)\n",
    "\n",
    "# 将标签转换为 numpy 数组并进行形状调整\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "#calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoencoder by previous research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qns = pd.read_csv(r'G:\\ABCD\\script\\trail/trail_tsne_RF/factor analysis/output/NA/data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbcl_q01_p</th>\n",
       "      <th>cbcl_q03_p</th>\n",
       "      <th>cbcl_q04_p</th>\n",
       "      <th>cbcl_q05_p</th>\n",
       "      <th>cbcl_q06_p</th>\n",
       "      <th>cbcl_q07_p</th>\n",
       "      <th>cbcl_q08_p</th>\n",
       "      <th>cbcl_q09_p</th>\n",
       "      <th>cbcl_q10_p</th>\n",
       "      <th>cbcl_q11_p</th>\n",
       "      <th>...</th>\n",
       "      <th>cbcl_q102_p</th>\n",
       "      <th>cbcl_q103_p</th>\n",
       "      <th>cbcl_q104_p</th>\n",
       "      <th>cbcl_q106_p</th>\n",
       "      <th>cbcl_q107_p</th>\n",
       "      <th>cbcl_q108_p</th>\n",
       "      <th>cbcl_q109_p</th>\n",
       "      <th>cbcl_q110_p</th>\n",
       "      <th>cbcl_q111_p</th>\n",
       "      <th>cbcl_q112_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cbcl_q01_p  cbcl_q03_p  cbcl_q04_p  cbcl_q05_p  cbcl_q06_p  cbcl_q07_p  \\\n",
       "0               0           0           0           0           0           0   \n",
       "1               0           0           0           0           0           0   \n",
       "2               0           1           0           0           0           1   \n",
       "3               0           1           1           0           0           1   \n",
       "4               1           2           1           0           0           1   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "10647           0           2           1           0           0           0   \n",
       "10648           0           1           0           1           0           0   \n",
       "10649           0           0           0           0           0           0   \n",
       "10650           0           1           0           0           0           0   \n",
       "10651           1           0           1           0           0           0   \n",
       "\n",
       "       cbcl_q08_p  cbcl_q09_p  cbcl_q10_p  cbcl_q11_p  ...  cbcl_q102_p  \\\n",
       "0               0           0           0           0  ...            0   \n",
       "1               0           0           0           0  ...            0   \n",
       "2               0           1           0           0  ...            0   \n",
       "3               1           1           1           0  ...            0   \n",
       "4               2           0           1           0  ...            0   \n",
       "...           ...         ...         ...         ...  ...          ...   \n",
       "10647           1           1           1           0  ...            1   \n",
       "10648           0           0           0           0  ...            0   \n",
       "10649           0           0           0           0  ...            0   \n",
       "10650           0           0           0           0  ...            0   \n",
       "10651           0           1           0           0  ...            0   \n",
       "\n",
       "       cbcl_q103_p  cbcl_q104_p  cbcl_q106_p  cbcl_q107_p  cbcl_q108_p  \\\n",
       "0                0            0            0            0            1   \n",
       "1                0            0            0            0            0   \n",
       "2                0            1            0            0            1   \n",
       "3                0            1            0            0            0   \n",
       "4                1            0            0            0            0   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10647            0            1            0            0            0   \n",
       "10648            1            0            0            0            0   \n",
       "10649            0            0            0            0            0   \n",
       "10650            0            0            0            0            0   \n",
       "10651            0            0            0            0            0   \n",
       "\n",
       "       cbcl_q109_p  cbcl_q110_p  cbcl_q111_p  cbcl_q112_p  \n",
       "0                0            0            0            0  \n",
       "1                0            0            0            0  \n",
       "2                0            0            0            1  \n",
       "3                0            0            0            1  \n",
       "4                0            0            0            0  \n",
       "...            ...          ...          ...          ...  \n",
       "10647            1            0            1            1  \n",
       "10648            1            0            0            0  \n",
       "10649            0            0            0            0  \n",
       "10650            0            0            0            0  \n",
       "10651            0            0            0            0  \n",
       "\n",
       "[10652 rows x 114 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qns.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = qns.iloc[:,2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10652, 114)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.8461, Val Loss: 0.7685\n",
      "Epoch [2/200], Train Loss: 0.7573, Val Loss: 0.7251\n",
      "Epoch [3/200], Train Loss: 0.7290, Val Loss: 0.7128\n",
      "Epoch [4/200], Train Loss: 0.7160, Val Loss: 0.7047\n",
      "Epoch [5/200], Train Loss: 0.7053, Val Loss: 0.6990\n",
      "Epoch [6/200], Train Loss: 0.6975, Val Loss: 0.6946\n",
      "Epoch [7/200], Train Loss: 0.6913, Val Loss: 0.6919\n",
      "Epoch [8/200], Train Loss: 0.6859, Val Loss: 0.6882\n",
      "Epoch [9/200], Train Loss: 0.6816, Val Loss: 0.6864\n",
      "Epoch [10/200], Train Loss: 0.6769, Val Loss: 0.6828\n",
      "Epoch [11/200], Train Loss: 0.6730, Val Loss: 0.6806\n",
      "Epoch [12/200], Train Loss: 0.6702, Val Loss: 0.6794\n",
      "Epoch [13/200], Train Loss: 0.6676, Val Loss: 0.6772\n",
      "Epoch [14/200], Train Loss: 0.6645, Val Loss: 0.6777\n",
      "Epoch [15/200], Train Loss: 0.6623, Val Loss: 0.6757\n",
      "Epoch [16/200], Train Loss: 0.6599, Val Loss: 0.6751\n",
      "Epoch [17/200], Train Loss: 0.6570, Val Loss: 0.6739\n",
      "Epoch [18/200], Train Loss: 0.6546, Val Loss: 0.6741\n",
      "Epoch [19/200], Train Loss: 0.6526, Val Loss: 0.6715\n",
      "Epoch [20/200], Train Loss: 0.6507, Val Loss: 0.6688\n",
      "Epoch [21/200], Train Loss: 0.6481, Val Loss: 0.6705\n",
      "Epoch [22/200], Train Loss: 0.6469, Val Loss: 0.6687\n",
      "Epoch [23/200], Train Loss: 0.6457, Val Loss: 0.6692\n",
      "Epoch [24/200], Train Loss: 0.6442, Val Loss: 0.6674\n",
      "Epoch [25/200], Train Loss: 0.6417, Val Loss: 0.6662\n",
      "Epoch [26/200], Train Loss: 0.6406, Val Loss: 0.6656\n",
      "Epoch [27/200], Train Loss: 0.6396, Val Loss: 0.6649\n",
      "Epoch [28/200], Train Loss: 0.6381, Val Loss: 0.6660\n",
      "Epoch [29/200], Train Loss: 0.6372, Val Loss: 0.6650\n",
      "Epoch [30/200], Train Loss: 0.6355, Val Loss: 0.6649\n",
      "Epoch [31/200], Train Loss: 0.6344, Val Loss: 0.6638\n",
      "Epoch [32/200], Train Loss: 0.6329, Val Loss: 0.6638\n",
      "Epoch [33/200], Train Loss: 0.6334, Val Loss: 0.6648\n",
      "Epoch [34/200], Train Loss: 0.6319, Val Loss: 0.6630\n",
      "Epoch [35/200], Train Loss: 0.6298, Val Loss: 0.6636\n",
      "Epoch [36/200], Train Loss: 0.6294, Val Loss: 0.6621\n",
      "Epoch [37/200], Train Loss: 0.6283, Val Loss: 0.6638\n",
      "Epoch [38/200], Train Loss: 0.6271, Val Loss: 0.6626\n",
      "Epoch [39/200], Train Loss: 0.6254, Val Loss: 0.6627\n",
      "Epoch [40/200], Train Loss: 0.6252, Val Loss: 0.6621\n",
      "Epoch [41/200], Train Loss: 0.6240, Val Loss: 0.6611\n",
      "Epoch [42/200], Train Loss: 0.6235, Val Loss: 0.6601\n",
      "Epoch [43/200], Train Loss: 0.6229, Val Loss: 0.6606\n",
      "Epoch [44/200], Train Loss: 0.6219, Val Loss: 0.6603\n",
      "Epoch [45/200], Train Loss: 0.6204, Val Loss: 0.6618\n",
      "Epoch [46/200], Train Loss: 0.6199, Val Loss: 0.6611\n",
      "Epoch [47/200], Train Loss: 0.6194, Val Loss: 0.6622\n",
      "Epoch [48/200], Train Loss: 0.6193, Val Loss: 0.6611\n",
      "Epoch [49/200], Train Loss: 0.6177, Val Loss: 0.6587\n",
      "Epoch [50/200], Train Loss: 0.6172, Val Loss: 0.6597\n",
      "Epoch [51/200], Train Loss: 0.6165, Val Loss: 0.6585\n",
      "Epoch [52/200], Train Loss: 0.6161, Val Loss: 0.6601\n",
      "Epoch [53/200], Train Loss: 0.6158, Val Loss: 0.6594\n",
      "Epoch [54/200], Train Loss: 0.6147, Val Loss: 0.6587\n",
      "Epoch [55/200], Train Loss: 0.6140, Val Loss: 0.6595\n",
      "Epoch [56/200], Train Loss: 0.6132, Val Loss: 0.6597\n",
      "Epoch [57/200], Train Loss: 0.6131, Val Loss: 0.6596\n",
      "Epoch [58/200], Train Loss: 0.6118, Val Loss: 0.6569\n",
      "Epoch [59/200], Train Loss: 0.6112, Val Loss: 0.6615\n",
      "Epoch [60/200], Train Loss: 0.6112, Val Loss: 0.6600\n",
      "Epoch [61/200], Train Loss: 0.6112, Val Loss: 0.6585\n",
      "Epoch [62/200], Train Loss: 0.6103, Val Loss: 0.6584\n",
      "Epoch [63/200], Train Loss: 0.6116, Val Loss: 0.6597\n",
      "Epoch [64/200], Train Loss: 0.6097, Val Loss: 0.6594\n",
      "Epoch [65/200], Train Loss: 0.6081, Val Loss: 0.6574\n",
      "Epoch [66/200], Train Loss: 0.6074, Val Loss: 0.6583\n",
      "Epoch [67/200], Train Loss: 0.6079, Val Loss: 0.6574\n",
      "Epoch [68/200], Train Loss: 0.6101, Val Loss: 0.6570\n",
      "Early stopping at epoch 68\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Assuming your questionnaire items are in columns named 'Q1' to 'Q100'\n",
    "# questionnaire_columns = [col for col in data.columns if col.startswith('Q')]\n",
    "X = qns.iloc[:,2:].values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "class QuestionnaireDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.data[idx]  # Input and target are the same\n",
    "\n",
    "train_dataset = QuestionnaireDataset(X_train)\n",
    "val_dataset = QuestionnaireDataset(X_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Step 2: Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, latent_dim),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, input_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 5  # Reduce to 5 dimensions\n",
    "\n",
    "model = Autoencoder(input_dim, latent_dim)\n",
    "\n",
    "# Step 3: Train the autoencoder\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 200\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_features, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_features)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch_features.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, _ in val_loader:\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_features)\n",
    "            val_loss += loss.item() * batch_features.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "\n",
    "    # 检查 Early Stopping 条件\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0  # 重置计数器\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "# Step 4: Extract the latent features\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Transform the entire dataset\n",
    "    X_latent = model.encoder(torch.tensor(X_scaled, dtype=torch.float32)).numpy()\n",
    "\n",
    "# The variable X_latent now contains the 5-dimensional representation of your data\n",
    "# You can convert it back to a DataFrame if needed\n",
    "latent_df = pd.DataFrame(X_latent, columns=[f'Factor_{i+1}' for i in range(latent_dim)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor_1</th>\n",
       "      <th>Factor_2</th>\n",
       "      <th>Factor_3</th>\n",
       "      <th>Factor_4</th>\n",
       "      <th>Factor_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.225962</td>\n",
       "      <td>-0.987701</td>\n",
       "      <td>1.428785</td>\n",
       "      <td>0.196275</td>\n",
       "      <td>2.294005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.270909</td>\n",
       "      <td>-1.172184</td>\n",
       "      <td>1.699451</td>\n",
       "      <td>1.889652</td>\n",
       "      <td>2.205972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170160</td>\n",
       "      <td>0.610330</td>\n",
       "      <td>-0.446073</td>\n",
       "      <td>2.320928</td>\n",
       "      <td>1.705356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.410402</td>\n",
       "      <td>-0.158501</td>\n",
       "      <td>0.223055</td>\n",
       "      <td>3.264879</td>\n",
       "      <td>1.840801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.462913</td>\n",
       "      <td>-5.130525</td>\n",
       "      <td>1.558750</td>\n",
       "      <td>5.146125</td>\n",
       "      <td>-4.048456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>-6.752625</td>\n",
       "      <td>5.188299</td>\n",
       "      <td>0.805525</td>\n",
       "      <td>-0.362465</td>\n",
       "      <td>-2.794127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>0.047847</td>\n",
       "      <td>-2.206021</td>\n",
       "      <td>1.290669</td>\n",
       "      <td>3.946332</td>\n",
       "      <td>-0.416121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>-0.721727</td>\n",
       "      <td>-0.715168</td>\n",
       "      <td>1.275204</td>\n",
       "      <td>0.842071</td>\n",
       "      <td>2.017429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>-0.617627</td>\n",
       "      <td>-1.584424</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>1.639837</td>\n",
       "      <td>0.536771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>1.174780</td>\n",
       "      <td>-0.877488</td>\n",
       "      <td>1.137938</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.846687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
       "0      0.225962 -0.987701  1.428785  0.196275  2.294005\n",
       "1     -0.270909 -1.172184  1.699451  1.889652  2.205972\n",
       "2     -0.170160  0.610330 -0.446073  2.320928  1.705356\n",
       "3      0.410402 -0.158501  0.223055  3.264879  1.840801\n",
       "4      3.462913 -5.130525  1.558750  5.146125 -4.048456\n",
       "...         ...       ...       ...       ...       ...\n",
       "10647 -6.752625  5.188299  0.805525 -0.362465 -2.794127\n",
       "10648  0.047847 -2.206021  1.290669  3.946332 -0.416121\n",
       "10649 -0.721727 -0.715168  1.275204  0.842071  2.017429\n",
       "10650 -0.617627 -1.584424  0.179293  1.639837  0.536771\n",
       "10651  1.174780 -0.877488  1.137938  0.925061  0.846687\n",
       "\n",
       "[10652 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute reconstruction error for each sample\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(torch.tensor(X_scaled, dtype=torch.float32))\n",
    "    mse = nn.MSELoss(reduction='none')\n",
    "    reconstruction_error = mse(reconstructed, torch.tensor(X_scaled, dtype=torch.float32)).mean(dim=1).numpy()\n",
    "\n",
    "plt.hist(reconstruction_error, bins=50)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总方差解释率: 36.96%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 原始数据的总方差\n",
    "total_variance = np.var(X_scaled, axis=0).sum()\n",
    "\n",
    "# 计算每个因子贡献的方差\n",
    "# 通过重建数据的方差贡献，计算解释率\n",
    "reconstruction_variance = np.var(reconstructed.numpy(), axis=0).sum()\n",
    "\n",
    "# 计算方差解释率\n",
    "explained_variance_ratio = reconstruction_variance / total_variance\n",
    "\n",
    "print(f\"总方差解释率: {explained_variance_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 获取编码器输出的潜在表示\n",
    "# latent_representations = model.predict(X)  # 假设 encoder 是你的编码器模型\n",
    "\n",
    "# 计算潜在表示每个维度的方差\n",
    "variances = np.var(latent_df, axis=0)\n",
    "\n",
    "# 绘制潜在表示方差图\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(variances) + 1), variances, marker='o', linestyle='--')\n",
    "plt.xlabel('Latent Dimension Index')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Latent Space Variance')\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.0340, Val Loss: 0.0316\n",
      "Epoch [2/200], Train Loss: 0.0315, Val Loss: 0.0315\n",
      "Epoch [3/200], Train Loss: 0.0314, Val Loss: 0.0315\n",
      "Epoch [4/200], Train Loss: 0.0314, Val Loss: 0.0315\n",
      "Epoch [5/200], Train Loss: 0.0314, Val Loss: 0.0315\n",
      "Epoch [6/200], Train Loss: 0.0314, Val Loss: 0.0314\n",
      "Epoch [7/200], Train Loss: 0.0314, Val Loss: 0.0314\n",
      "Epoch [8/200], Train Loss: 0.0313, Val Loss: 0.0314\n",
      "Epoch [9/200], Train Loss: 0.0313, Val Loss: 0.0314\n",
      "Epoch [10/200], Train Loss: 0.0313, Val Loss: 0.0313\n",
      "Epoch [11/200], Train Loss: 0.0313, Val Loss: 0.0313\n",
      "Epoch [12/200], Train Loss: 0.0312, Val Loss: 0.0313\n",
      "Epoch [13/200], Train Loss: 0.0312, Val Loss: 0.0312\n",
      "Epoch [14/200], Train Loss: 0.0312, Val Loss: 0.0312\n",
      "Epoch [15/200], Train Loss: 0.0311, Val Loss: 0.0312\n",
      "Epoch [16/200], Train Loss: 0.0311, Val Loss: 0.0311\n",
      "Epoch [17/200], Train Loss: 0.0310, Val Loss: 0.0312\n",
      "Epoch [18/200], Train Loss: 0.0310, Val Loss: 0.0311\n",
      "Epoch [19/200], Train Loss: 0.0310, Val Loss: 0.0310\n",
      "Epoch [20/200], Train Loss: 0.0309, Val Loss: 0.0310\n",
      "Epoch [21/200], Train Loss: 0.0309, Val Loss: 0.0309\n",
      "Epoch [22/200], Train Loss: 0.0308, Val Loss: 0.0310\n",
      "Epoch [23/200], Train Loss: 0.0308, Val Loss: 0.0309\n",
      "Epoch [24/200], Train Loss: 0.0308, Val Loss: 0.0310\n",
      "Epoch [25/200], Train Loss: 0.0308, Val Loss: 0.0309\n",
      "Epoch [26/200], Train Loss: 0.0307, Val Loss: 0.0309\n",
      "Epoch [27/200], Train Loss: 0.0307, Val Loss: 0.0309\n",
      "Epoch [28/200], Train Loss: 0.0307, Val Loss: 0.0309\n",
      "Epoch [29/200], Train Loss: 0.0306, Val Loss: 0.0309\n",
      "Epoch [30/200], Train Loss: 0.0306, Val Loss: 0.0309\n",
      "Epoch [31/200], Train Loss: 0.0306, Val Loss: 0.0309\n",
      "Epoch [32/200], Train Loss: 0.0306, Val Loss: 0.0309\n",
      "Epoch [33/200], Train Loss: 0.0306, Val Loss: 0.0310\n",
      "Epoch [34/200], Train Loss: 0.0306, Val Loss: 0.0309\n",
      "Epoch [35/200], Train Loss: 0.0305, Val Loss: 0.0310\n",
      "Epoch [36/200], Train Loss: 0.0305, Val Loss: 0.0309\n",
      "Epoch [37/200], Train Loss: 0.0305, Val Loss: 0.0309\n",
      "Epoch [38/200], Train Loss: 0.0305, Val Loss: 0.0309\n",
      "Epoch [39/200], Train Loss: 0.0305, Val Loss: 0.0309\n",
      "Epoch [40/200], Train Loss: 0.0305, Val Loss: 0.0309\n",
      "Epoch [41/200], Train Loss: 0.0304, Val Loss: 0.0309\n",
      "Epoch [42/200], Train Loss: 0.0304, Val Loss: 0.0310\n",
      "Early stopping at epoch 42\n",
      "Epoch [1/200], Train Loss: 0.0333, Val Loss: 0.0302\n",
      "Epoch [2/200], Train Loss: 0.0298, Val Loss: 0.0296\n",
      "Epoch [3/200], Train Loss: 0.0295, Val Loss: 0.0294\n",
      "Epoch [4/200], Train Loss: 0.0294, Val Loss: 0.0292\n",
      "Epoch [5/200], Train Loss: 0.0292, Val Loss: 0.0291\n",
      "Epoch [6/200], Train Loss: 0.0291, Val Loss: 0.0290\n",
      "Epoch [7/200], Train Loss: 0.0290, Val Loss: 0.0289\n",
      "Epoch [8/200], Train Loss: 0.0289, Val Loss: 0.0289\n",
      "Epoch [9/200], Train Loss: 0.0288, Val Loss: 0.0289\n",
      "Epoch [10/200], Train Loss: 0.0287, Val Loss: 0.0287\n",
      "Epoch [11/200], Train Loss: 0.0286, Val Loss: 0.0287\n",
      "Epoch [12/200], Train Loss: 0.0285, Val Loss: 0.0288\n",
      "Epoch [13/200], Train Loss: 0.0285, Val Loss: 0.0287\n",
      "Epoch [14/200], Train Loss: 0.0285, Val Loss: 0.0287\n",
      "Epoch [15/200], Train Loss: 0.0284, Val Loss: 0.0287\n",
      "Epoch [16/200], Train Loss: 0.0284, Val Loss: 0.0287\n",
      "Epoch [17/200], Train Loss: 0.0283, Val Loss: 0.0286\n",
      "Epoch [18/200], Train Loss: 0.0283, Val Loss: 0.0286\n",
      "Epoch [19/200], Train Loss: 0.0283, Val Loss: 0.0286\n",
      "Epoch [20/200], Train Loss: 0.0282, Val Loss: 0.0286\n",
      "Epoch [21/200], Train Loss: 0.0282, Val Loss: 0.0286\n",
      "Epoch [22/200], Train Loss: 0.0281, Val Loss: 0.0285\n",
      "Epoch [23/200], Train Loss: 0.0281, Val Loss: 0.0285\n",
      "Epoch [24/200], Train Loss: 0.0281, Val Loss: 0.0285\n",
      "Epoch [25/200], Train Loss: 0.0280, Val Loss: 0.0284\n",
      "Epoch [26/200], Train Loss: 0.0280, Val Loss: 0.0284\n",
      "Epoch [27/200], Train Loss: 0.0280, Val Loss: 0.0285\n",
      "Epoch [28/200], Train Loss: 0.0280, Val Loss: 0.0284\n",
      "Epoch [29/200], Train Loss: 0.0279, Val Loss: 0.0284\n",
      "Epoch [30/200], Train Loss: 0.0279, Val Loss: 0.0285\n",
      "Epoch [31/200], Train Loss: 0.0279, Val Loss: 0.0284\n",
      "Epoch [32/200], Train Loss: 0.0279, Val Loss: 0.0285\n",
      "Epoch [33/200], Train Loss: 0.0278, Val Loss: 0.0284\n",
      "Epoch [34/200], Train Loss: 0.0278, Val Loss: 0.0284\n",
      "Epoch [35/200], Train Loss: 0.0278, Val Loss: 0.0285\n",
      "Epoch [36/200], Train Loss: 0.0278, Val Loss: 0.0284\n",
      "Epoch [37/200], Train Loss: 0.0278, Val Loss: 0.0284\n",
      "Epoch [38/200], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [39/200], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [40/200], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [41/200], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [42/200], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [43/200], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [44/200], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [45/200], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [46/200], Train Loss: 0.0277, Val Loss: 0.0284\n",
      "Epoch [47/200], Train Loss: 0.0277, Val Loss: 0.0285\n",
      "Epoch [48/200], Train Loss: 0.0276, Val Loss: 0.0284\n",
      "Epoch [49/200], Train Loss: 0.0276, Val Loss: 0.0284\n",
      "Epoch [50/200], Train Loss: 0.0276, Val Loss: 0.0284\n",
      "Epoch [51/200], Train Loss: 0.0276, Val Loss: 0.0284\n",
      "Epoch [52/200], Train Loss: 0.0276, Val Loss: 0.0284\n",
      "Epoch [53/200], Train Loss: 0.0276, Val Loss: 0.0285\n",
      "Epoch [54/200], Train Loss: 0.0276, Val Loss: 0.0284\n",
      "Epoch [55/200], Train Loss: 0.0276, Val Loss: 0.0284\n",
      "Epoch [56/200], Train Loss: 0.0276, Val Loss: 0.0284\n",
      "Early stopping at epoch 56\n",
      "Epoch [1/200], Train Loss: 0.0329, Val Loss: 0.0289\n",
      "Epoch [2/200], Train Loss: 0.0284, Val Loss: 0.0281\n",
      "Epoch [3/200], Train Loss: 0.0280, Val Loss: 0.0278\n",
      "Epoch [4/200], Train Loss: 0.0277, Val Loss: 0.0277\n",
      "Epoch [5/200], Train Loss: 0.0276, Val Loss: 0.0276\n",
      "Epoch [6/200], Train Loss: 0.0275, Val Loss: 0.0275\n",
      "Epoch [7/200], Train Loss: 0.0274, Val Loss: 0.0275\n",
      "Epoch [8/200], Train Loss: 0.0273, Val Loss: 0.0274\n",
      "Epoch [9/200], Train Loss: 0.0272, Val Loss: 0.0273\n",
      "Epoch [10/200], Train Loss: 0.0272, Val Loss: 0.0273\n",
      "Epoch [11/200], Train Loss: 0.0271, Val Loss: 0.0274\n",
      "Epoch [12/200], Train Loss: 0.0271, Val Loss: 0.0273\n",
      "Epoch [13/200], Train Loss: 0.0270, Val Loss: 0.0272\n",
      "Epoch [14/200], Train Loss: 0.0270, Val Loss: 0.0272\n",
      "Epoch [15/200], Train Loss: 0.0270, Val Loss: 0.0272\n",
      "Epoch [16/200], Train Loss: 0.0270, Val Loss: 0.0272\n",
      "Epoch [17/200], Train Loss: 0.0269, Val Loss: 0.0272\n",
      "Epoch [18/200], Train Loss: 0.0269, Val Loss: 0.0272\n",
      "Epoch [19/200], Train Loss: 0.0269, Val Loss: 0.0272\n",
      "Epoch [20/200], Train Loss: 0.0268, Val Loss: 0.0272\n",
      "Epoch [21/200], Train Loss: 0.0268, Val Loss: 0.0272\n",
      "Epoch [22/200], Train Loss: 0.0268, Val Loss: 0.0272\n",
      "Epoch [23/200], Train Loss: 0.0268, Val Loss: 0.0272\n",
      "Epoch [24/200], Train Loss: 0.0267, Val Loss: 0.0271\n",
      "Epoch [25/200], Train Loss: 0.0267, Val Loss: 0.0272\n",
      "Epoch [26/200], Train Loss: 0.0267, Val Loss: 0.0272\n",
      "Epoch [27/200], Train Loss: 0.0267, Val Loss: 0.0271\n",
      "Epoch [28/200], Train Loss: 0.0267, Val Loss: 0.0271\n",
      "Epoch [29/200], Train Loss: 0.0267, Val Loss: 0.0271\n",
      "Epoch [30/200], Train Loss: 0.0266, Val Loss: 0.0271\n",
      "Epoch [31/200], Train Loss: 0.0266, Val Loss: 0.0271\n",
      "Epoch [32/200], Train Loss: 0.0266, Val Loss: 0.0271\n",
      "Epoch [33/200], Train Loss: 0.0266, Val Loss: 0.0272\n",
      "Epoch [34/200], Train Loss: 0.0265, Val Loss: 0.0271\n",
      "Epoch [35/200], Train Loss: 0.0265, Val Loss: 0.0271\n",
      "Epoch [36/200], Train Loss: 0.0265, Val Loss: 0.0271\n",
      "Epoch [37/200], Train Loss: 0.0265, Val Loss: 0.0271\n",
      "Epoch [38/200], Train Loss: 0.0265, Val Loss: 0.0271\n",
      "Epoch [39/200], Train Loss: 0.0265, Val Loss: 0.0271\n",
      "Epoch [40/200], Train Loss: 0.0264, Val Loss: 0.0271\n",
      "Epoch [41/200], Train Loss: 0.0264, Val Loss: 0.0271\n",
      "Epoch [42/200], Train Loss: 0.0264, Val Loss: 0.0271\n",
      "Epoch [43/200], Train Loss: 0.0264, Val Loss: 0.0271\n",
      "Epoch [44/200], Train Loss: 0.0264, Val Loss: 0.0271\n",
      "Epoch [45/200], Train Loss: 0.0264, Val Loss: 0.0271\n",
      "Epoch [46/200], Train Loss: 0.0263, Val Loss: 0.0271\n",
      "Epoch [47/200], Train Loss: 0.0263, Val Loss: 0.0270\n",
      "Epoch [48/200], Train Loss: 0.0263, Val Loss: 0.0271\n",
      "Epoch [49/200], Train Loss: 0.0263, Val Loss: 0.0271\n",
      "Epoch [50/200], Train Loss: 0.0263, Val Loss: 0.0271\n",
      "Epoch [51/200], Train Loss: 0.0263, Val Loss: 0.0271\n",
      "Epoch [52/200], Train Loss: 0.0263, Val Loss: 0.0271\n",
      "Epoch [53/200], Train Loss: 0.0263, Val Loss: 0.0271\n",
      "Epoch [54/200], Train Loss: 0.0262, Val Loss: 0.0271\n",
      "Epoch [55/200], Train Loss: 0.0262, Val Loss: 0.0271\n",
      "Epoch [56/200], Train Loss: 0.0262, Val Loss: 0.0271\n",
      "Epoch [57/200], Train Loss: 0.0262, Val Loss: 0.0271\n",
      "Early stopping at epoch 57\n",
      "Epoch [1/200], Train Loss: 0.0336, Val Loss: 0.0290\n",
      "Epoch [2/200], Train Loss: 0.0282, Val Loss: 0.0276\n",
      "Epoch [3/200], Train Loss: 0.0273, Val Loss: 0.0273\n",
      "Epoch [4/200], Train Loss: 0.0271, Val Loss: 0.0271\n",
      "Epoch [5/200], Train Loss: 0.0270, Val Loss: 0.0270\n",
      "Epoch [6/200], Train Loss: 0.0268, Val Loss: 0.0268\n",
      "Epoch [7/200], Train Loss: 0.0266, Val Loss: 0.0267\n",
      "Epoch [8/200], Train Loss: 0.0265, Val Loss: 0.0266\n",
      "Epoch [9/200], Train Loss: 0.0264, Val Loss: 0.0265\n",
      "Epoch [10/200], Train Loss: 0.0263, Val Loss: 0.0265\n",
      "Epoch [11/200], Train Loss: 0.0262, Val Loss: 0.0264\n",
      "Epoch [12/200], Train Loss: 0.0262, Val Loss: 0.0264\n",
      "Epoch [13/200], Train Loss: 0.0261, Val Loss: 0.0264\n",
      "Epoch [14/200], Train Loss: 0.0260, Val Loss: 0.0263\n",
      "Epoch [15/200], Train Loss: 0.0260, Val Loss: 0.0263\n",
      "Epoch [16/200], Train Loss: 0.0259, Val Loss: 0.0262\n",
      "Epoch [17/200], Train Loss: 0.0259, Val Loss: 0.0261\n",
      "Epoch [18/200], Train Loss: 0.0258, Val Loss: 0.0262\n",
      "Epoch [19/200], Train Loss: 0.0258, Val Loss: 0.0261\n",
      "Epoch [20/200], Train Loss: 0.0257, Val Loss: 0.0261\n",
      "Epoch [21/200], Train Loss: 0.0257, Val Loss: 0.0261\n",
      "Epoch [22/200], Train Loss: 0.0257, Val Loss: 0.0260\n",
      "Epoch [23/200], Train Loss: 0.0256, Val Loss: 0.0260\n",
      "Epoch [24/200], Train Loss: 0.0256, Val Loss: 0.0261\n",
      "Epoch [25/200], Train Loss: 0.0256, Val Loss: 0.0260\n",
      "Epoch [26/200], Train Loss: 0.0255, Val Loss: 0.0260\n",
      "Epoch [27/200], Train Loss: 0.0255, Val Loss: 0.0260\n",
      "Epoch [28/200], Train Loss: 0.0255, Val Loss: 0.0260\n",
      "Epoch [29/200], Train Loss: 0.0254, Val Loss: 0.0259\n",
      "Epoch [30/200], Train Loss: 0.0254, Val Loss: 0.0259\n",
      "Epoch [31/200], Train Loss: 0.0254, Val Loss: 0.0260\n",
      "Epoch [32/200], Train Loss: 0.0254, Val Loss: 0.0259\n",
      "Epoch [33/200], Train Loss: 0.0254, Val Loss: 0.0259\n",
      "Epoch [34/200], Train Loss: 0.0253, Val Loss: 0.0259\n",
      "Epoch [35/200], Train Loss: 0.0253, Val Loss: 0.0259\n",
      "Epoch [36/200], Train Loss: 0.0253, Val Loss: 0.0259\n",
      "Epoch [37/200], Train Loss: 0.0253, Val Loss: 0.0259\n",
      "Epoch [38/200], Train Loss: 0.0253, Val Loss: 0.0259\n",
      "Epoch [39/200], Train Loss: 0.0252, Val Loss: 0.0259\n",
      "Epoch [40/200], Train Loss: 0.0252, Val Loss: 0.0259\n",
      "Epoch [41/200], Train Loss: 0.0252, Val Loss: 0.0259\n",
      "Epoch [42/200], Train Loss: 0.0252, Val Loss: 0.0259\n",
      "Epoch [43/200], Train Loss: 0.0252, Val Loss: 0.0259\n",
      "Epoch [44/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [45/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [46/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [47/200], Train Loss: 0.0251, Val Loss: 0.0258\n",
      "Epoch [48/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [49/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [50/200], Train Loss: 0.0251, Val Loss: 0.0258\n",
      "Epoch [51/200], Train Loss: 0.0250, Val Loss: 0.0258\n",
      "Epoch [52/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [53/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [54/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [55/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [56/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [57/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Early stopping at epoch 57\n",
      "Epoch [1/200], Train Loss: 0.0330, Val Loss: 0.0286\n",
      "Epoch [2/200], Train Loss: 0.0274, Val Loss: 0.0267\n",
      "Epoch [3/200], Train Loss: 0.0263, Val Loss: 0.0262\n",
      "Epoch [4/200], Train Loss: 0.0259, Val Loss: 0.0259\n",
      "Epoch [5/200], Train Loss: 0.0257, Val Loss: 0.0258\n",
      "Epoch [6/200], Train Loss: 0.0255, Val Loss: 0.0256\n",
      "Epoch [7/200], Train Loss: 0.0253, Val Loss: 0.0255\n",
      "Epoch [8/200], Train Loss: 0.0252, Val Loss: 0.0254\n",
      "Epoch [9/200], Train Loss: 0.0251, Val Loss: 0.0254\n",
      "Epoch [10/200], Train Loss: 0.0251, Val Loss: 0.0253\n",
      "Epoch [11/200], Train Loss: 0.0250, Val Loss: 0.0253\n",
      "Epoch [12/200], Train Loss: 0.0250, Val Loss: 0.0252\n",
      "Epoch [13/200], Train Loss: 0.0249, Val Loss: 0.0252\n",
      "Epoch [14/200], Train Loss: 0.0249, Val Loss: 0.0252\n",
      "Epoch [15/200], Train Loss: 0.0248, Val Loss: 0.0252\n",
      "Epoch [16/200], Train Loss: 0.0248, Val Loss: 0.0252\n",
      "Epoch [17/200], Train Loss: 0.0247, Val Loss: 0.0251\n",
      "Epoch [18/200], Train Loss: 0.0247, Val Loss: 0.0250\n",
      "Epoch [19/200], Train Loss: 0.0246, Val Loss: 0.0251\n",
      "Epoch [20/200], Train Loss: 0.0246, Val Loss: 0.0250\n",
      "Epoch [21/200], Train Loss: 0.0245, Val Loss: 0.0249\n",
      "Epoch [22/200], Train Loss: 0.0244, Val Loss: 0.0249\n",
      "Epoch [23/200], Train Loss: 0.0244, Val Loss: 0.0249\n",
      "Epoch [24/200], Train Loss: 0.0244, Val Loss: 0.0249\n",
      "Epoch [25/200], Train Loss: 0.0243, Val Loss: 0.0249\n",
      "Epoch [26/200], Train Loss: 0.0243, Val Loss: 0.0248\n",
      "Epoch [27/200], Train Loss: 0.0243, Val Loss: 0.0248\n",
      "Epoch [28/200], Train Loss: 0.0243, Val Loss: 0.0248\n",
      "Epoch [29/200], Train Loss: 0.0242, Val Loss: 0.0248\n",
      "Epoch [30/200], Train Loss: 0.0242, Val Loss: 0.0248\n",
      "Epoch [31/200], Train Loss: 0.0242, Val Loss: 0.0247\n",
      "Epoch [32/200], Train Loss: 0.0242, Val Loss: 0.0248\n",
      "Epoch [33/200], Train Loss: 0.0241, Val Loss: 0.0248\n",
      "Epoch [34/200], Train Loss: 0.0241, Val Loss: 0.0247\n",
      "Epoch [35/200], Train Loss: 0.0241, Val Loss: 0.0247\n",
      "Epoch [36/200], Train Loss: 0.0241, Val Loss: 0.0247\n",
      "Epoch [37/200], Train Loss: 0.0241, Val Loss: 0.0247\n",
      "Epoch [38/200], Train Loss: 0.0240, Val Loss: 0.0247\n",
      "Epoch [39/200], Train Loss: 0.0240, Val Loss: 0.0247\n",
      "Epoch [40/200], Train Loss: 0.0240, Val Loss: 0.0247\n",
      "Epoch [41/200], Train Loss: 0.0240, Val Loss: 0.0247\n",
      "Epoch [42/200], Train Loss: 0.0239, Val Loss: 0.0247\n",
      "Epoch [43/200], Train Loss: 0.0239, Val Loss: 0.0247\n",
      "Epoch [44/200], Train Loss: 0.0239, Val Loss: 0.0247\n",
      "Epoch [45/200], Train Loss: 0.0239, Val Loss: 0.0247\n",
      "Epoch [46/200], Train Loss: 0.0239, Val Loss: 0.0247\n",
      "Epoch [47/200], Train Loss: 0.0239, Val Loss: 0.0246\n",
      "Epoch [48/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [49/200], Train Loss: 0.0238, Val Loss: 0.0247\n",
      "Epoch [50/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [51/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [52/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [53/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [54/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [55/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [56/200], Train Loss: 0.0237, Val Loss: 0.0246\n",
      "Epoch [57/200], Train Loss: 0.0237, Val Loss: 0.0246\n",
      "Epoch [58/200], Train Loss: 0.0237, Val Loss: 0.0246\n",
      "Epoch [59/200], Train Loss: 0.0237, Val Loss: 0.0246\n",
      "Epoch [60/200], Train Loss: 0.0237, Val Loss: 0.0247\n",
      "Epoch [61/200], Train Loss: 0.0237, Val Loss: 0.0246\n",
      "Epoch [62/200], Train Loss: 0.0237, Val Loss: 0.0246\n",
      "Epoch [63/200], Train Loss: 0.0237, Val Loss: 0.0246\n",
      "Epoch [64/200], Train Loss: 0.0236, Val Loss: 0.0247\n",
      "Epoch [65/200], Train Loss: 0.0236, Val Loss: 0.0246\n",
      "Epoch [66/200], Train Loss: 0.0236, Val Loss: 0.0246\n",
      "Epoch [67/200], Train Loss: 0.0236, Val Loss: 0.0246\n",
      "Epoch [68/200], Train Loss: 0.0236, Val Loss: 0.0246\n",
      "Early stopping at epoch 68\n",
      "Epoch [1/200], Train Loss: 0.0335, Val Loss: 0.0289\n",
      "Epoch [2/200], Train Loss: 0.0274, Val Loss: 0.0265\n",
      "Epoch [3/200], Train Loss: 0.0260, Val Loss: 0.0259\n",
      "Epoch [4/200], Train Loss: 0.0255, Val Loss: 0.0255\n",
      "Epoch [5/200], Train Loss: 0.0252, Val Loss: 0.0252\n",
      "Epoch [6/200], Train Loss: 0.0250, Val Loss: 0.0251\n",
      "Epoch [7/200], Train Loss: 0.0249, Val Loss: 0.0250\n",
      "Epoch [8/200], Train Loss: 0.0248, Val Loss: 0.0250\n",
      "Epoch [9/200], Train Loss: 0.0247, Val Loss: 0.0248\n",
      "Epoch [10/200], Train Loss: 0.0246, Val Loss: 0.0248\n",
      "Epoch [11/200], Train Loss: 0.0245, Val Loss: 0.0247\n",
      "Epoch [12/200], Train Loss: 0.0244, Val Loss: 0.0246\n",
      "Epoch [13/200], Train Loss: 0.0243, Val Loss: 0.0245\n",
      "Epoch [14/200], Train Loss: 0.0243, Val Loss: 0.0245\n",
      "Epoch [15/200], Train Loss: 0.0242, Val Loss: 0.0244\n",
      "Epoch [16/200], Train Loss: 0.0241, Val Loss: 0.0244\n",
      "Epoch [17/200], Train Loss: 0.0241, Val Loss: 0.0243\n",
      "Epoch [18/200], Train Loss: 0.0240, Val Loss: 0.0243\n",
      "Epoch [19/200], Train Loss: 0.0240, Val Loss: 0.0243\n",
      "Epoch [20/200], Train Loss: 0.0240, Val Loss: 0.0243\n",
      "Epoch [21/200], Train Loss: 0.0239, Val Loss: 0.0242\n",
      "Epoch [22/200], Train Loss: 0.0239, Val Loss: 0.0242\n",
      "Epoch [23/200], Train Loss: 0.0239, Val Loss: 0.0241\n",
      "Epoch [24/200], Train Loss: 0.0238, Val Loss: 0.0241\n",
      "Epoch [25/200], Train Loss: 0.0238, Val Loss: 0.0241\n",
      "Epoch [26/200], Train Loss: 0.0237, Val Loss: 0.0240\n",
      "Epoch [27/200], Train Loss: 0.0237, Val Loss: 0.0240\n",
      "Epoch [28/200], Train Loss: 0.0237, Val Loss: 0.0240\n",
      "Epoch [29/200], Train Loss: 0.0236, Val Loss: 0.0240\n",
      "Epoch [30/200], Train Loss: 0.0236, Val Loss: 0.0239\n",
      "Epoch [31/200], Train Loss: 0.0236, Val Loss: 0.0239\n",
      "Epoch [32/200], Train Loss: 0.0235, Val Loss: 0.0239\n",
      "Epoch [33/200], Train Loss: 0.0235, Val Loss: 0.0239\n",
      "Epoch [34/200], Train Loss: 0.0235, Val Loss: 0.0239\n",
      "Epoch [35/200], Train Loss: 0.0234, Val Loss: 0.0239\n",
      "Epoch [36/200], Train Loss: 0.0234, Val Loss: 0.0238\n",
      "Epoch [37/200], Train Loss: 0.0234, Val Loss: 0.0238\n",
      "Epoch [38/200], Train Loss: 0.0233, Val Loss: 0.0238\n",
      "Epoch [39/200], Train Loss: 0.0233, Val Loss: 0.0237\n",
      "Epoch [40/200], Train Loss: 0.0233, Val Loss: 0.0237\n",
      "Epoch [41/200], Train Loss: 0.0233, Val Loss: 0.0237\n",
      "Epoch [42/200], Train Loss: 0.0233, Val Loss: 0.0237\n",
      "Epoch [43/200], Train Loss: 0.0232, Val Loss: 0.0237\n",
      "Epoch [44/200], Train Loss: 0.0232, Val Loss: 0.0237\n",
      "Epoch [45/200], Train Loss: 0.0232, Val Loss: 0.0237\n",
      "Epoch [46/200], Train Loss: 0.0232, Val Loss: 0.0237\n",
      "Epoch [47/200], Train Loss: 0.0232, Val Loss: 0.0236\n",
      "Epoch [48/200], Train Loss: 0.0231, Val Loss: 0.0236\n",
      "Epoch [49/200], Train Loss: 0.0231, Val Loss: 0.0236\n",
      "Epoch [50/200], Train Loss: 0.0231, Val Loss: 0.0237\n",
      "Epoch [51/200], Train Loss: 0.0231, Val Loss: 0.0236\n",
      "Epoch [52/200], Train Loss: 0.0231, Val Loss: 0.0236\n",
      "Epoch [53/200], Train Loss: 0.0230, Val Loss: 0.0236\n",
      "Epoch [54/200], Train Loss: 0.0230, Val Loss: 0.0236\n",
      "Epoch [55/200], Train Loss: 0.0230, Val Loss: 0.0236\n",
      "Epoch [56/200], Train Loss: 0.0230, Val Loss: 0.0236\n",
      "Epoch [57/200], Train Loss: 0.0230, Val Loss: 0.0236\n",
      "Epoch [58/200], Train Loss: 0.0230, Val Loss: 0.0236\n",
      "Epoch [59/200], Train Loss: 0.0229, Val Loss: 0.0236\n",
      "Epoch [60/200], Train Loss: 0.0229, Val Loss: 0.0236\n",
      "Epoch [61/200], Train Loss: 0.0229, Val Loss: 0.0235\n",
      "Epoch [62/200], Train Loss: 0.0229, Val Loss: 0.0235\n",
      "Epoch [63/200], Train Loss: 0.0229, Val Loss: 0.0235\n",
      "Epoch [64/200], Train Loss: 0.0229, Val Loss: 0.0235\n",
      "Epoch [65/200], Train Loss: 0.0229, Val Loss: 0.0235\n",
      "Epoch [66/200], Train Loss: 0.0229, Val Loss: 0.0236\n",
      "Epoch [67/200], Train Loss: 0.0228, Val Loss: 0.0235\n",
      "Epoch [68/200], Train Loss: 0.0228, Val Loss: 0.0236\n",
      "Epoch [69/200], Train Loss: 0.0228, Val Loss: 0.0235\n",
      "Epoch [70/200], Train Loss: 0.0228, Val Loss: 0.0235\n",
      "Epoch [71/200], Train Loss: 0.0228, Val Loss: 0.0236\n",
      "Epoch [72/200], Train Loss: 0.0228, Val Loss: 0.0235\n",
      "Epoch [73/200], Train Loss: 0.0228, Val Loss: 0.0236\n",
      "Epoch [74/200], Train Loss: 0.0228, Val Loss: 0.0235\n",
      "Epoch [75/200], Train Loss: 0.0228, Val Loss: 0.0235\n",
      "Epoch [76/200], Train Loss: 0.0228, Val Loss: 0.0235\n",
      "Epoch [77/200], Train Loss: 0.0228, Val Loss: 0.0235\n",
      "Epoch [78/200], Train Loss: 0.0227, Val Loss: 0.0235\n",
      "Epoch [79/200], Train Loss: 0.0227, Val Loss: 0.0235\n",
      "Early stopping at epoch 79\n",
      "Epoch [1/200], Train Loss: 0.0331, Val Loss: 0.0282\n",
      "Epoch [2/200], Train Loss: 0.0269, Val Loss: 0.0260\n",
      "Epoch [3/200], Train Loss: 0.0255, Val Loss: 0.0251\n",
      "Epoch [4/200], Train Loss: 0.0249, Val Loss: 0.0248\n",
      "Epoch [5/200], Train Loss: 0.0246, Val Loss: 0.0246\n",
      "Epoch [6/200], Train Loss: 0.0245, Val Loss: 0.0245\n",
      "Epoch [7/200], Train Loss: 0.0243, Val Loss: 0.0243\n",
      "Epoch [8/200], Train Loss: 0.0242, Val Loss: 0.0242\n",
      "Epoch [9/200], Train Loss: 0.0241, Val Loss: 0.0242\n",
      "Epoch [10/200], Train Loss: 0.0240, Val Loss: 0.0244\n",
      "Epoch [11/200], Train Loss: 0.0239, Val Loss: 0.0241\n",
      "Epoch [12/200], Train Loss: 0.0239, Val Loss: 0.0240\n",
      "Epoch [13/200], Train Loss: 0.0238, Val Loss: 0.0240\n",
      "Epoch [14/200], Train Loss: 0.0238, Val Loss: 0.0239\n",
      "Epoch [15/200], Train Loss: 0.0237, Val Loss: 0.0239\n",
      "Epoch [16/200], Train Loss: 0.0237, Val Loss: 0.0239\n",
      "Epoch [17/200], Train Loss: 0.0236, Val Loss: 0.0238\n",
      "Epoch [18/200], Train Loss: 0.0236, Val Loss: 0.0238\n",
      "Epoch [19/200], Train Loss: 0.0235, Val Loss: 0.0237\n",
      "Epoch [20/200], Train Loss: 0.0235, Val Loss: 0.0237\n",
      "Epoch [21/200], Train Loss: 0.0234, Val Loss: 0.0237\n",
      "Epoch [22/200], Train Loss: 0.0234, Val Loss: 0.0236\n",
      "Epoch [23/200], Train Loss: 0.0234, Val Loss: 0.0237\n",
      "Epoch [24/200], Train Loss: 0.0233, Val Loss: 0.0236\n",
      "Epoch [25/200], Train Loss: 0.0233, Val Loss: 0.0236\n",
      "Epoch [26/200], Train Loss: 0.0233, Val Loss: 0.0235\n",
      "Epoch [27/200], Train Loss: 0.0232, Val Loss: 0.0236\n",
      "Epoch [28/200], Train Loss: 0.0232, Val Loss: 0.0235\n",
      "Epoch [29/200], Train Loss: 0.0231, Val Loss: 0.0234\n",
      "Epoch [30/200], Train Loss: 0.0231, Val Loss: 0.0235\n",
      "Epoch [31/200], Train Loss: 0.0231, Val Loss: 0.0234\n",
      "Epoch [32/200], Train Loss: 0.0230, Val Loss: 0.0234\n",
      "Epoch [33/200], Train Loss: 0.0230, Val Loss: 0.0233\n",
      "Epoch [34/200], Train Loss: 0.0229, Val Loss: 0.0233\n",
      "Epoch [35/200], Train Loss: 0.0229, Val Loss: 0.0234\n",
      "Epoch [36/200], Train Loss: 0.0229, Val Loss: 0.0233\n",
      "Epoch [37/200], Train Loss: 0.0228, Val Loss: 0.0233\n",
      "Epoch [38/200], Train Loss: 0.0228, Val Loss: 0.0233\n",
      "Epoch [39/200], Train Loss: 0.0228, Val Loss: 0.0233\n",
      "Epoch [40/200], Train Loss: 0.0228, Val Loss: 0.0232\n",
      "Epoch [41/200], Train Loss: 0.0227, Val Loss: 0.0232\n",
      "Epoch [42/200], Train Loss: 0.0227, Val Loss: 0.0232\n",
      "Epoch [43/200], Train Loss: 0.0227, Val Loss: 0.0232\n",
      "Epoch [44/200], Train Loss: 0.0227, Val Loss: 0.0231\n",
      "Epoch [45/200], Train Loss: 0.0226, Val Loss: 0.0231\n",
      "Epoch [46/200], Train Loss: 0.0226, Val Loss: 0.0231\n",
      "Epoch [47/200], Train Loss: 0.0226, Val Loss: 0.0231\n",
      "Epoch [48/200], Train Loss: 0.0226, Val Loss: 0.0231\n",
      "Epoch [49/200], Train Loss: 0.0226, Val Loss: 0.0231\n",
      "Epoch [50/200], Train Loss: 0.0226, Val Loss: 0.0231\n",
      "Epoch [51/200], Train Loss: 0.0226, Val Loss: 0.0230\n",
      "Epoch [52/200], Train Loss: 0.0225, Val Loss: 0.0231\n",
      "Epoch [53/200], Train Loss: 0.0225, Val Loss: 0.0231\n",
      "Epoch [54/200], Train Loss: 0.0225, Val Loss: 0.0231\n",
      "Epoch [55/200], Train Loss: 0.0225, Val Loss: 0.0230\n",
      "Epoch [56/200], Train Loss: 0.0225, Val Loss: 0.0231\n",
      "Epoch [57/200], Train Loss: 0.0225, Val Loss: 0.0230\n",
      "Epoch [58/200], Train Loss: 0.0224, Val Loss: 0.0230\n",
      "Epoch [59/200], Train Loss: 0.0224, Val Loss: 0.0230\n",
      "Epoch [60/200], Train Loss: 0.0224, Val Loss: 0.0230\n",
      "Epoch [61/200], Train Loss: 0.0224, Val Loss: 0.0230\n",
      "Epoch [62/200], Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Epoch [63/200], Train Loss: 0.0224, Val Loss: 0.0230\n",
      "Epoch [64/200], Train Loss: 0.0224, Val Loss: 0.0230\n",
      "Epoch [65/200], Train Loss: 0.0224, Val Loss: 0.0230\n",
      "Epoch [66/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [67/200], Train Loss: 0.0223, Val Loss: 0.0231\n",
      "Epoch [68/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [69/200], Train Loss: 0.0223, Val Loss: 0.0229\n",
      "Epoch [70/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [71/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [72/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [73/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [74/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [75/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [76/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [77/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [78/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [79/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Early stopping at epoch 79\n",
      "Epoch [1/200], Train Loss: 0.0325, Val Loss: 0.0279\n",
      "Epoch [2/200], Train Loss: 0.0265, Val Loss: 0.0257\n",
      "Epoch [3/200], Train Loss: 0.0251, Val Loss: 0.0246\n",
      "Epoch [4/200], Train Loss: 0.0243, Val Loss: 0.0243\n",
      "Epoch [5/200], Train Loss: 0.0240, Val Loss: 0.0240\n",
      "Epoch [6/200], Train Loss: 0.0238, Val Loss: 0.0240\n",
      "Epoch [7/200], Train Loss: 0.0238, Val Loss: 0.0239\n",
      "Epoch [8/200], Train Loss: 0.0237, Val Loss: 0.0238\n",
      "Epoch [9/200], Train Loss: 0.0236, Val Loss: 0.0239\n",
      "Epoch [10/200], Train Loss: 0.0235, Val Loss: 0.0237\n",
      "Epoch [11/200], Train Loss: 0.0234, Val Loss: 0.0236\n",
      "Epoch [12/200], Train Loss: 0.0233, Val Loss: 0.0235\n",
      "Epoch [13/200], Train Loss: 0.0232, Val Loss: 0.0234\n",
      "Epoch [14/200], Train Loss: 0.0231, Val Loss: 0.0234\n",
      "Epoch [15/200], Train Loss: 0.0230, Val Loss: 0.0233\n",
      "Epoch [16/200], Train Loss: 0.0229, Val Loss: 0.0232\n",
      "Epoch [17/200], Train Loss: 0.0229, Val Loss: 0.0232\n",
      "Epoch [18/200], Train Loss: 0.0228, Val Loss: 0.0231\n",
      "Epoch [19/200], Train Loss: 0.0227, Val Loss: 0.0230\n",
      "Epoch [20/200], Train Loss: 0.0227, Val Loss: 0.0230\n",
      "Epoch [21/200], Train Loss: 0.0226, Val Loss: 0.0229\n",
      "Epoch [22/200], Train Loss: 0.0226, Val Loss: 0.0229\n",
      "Epoch [23/200], Train Loss: 0.0225, Val Loss: 0.0228\n",
      "Epoch [24/200], Train Loss: 0.0225, Val Loss: 0.0228\n",
      "Epoch [25/200], Train Loss: 0.0224, Val Loss: 0.0228\n",
      "Epoch [26/200], Train Loss: 0.0224, Val Loss: 0.0227\n",
      "Epoch [27/200], Train Loss: 0.0223, Val Loss: 0.0228\n",
      "Epoch [28/200], Train Loss: 0.0223, Val Loss: 0.0227\n",
      "Epoch [29/200], Train Loss: 0.0223, Val Loss: 0.0227\n",
      "Epoch [30/200], Train Loss: 0.0222, Val Loss: 0.0226\n",
      "Epoch [31/200], Train Loss: 0.0222, Val Loss: 0.0226\n",
      "Epoch [32/200], Train Loss: 0.0222, Val Loss: 0.0225\n",
      "Epoch [33/200], Train Loss: 0.0221, Val Loss: 0.0225\n",
      "Epoch [34/200], Train Loss: 0.0221, Val Loss: 0.0226\n",
      "Epoch [35/200], Train Loss: 0.0221, Val Loss: 0.0225\n",
      "Epoch [36/200], Train Loss: 0.0221, Val Loss: 0.0225\n",
      "Epoch [37/200], Train Loss: 0.0220, Val Loss: 0.0225\n",
      "Epoch [38/200], Train Loss: 0.0220, Val Loss: 0.0225\n",
      "Epoch [39/200], Train Loss: 0.0220, Val Loss: 0.0224\n",
      "Epoch [40/200], Train Loss: 0.0220, Val Loss: 0.0224\n",
      "Epoch [41/200], Train Loss: 0.0220, Val Loss: 0.0224\n",
      "Epoch [42/200], Train Loss: 0.0220, Val Loss: 0.0224\n",
      "Epoch [43/200], Train Loss: 0.0219, Val Loss: 0.0225\n",
      "Epoch [44/200], Train Loss: 0.0219, Val Loss: 0.0223\n",
      "Epoch [45/200], Train Loss: 0.0219, Val Loss: 0.0224\n",
      "Epoch [46/200], Train Loss: 0.0219, Val Loss: 0.0223\n",
      "Epoch [47/200], Train Loss: 0.0219, Val Loss: 0.0223\n",
      "Epoch [48/200], Train Loss: 0.0218, Val Loss: 0.0223\n",
      "Epoch [49/200], Train Loss: 0.0218, Val Loss: 0.0223\n",
      "Epoch [50/200], Train Loss: 0.0218, Val Loss: 0.0223\n",
      "Epoch [51/200], Train Loss: 0.0218, Val Loss: 0.0223\n",
      "Epoch [52/200], Train Loss: 0.0218, Val Loss: 0.0223\n",
      "Epoch [53/200], Train Loss: 0.0218, Val Loss: 0.0223\n",
      "Epoch [54/200], Train Loss: 0.0217, Val Loss: 0.0223\n",
      "Epoch [55/200], Train Loss: 0.0217, Val Loss: 0.0223\n",
      "Epoch [56/200], Train Loss: 0.0217, Val Loss: 0.0223\n",
      "Epoch [57/200], Train Loss: 0.0217, Val Loss: 0.0222\n",
      "Epoch [58/200], Train Loss: 0.0217, Val Loss: 0.0222\n",
      "Epoch [59/200], Train Loss: 0.0217, Val Loss: 0.0222\n",
      "Epoch [60/200], Train Loss: 0.0217, Val Loss: 0.0222\n",
      "Epoch [61/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [62/200], Train Loss: 0.0216, Val Loss: 0.0221\n",
      "Epoch [63/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [64/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [65/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [66/200], Train Loss: 0.0216, Val Loss: 0.0221\n",
      "Epoch [67/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [68/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [69/200], Train Loss: 0.0215, Val Loss: 0.0222\n",
      "Epoch [70/200], Train Loss: 0.0215, Val Loss: 0.0221\n",
      "Epoch [71/200], Train Loss: 0.0215, Val Loss: 0.0221\n",
      "Epoch [72/200], Train Loss: 0.0215, Val Loss: 0.0221\n",
      "Epoch [73/200], Train Loss: 0.0215, Val Loss: 0.0221\n",
      "Epoch [74/200], Train Loss: 0.0215, Val Loss: 0.0221\n",
      "Epoch [75/200], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [76/200], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [77/200], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [78/200], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [79/200], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [80/200], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [81/200], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [82/200], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [83/200], Train Loss: 0.0214, Val Loss: 0.0221\n",
      "Epoch [84/200], Train Loss: 0.0213, Val Loss: 0.0221\n",
      "Epoch [85/200], Train Loss: 0.0213, Val Loss: 0.0221\n",
      "Epoch [86/200], Train Loss: 0.0213, Val Loss: 0.0220\n",
      "Epoch [87/200], Train Loss: 0.0213, Val Loss: 0.0220\n",
      "Epoch [88/200], Train Loss: 0.0213, Val Loss: 0.0221\n",
      "Epoch [89/200], Train Loss: 0.0213, Val Loss: 0.0220\n",
      "Epoch [90/200], Train Loss: 0.0213, Val Loss: 0.0220\n",
      "Epoch [91/200], Train Loss: 0.0213, Val Loss: 0.0220\n",
      "Epoch [92/200], Train Loss: 0.0213, Val Loss: 0.0220\n",
      "Epoch [93/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [94/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [95/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [96/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [97/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [98/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [99/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [100/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [101/200], Train Loss: 0.0212, Val Loss: 0.0219\n",
      "Epoch [102/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [103/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [104/200], Train Loss: 0.0212, Val Loss: 0.0220\n",
      "Epoch [105/200], Train Loss: 0.0211, Val Loss: 0.0220\n",
      "Epoch [106/200], Train Loss: 0.0211, Val Loss: 0.0220\n",
      "Epoch [107/200], Train Loss: 0.0211, Val Loss: 0.0219\n",
      "Epoch [108/200], Train Loss: 0.0211, Val Loss: 0.0219\n",
      "Epoch [109/200], Train Loss: 0.0211, Val Loss: 0.0220\n",
      "Epoch [110/200], Train Loss: 0.0211, Val Loss: 0.0220\n",
      "Epoch [111/200], Train Loss: 0.0211, Val Loss: 0.0220\n",
      "Epoch [112/200], Train Loss: 0.0211, Val Loss: 0.0220\n",
      "Epoch [113/200], Train Loss: 0.0211, Val Loss: 0.0221\n",
      "Epoch [114/200], Train Loss: 0.0211, Val Loss: 0.0220\n",
      "Epoch [115/200], Train Loss: 0.0211, Val Loss: 0.0219\n",
      "Epoch [116/200], Train Loss: 0.0211, Val Loss: 0.0219\n",
      "Epoch [117/200], Train Loss: 0.0210, Val Loss: 0.0219\n",
      "Early stopping at epoch 117\n",
      "Epoch [1/200], Train Loss: 0.0331, Val Loss: 0.0285\n",
      "Epoch [2/200], Train Loss: 0.0272, Val Loss: 0.0263\n",
      "Epoch [3/200], Train Loss: 0.0254, Val Loss: 0.0249\n",
      "Epoch [4/200], Train Loss: 0.0244, Val Loss: 0.0242\n",
      "Epoch [5/200], Train Loss: 0.0237, Val Loss: 0.0237\n",
      "Epoch [6/200], Train Loss: 0.0234, Val Loss: 0.0235\n",
      "Epoch [7/200], Train Loss: 0.0232, Val Loss: 0.0233\n",
      "Epoch [8/200], Train Loss: 0.0231, Val Loss: 0.0233\n",
      "Epoch [9/200], Train Loss: 0.0230, Val Loss: 0.0232\n",
      "Epoch [10/200], Train Loss: 0.0229, Val Loss: 0.0231\n",
      "Epoch [11/200], Train Loss: 0.0228, Val Loss: 0.0231\n",
      "Epoch [12/200], Train Loss: 0.0228, Val Loss: 0.0229\n",
      "Epoch [13/200], Train Loss: 0.0227, Val Loss: 0.0229\n",
      "Epoch [14/200], Train Loss: 0.0226, Val Loss: 0.0227\n",
      "Epoch [15/200], Train Loss: 0.0225, Val Loss: 0.0227\n",
      "Epoch [16/200], Train Loss: 0.0224, Val Loss: 0.0226\n",
      "Epoch [17/200], Train Loss: 0.0223, Val Loss: 0.0226\n",
      "Epoch [18/200], Train Loss: 0.0223, Val Loss: 0.0226\n",
      "Epoch [19/200], Train Loss: 0.0222, Val Loss: 0.0225\n",
      "Epoch [20/200], Train Loss: 0.0222, Val Loss: 0.0225\n",
      "Epoch [21/200], Train Loss: 0.0221, Val Loss: 0.0225\n",
      "Epoch [22/200], Train Loss: 0.0221, Val Loss: 0.0224\n",
      "Epoch [23/200], Train Loss: 0.0221, Val Loss: 0.0224\n",
      "Epoch [24/200], Train Loss: 0.0220, Val Loss: 0.0223\n",
      "Epoch [25/200], Train Loss: 0.0220, Val Loss: 0.0223\n",
      "Epoch [26/200], Train Loss: 0.0219, Val Loss: 0.0223\n",
      "Epoch [27/200], Train Loss: 0.0219, Val Loss: 0.0222\n",
      "Epoch [28/200], Train Loss: 0.0219, Val Loss: 0.0223\n",
      "Epoch [29/200], Train Loss: 0.0218, Val Loss: 0.0222\n",
      "Epoch [30/200], Train Loss: 0.0218, Val Loss: 0.0222\n",
      "Epoch [31/200], Train Loss: 0.0218, Val Loss: 0.0222\n",
      "Epoch [32/200], Train Loss: 0.0218, Val Loss: 0.0221\n",
      "Epoch [33/200], Train Loss: 0.0217, Val Loss: 0.0221\n",
      "Epoch [34/200], Train Loss: 0.0217, Val Loss: 0.0221\n",
      "Epoch [35/200], Train Loss: 0.0217, Val Loss: 0.0221\n",
      "Epoch [36/200], Train Loss: 0.0216, Val Loss: 0.0221\n",
      "Epoch [37/200], Train Loss: 0.0216, Val Loss: 0.0221\n",
      "Epoch [38/200], Train Loss: 0.0216, Val Loss: 0.0220\n",
      "Epoch [39/200], Train Loss: 0.0216, Val Loss: 0.0220\n",
      "Epoch [40/200], Train Loss: 0.0216, Val Loss: 0.0220\n",
      "Epoch [41/200], Train Loss: 0.0215, Val Loss: 0.0220\n",
      "Epoch [42/200], Train Loss: 0.0215, Val Loss: 0.0220\n",
      "Epoch [43/200], Train Loss: 0.0215, Val Loss: 0.0219\n",
      "Epoch [44/200], Train Loss: 0.0215, Val Loss: 0.0219\n",
      "Epoch [45/200], Train Loss: 0.0214, Val Loss: 0.0218\n",
      "Epoch [46/200], Train Loss: 0.0214, Val Loss: 0.0219\n",
      "Epoch [47/200], Train Loss: 0.0214, Val Loss: 0.0219\n",
      "Epoch [48/200], Train Loss: 0.0214, Val Loss: 0.0218\n",
      "Epoch [49/200], Train Loss: 0.0213, Val Loss: 0.0218\n",
      "Epoch [50/200], Train Loss: 0.0213, Val Loss: 0.0218\n",
      "Epoch [51/200], Train Loss: 0.0213, Val Loss: 0.0218\n",
      "Epoch [52/200], Train Loss: 0.0213, Val Loss: 0.0218\n",
      "Epoch [53/200], Train Loss: 0.0213, Val Loss: 0.0218\n",
      "Epoch [54/200], Train Loss: 0.0212, Val Loss: 0.0217\n",
      "Epoch [55/200], Train Loss: 0.0212, Val Loss: 0.0217\n",
      "Epoch [56/200], Train Loss: 0.0212, Val Loss: 0.0218\n",
      "Epoch [57/200], Train Loss: 0.0212, Val Loss: 0.0217\n",
      "Epoch [58/200], Train Loss: 0.0212, Val Loss: 0.0217\n",
      "Epoch [59/200], Train Loss: 0.0212, Val Loss: 0.0217\n",
      "Epoch [60/200], Train Loss: 0.0211, Val Loss: 0.0217\n",
      "Epoch [61/200], Train Loss: 0.0211, Val Loss: 0.0216\n",
      "Epoch [62/200], Train Loss: 0.0211, Val Loss: 0.0216\n",
      "Epoch [63/200], Train Loss: 0.0211, Val Loss: 0.0217\n",
      "Epoch [64/200], Train Loss: 0.0211, Val Loss: 0.0217\n",
      "Epoch [65/200], Train Loss: 0.0210, Val Loss: 0.0217\n",
      "Epoch [66/200], Train Loss: 0.0210, Val Loss: 0.0216\n",
      "Epoch [67/200], Train Loss: 0.0210, Val Loss: 0.0216\n",
      "Epoch [68/200], Train Loss: 0.0210, Val Loss: 0.0216\n",
      "Epoch [69/200], Train Loss: 0.0210, Val Loss: 0.0216\n",
      "Epoch [70/200], Train Loss: 0.0210, Val Loss: 0.0216\n",
      "Epoch [71/200], Train Loss: 0.0210, Val Loss: 0.0216\n",
      "Epoch [72/200], Train Loss: 0.0210, Val Loss: 0.0215\n",
      "Epoch [73/200], Train Loss: 0.0210, Val Loss: 0.0216\n",
      "Epoch [74/200], Train Loss: 0.0209, Val Loss: 0.0215\n",
      "Epoch [75/200], Train Loss: 0.0209, Val Loss: 0.0216\n",
      "Epoch [76/200], Train Loss: 0.0209, Val Loss: 0.0215\n",
      "Epoch [77/200], Train Loss: 0.0209, Val Loss: 0.0215\n",
      "Epoch [78/200], Train Loss: 0.0209, Val Loss: 0.0216\n",
      "Epoch [79/200], Train Loss: 0.0209, Val Loss: 0.0215\n",
      "Epoch [80/200], Train Loss: 0.0209, Val Loss: 0.0215\n",
      "Epoch [81/200], Train Loss: 0.0208, Val Loss: 0.0215\n",
      "Epoch [82/200], Train Loss: 0.0208, Val Loss: 0.0215\n",
      "Epoch [83/200], Train Loss: 0.0208, Val Loss: 0.0214\n",
      "Epoch [84/200], Train Loss: 0.0208, Val Loss: 0.0215\n",
      "Epoch [85/200], Train Loss: 0.0208, Val Loss: 0.0215\n",
      "Epoch [86/200], Train Loss: 0.0208, Val Loss: 0.0215\n",
      "Epoch [87/200], Train Loss: 0.0208, Val Loss: 0.0214\n",
      "Epoch [88/200], Train Loss: 0.0208, Val Loss: 0.0214\n",
      "Epoch [89/200], Train Loss: 0.0208, Val Loss: 0.0215\n",
      "Epoch [90/200], Train Loss: 0.0208, Val Loss: 0.0214\n",
      "Epoch [91/200], Train Loss: 0.0208, Val Loss: 0.0215\n",
      "Epoch [92/200], Train Loss: 0.0207, Val Loss: 0.0214\n",
      "Epoch [93/200], Train Loss: 0.0207, Val Loss: 0.0214\n",
      "Epoch [94/200], Train Loss: 0.0207, Val Loss: 0.0214\n",
      "Epoch [95/200], Train Loss: 0.0207, Val Loss: 0.0214\n",
      "Epoch [96/200], Train Loss: 0.0207, Val Loss: 0.0215\n",
      "Epoch [97/200], Train Loss: 0.0207, Val Loss: 0.0215\n",
      "Epoch [98/200], Train Loss: 0.0207, Val Loss: 0.0214\n",
      "Epoch [99/200], Train Loss: 0.0207, Val Loss: 0.0214\n",
      "Epoch [100/200], Train Loss: 0.0207, Val Loss: 0.0214\n",
      "Epoch [101/200], Train Loss: 0.0207, Val Loss: 0.0214\n",
      "Epoch [102/200], Train Loss: 0.0206, Val Loss: 0.0215\n",
      "Epoch [103/200], Train Loss: 0.0206, Val Loss: 0.0215\n",
      "Epoch [104/200], Train Loss: 0.0206, Val Loss: 0.0213\n",
      "Epoch [105/200], Train Loss: 0.0206, Val Loss: 0.0214\n",
      "Epoch [106/200], Train Loss: 0.0206, Val Loss: 0.0214\n",
      "Epoch [107/200], Train Loss: 0.0206, Val Loss: 0.0214\n",
      "Epoch [108/200], Train Loss: 0.0206, Val Loss: 0.0214\n",
      "Epoch [109/200], Train Loss: 0.0206, Val Loss: 0.0214\n",
      "Epoch [110/200], Train Loss: 0.0206, Val Loss: 0.0214\n",
      "Epoch [111/200], Train Loss: 0.0206, Val Loss: 0.0214\n",
      "Epoch [112/200], Train Loss: 0.0206, Val Loss: 0.0214\n",
      "Epoch [113/200], Train Loss: 0.0206, Val Loss: 0.0214\n",
      "Epoch [114/200], Train Loss: 0.0205, Val Loss: 0.0214\n",
      "Early stopping at epoch 114\n",
      "Epoch [1/200], Train Loss: 0.0324, Val Loss: 0.0277\n",
      "Epoch [2/200], Train Loss: 0.0263, Val Loss: 0.0252\n",
      "Epoch [3/200], Train Loss: 0.0244, Val Loss: 0.0236\n",
      "Epoch [4/200], Train Loss: 0.0232, Val Loss: 0.0230\n",
      "Epoch [5/200], Train Loss: 0.0228, Val Loss: 0.0228\n",
      "Epoch [6/200], Train Loss: 0.0226, Val Loss: 0.0227\n",
      "Epoch [7/200], Train Loss: 0.0224, Val Loss: 0.0226\n",
      "Epoch [8/200], Train Loss: 0.0224, Val Loss: 0.0225\n",
      "Epoch [9/200], Train Loss: 0.0223, Val Loss: 0.0225\n",
      "Epoch [10/200], Train Loss: 0.0222, Val Loss: 0.0224\n",
      "Epoch [11/200], Train Loss: 0.0221, Val Loss: 0.0224\n",
      "Epoch [12/200], Train Loss: 0.0221, Val Loss: 0.0223\n",
      "Epoch [13/200], Train Loss: 0.0220, Val Loss: 0.0223\n",
      "Epoch [14/200], Train Loss: 0.0220, Val Loss: 0.0222\n",
      "Epoch [15/200], Train Loss: 0.0219, Val Loss: 0.0222\n",
      "Epoch [16/200], Train Loss: 0.0219, Val Loss: 0.0222\n",
      "Epoch [17/200], Train Loss: 0.0219, Val Loss: 0.0222\n",
      "Epoch [18/200], Train Loss: 0.0218, Val Loss: 0.0221\n",
      "Epoch [19/200], Train Loss: 0.0218, Val Loss: 0.0221\n",
      "Epoch [20/200], Train Loss: 0.0217, Val Loss: 0.0219\n",
      "Epoch [21/200], Train Loss: 0.0217, Val Loss: 0.0220\n",
      "Epoch [22/200], Train Loss: 0.0216, Val Loss: 0.0220\n",
      "Epoch [23/200], Train Loss: 0.0216, Val Loss: 0.0220\n",
      "Epoch [24/200], Train Loss: 0.0216, Val Loss: 0.0219\n",
      "Epoch [25/200], Train Loss: 0.0215, Val Loss: 0.0219\n",
      "Epoch [26/200], Train Loss: 0.0215, Val Loss: 0.0218\n",
      "Epoch [27/200], Train Loss: 0.0215, Val Loss: 0.0218\n",
      "Epoch [28/200], Train Loss: 0.0214, Val Loss: 0.0218\n",
      "Epoch [29/200], Train Loss: 0.0214, Val Loss: 0.0217\n",
      "Epoch [30/200], Train Loss: 0.0214, Val Loss: 0.0217\n",
      "Epoch [31/200], Train Loss: 0.0214, Val Loss: 0.0217\n",
      "Epoch [32/200], Train Loss: 0.0213, Val Loss: 0.0217\n",
      "Epoch [33/200], Train Loss: 0.0213, Val Loss: 0.0217\n",
      "Epoch [34/200], Train Loss: 0.0213, Val Loss: 0.0216\n",
      "Epoch [35/200], Train Loss: 0.0212, Val Loss: 0.0216\n",
      "Epoch [36/200], Train Loss: 0.0212, Val Loss: 0.0216\n",
      "Epoch [37/200], Train Loss: 0.0212, Val Loss: 0.0216\n",
      "Epoch [38/200], Train Loss: 0.0212, Val Loss: 0.0216\n",
      "Epoch [39/200], Train Loss: 0.0212, Val Loss: 0.0216\n",
      "Epoch [40/200], Train Loss: 0.0211, Val Loss: 0.0216\n",
      "Epoch [41/200], Train Loss: 0.0211, Val Loss: 0.0215\n",
      "Epoch [42/200], Train Loss: 0.0211, Val Loss: 0.0215\n",
      "Epoch [43/200], Train Loss: 0.0211, Val Loss: 0.0215\n",
      "Epoch [44/200], Train Loss: 0.0211, Val Loss: 0.0215\n",
      "Epoch [45/200], Train Loss: 0.0210, Val Loss: 0.0215\n",
      "Epoch [46/200], Train Loss: 0.0210, Val Loss: 0.0214\n",
      "Epoch [47/200], Train Loss: 0.0210, Val Loss: 0.0214\n",
      "Epoch [48/200], Train Loss: 0.0210, Val Loss: 0.0214\n",
      "Epoch [49/200], Train Loss: 0.0210, Val Loss: 0.0214\n",
      "Epoch [50/200], Train Loss: 0.0209, Val Loss: 0.0213\n",
      "Epoch [51/200], Train Loss: 0.0209, Val Loss: 0.0213\n",
      "Epoch [52/200], Train Loss: 0.0209, Val Loss: 0.0213\n",
      "Epoch [53/200], Train Loss: 0.0209, Val Loss: 0.0213\n",
      "Epoch [54/200], Train Loss: 0.0209, Val Loss: 0.0213\n",
      "Epoch [55/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [56/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [57/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [58/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [59/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [60/200], Train Loss: 0.0207, Val Loss: 0.0213\n",
      "Epoch [61/200], Train Loss: 0.0207, Val Loss: 0.0212\n",
      "Epoch [62/200], Train Loss: 0.0207, Val Loss: 0.0211\n",
      "Epoch [63/200], Train Loss: 0.0207, Val Loss: 0.0212\n",
      "Epoch [64/200], Train Loss: 0.0207, Val Loss: 0.0212\n",
      "Epoch [65/200], Train Loss: 0.0207, Val Loss: 0.0211\n",
      "Epoch [66/200], Train Loss: 0.0206, Val Loss: 0.0212\n",
      "Epoch [67/200], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [68/200], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [69/200], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [70/200], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [71/200], Train Loss: 0.0205, Val Loss: 0.0211\n",
      "Epoch [72/200], Train Loss: 0.0205, Val Loss: 0.0210\n",
      "Epoch [73/200], Train Loss: 0.0205, Val Loss: 0.0210\n",
      "Epoch [74/200], Train Loss: 0.0205, Val Loss: 0.0210\n",
      "Epoch [75/200], Train Loss: 0.0204, Val Loss: 0.0209\n",
      "Epoch [76/200], Train Loss: 0.0204, Val Loss: 0.0210\n",
      "Epoch [77/200], Train Loss: 0.0204, Val Loss: 0.0210\n",
      "Epoch [78/200], Train Loss: 0.0204, Val Loss: 0.0209\n",
      "Epoch [79/200], Train Loss: 0.0204, Val Loss: 0.0208\n",
      "Epoch [80/200], Train Loss: 0.0203, Val Loss: 0.0210\n",
      "Epoch [81/200], Train Loss: 0.0203, Val Loss: 0.0209\n",
      "Epoch [82/200], Train Loss: 0.0203, Val Loss: 0.0210\n",
      "Epoch [83/200], Train Loss: 0.0203, Val Loss: 0.0208\n",
      "Epoch [84/200], Train Loss: 0.0203, Val Loss: 0.0208\n",
      "Epoch [85/200], Train Loss: 0.0202, Val Loss: 0.0209\n",
      "Epoch [86/200], Train Loss: 0.0202, Val Loss: 0.0208\n",
      "Epoch [87/200], Train Loss: 0.0202, Val Loss: 0.0208\n",
      "Epoch [88/200], Train Loss: 0.0202, Val Loss: 0.0207\n",
      "Epoch [89/200], Train Loss: 0.0202, Val Loss: 0.0207\n",
      "Epoch [90/200], Train Loss: 0.0202, Val Loss: 0.0208\n",
      "Epoch [91/200], Train Loss: 0.0202, Val Loss: 0.0208\n",
      "Epoch [92/200], Train Loss: 0.0201, Val Loss: 0.0207\n",
      "Epoch [93/200], Train Loss: 0.0201, Val Loss: 0.0207\n",
      "Epoch [94/200], Train Loss: 0.0201, Val Loss: 0.0207\n",
      "Epoch [95/200], Train Loss: 0.0201, Val Loss: 0.0207\n",
      "Epoch [96/200], Train Loss: 0.0201, Val Loss: 0.0207\n",
      "Epoch [97/200], Train Loss: 0.0201, Val Loss: 0.0206\n",
      "Epoch [98/200], Train Loss: 0.0200, Val Loss: 0.0207\n",
      "Epoch [99/200], Train Loss: 0.0200, Val Loss: 0.0206\n",
      "Epoch [100/200], Train Loss: 0.0200, Val Loss: 0.0207\n",
      "Epoch [101/200], Train Loss: 0.0200, Val Loss: 0.0207\n",
      "Epoch [102/200], Train Loss: 0.0200, Val Loss: 0.0207\n",
      "Epoch [103/200], Train Loss: 0.0200, Val Loss: 0.0207\n",
      "Epoch [104/200], Train Loss: 0.0200, Val Loss: 0.0206\n",
      "Epoch [105/200], Train Loss: 0.0200, Val Loss: 0.0206\n",
      "Epoch [106/200], Train Loss: 0.0200, Val Loss: 0.0207\n",
      "Epoch [107/200], Train Loss: 0.0200, Val Loss: 0.0206\n",
      "Epoch [108/200], Train Loss: 0.0200, Val Loss: 0.0207\n",
      "Epoch [109/200], Train Loss: 0.0200, Val Loss: 0.0206\n",
      "Epoch [110/200], Train Loss: 0.0199, Val Loss: 0.0206\n",
      "Epoch [111/200], Train Loss: 0.0199, Val Loss: 0.0206\n",
      "Epoch [112/200], Train Loss: 0.0199, Val Loss: 0.0206\n",
      "Epoch [113/200], Train Loss: 0.0199, Val Loss: 0.0206\n",
      "Epoch [114/200], Train Loss: 0.0199, Val Loss: 0.0206\n",
      "Epoch [115/200], Train Loss: 0.0199, Val Loss: 0.0205\n",
      "Epoch [116/200], Train Loss: 0.0198, Val Loss: 0.0206\n",
      "Epoch [117/200], Train Loss: 0.0199, Val Loss: 0.0206\n",
      "Epoch [118/200], Train Loss: 0.0198, Val Loss: 0.0206\n",
      "Epoch [119/200], Train Loss: 0.0198, Val Loss: 0.0205\n",
      "Epoch [120/200], Train Loss: 0.0198, Val Loss: 0.0205\n",
      "Epoch [121/200], Train Loss: 0.0198, Val Loss: 0.0205\n",
      "Epoch [122/200], Train Loss: 0.0198, Val Loss: 0.0206\n",
      "Epoch [123/200], Train Loss: 0.0198, Val Loss: 0.0205\n",
      "Epoch [124/200], Train Loss: 0.0198, Val Loss: 0.0205\n",
      "Epoch [125/200], Train Loss: 0.0198, Val Loss: 0.0205\n",
      "Epoch [126/200], Train Loss: 0.0197, Val Loss: 0.0205\n",
      "Epoch [127/200], Train Loss: 0.0197, Val Loss: 0.0205\n",
      "Epoch [128/200], Train Loss: 0.0197, Val Loss: 0.0204\n",
      "Epoch [129/200], Train Loss: 0.0197, Val Loss: 0.0205\n",
      "Epoch [130/200], Train Loss: 0.0197, Val Loss: 0.0205\n",
      "Epoch [131/200], Train Loss: 0.0197, Val Loss: 0.0205\n",
      "Epoch [132/200], Train Loss: 0.0197, Val Loss: 0.0205\n",
      "Epoch [133/200], Train Loss: 0.0197, Val Loss: 0.0205\n",
      "Epoch [134/200], Train Loss: 0.0197, Val Loss: 0.0204\n",
      "Epoch [135/200], Train Loss: 0.0197, Val Loss: 0.0204\n",
      "Epoch [136/200], Train Loss: 0.0197, Val Loss: 0.0205\n",
      "Epoch [137/200], Train Loss: 0.0196, Val Loss: 0.0205\n",
      "Epoch [138/200], Train Loss: 0.0196, Val Loss: 0.0205\n",
      "Epoch [139/200], Train Loss: 0.0196, Val Loss: 0.0205\n",
      "Epoch [140/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [141/200], Train Loss: 0.0196, Val Loss: 0.0205\n",
      "Epoch [142/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [143/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [144/200], Train Loss: 0.0196, Val Loss: 0.0205\n",
      "Epoch [145/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [146/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [147/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [148/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [149/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [150/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [151/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [152/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [153/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [154/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [155/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [156/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [157/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [158/200], Train Loss: 0.0195, Val Loss: 0.0203\n",
      "Epoch [159/200], Train Loss: 0.0195, Val Loss: 0.0203\n",
      "Epoch [160/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [161/200], Train Loss: 0.0195, Val Loss: 0.0204\n",
      "Epoch [162/200], Train Loss: 0.0194, Val Loss: 0.0203\n",
      "Epoch [163/200], Train Loss: 0.0194, Val Loss: 0.0204\n",
      "Epoch [164/200], Train Loss: 0.0194, Val Loss: 0.0204\n",
      "Epoch [165/200], Train Loss: 0.0194, Val Loss: 0.0204\n",
      "Epoch [166/200], Train Loss: 0.0194, Val Loss: 0.0204\n",
      "Epoch [167/200], Train Loss: 0.0194, Val Loss: 0.0204\n",
      "Epoch [168/200], Train Loss: 0.0194, Val Loss: 0.0204\n",
      "Epoch [169/200], Train Loss: 0.0194, Val Loss: 0.0203\n",
      "Epoch [170/200], Train Loss: 0.0194, Val Loss: 0.0203\n",
      "Epoch [171/200], Train Loss: 0.0194, Val Loss: 0.0204\n",
      "Epoch [172/200], Train Loss: 0.0194, Val Loss: 0.0203\n",
      "Epoch [173/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [174/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [175/200], Train Loss: 0.0194, Val Loss: 0.0203\n",
      "Epoch [176/200], Train Loss: 0.0193, Val Loss: 0.0204\n",
      "Epoch [177/200], Train Loss: 0.0194, Val Loss: 0.0203\n",
      "Epoch [178/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [179/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [180/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [181/200], Train Loss: 0.0193, Val Loss: 0.0204\n",
      "Epoch [182/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [183/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [184/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [185/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [186/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [187/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [188/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Epoch [189/200], Train Loss: 0.0193, Val Loss: 0.0203\n",
      "Early stopping at epoch 189\n",
      "Epoch [1/200], Train Loss: 0.0331, Val Loss: 0.0285\n",
      "Epoch [2/200], Train Loss: 0.0265, Val Loss: 0.0249\n",
      "Epoch [3/200], Train Loss: 0.0242, Val Loss: 0.0236\n",
      "Epoch [4/200], Train Loss: 0.0232, Val Loss: 0.0228\n",
      "Epoch [5/200], Train Loss: 0.0226, Val Loss: 0.0226\n",
      "Epoch [6/200], Train Loss: 0.0224, Val Loss: 0.0225\n",
      "Epoch [7/200], Train Loss: 0.0223, Val Loss: 0.0224\n",
      "Epoch [8/200], Train Loss: 0.0221, Val Loss: 0.0222\n",
      "Epoch [9/200], Train Loss: 0.0220, Val Loss: 0.0221\n",
      "Epoch [10/200], Train Loss: 0.0219, Val Loss: 0.0221\n",
      "Epoch [11/200], Train Loss: 0.0218, Val Loss: 0.0221\n",
      "Epoch [12/200], Train Loss: 0.0218, Val Loss: 0.0219\n",
      "Epoch [13/200], Train Loss: 0.0217, Val Loss: 0.0220\n",
      "Epoch [14/200], Train Loss: 0.0217, Val Loss: 0.0219\n",
      "Epoch [15/200], Train Loss: 0.0216, Val Loss: 0.0219\n",
      "Epoch [16/200], Train Loss: 0.0216, Val Loss: 0.0218\n",
      "Epoch [17/200], Train Loss: 0.0216, Val Loss: 0.0219\n",
      "Epoch [18/200], Train Loss: 0.0216, Val Loss: 0.0218\n",
      "Epoch [19/200], Train Loss: 0.0215, Val Loss: 0.0218\n",
      "Epoch [20/200], Train Loss: 0.0215, Val Loss: 0.0217\n",
      "Epoch [21/200], Train Loss: 0.0215, Val Loss: 0.0217\n",
      "Epoch [22/200], Train Loss: 0.0214, Val Loss: 0.0217\n",
      "Epoch [23/200], Train Loss: 0.0214, Val Loss: 0.0216\n",
      "Epoch [24/200], Train Loss: 0.0214, Val Loss: 0.0216\n",
      "Epoch [25/200], Train Loss: 0.0213, Val Loss: 0.0216\n",
      "Epoch [26/200], Train Loss: 0.0213, Val Loss: 0.0216\n",
      "Epoch [27/200], Train Loss: 0.0213, Val Loss: 0.0215\n",
      "Epoch [28/200], Train Loss: 0.0212, Val Loss: 0.0216\n",
      "Epoch [29/200], Train Loss: 0.0212, Val Loss: 0.0215\n",
      "Epoch [30/200], Train Loss: 0.0212, Val Loss: 0.0215\n",
      "Epoch [31/200], Train Loss: 0.0211, Val Loss: 0.0214\n",
      "Epoch [32/200], Train Loss: 0.0211, Val Loss: 0.0215\n",
      "Epoch [33/200], Train Loss: 0.0211, Val Loss: 0.0214\n",
      "Epoch [34/200], Train Loss: 0.0211, Val Loss: 0.0215\n",
      "Epoch [35/200], Train Loss: 0.0210, Val Loss: 0.0214\n",
      "Epoch [36/200], Train Loss: 0.0210, Val Loss: 0.0214\n",
      "Epoch [37/200], Train Loss: 0.0210, Val Loss: 0.0214\n",
      "Epoch [38/200], Train Loss: 0.0209, Val Loss: 0.0213\n",
      "Epoch [39/200], Train Loss: 0.0209, Val Loss: 0.0213\n",
      "Epoch [40/200], Train Loss: 0.0209, Val Loss: 0.0213\n",
      "Epoch [41/200], Train Loss: 0.0209, Val Loss: 0.0212\n",
      "Epoch [42/200], Train Loss: 0.0208, Val Loss: 0.0213\n",
      "Epoch [43/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [44/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [45/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [46/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [47/200], Train Loss: 0.0208, Val Loss: 0.0212\n",
      "Epoch [48/200], Train Loss: 0.0207, Val Loss: 0.0213\n",
      "Epoch [49/200], Train Loss: 0.0207, Val Loss: 0.0211\n",
      "Epoch [50/200], Train Loss: 0.0207, Val Loss: 0.0211\n",
      "Epoch [51/200], Train Loss: 0.0207, Val Loss: 0.0211\n",
      "Epoch [52/200], Train Loss: 0.0207, Val Loss: 0.0211\n",
      "Epoch [53/200], Train Loss: 0.0207, Val Loss: 0.0212\n",
      "Epoch [54/200], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [55/200], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [56/200], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [57/200], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [58/200], Train Loss: 0.0206, Val Loss: 0.0211\n",
      "Epoch [59/200], Train Loss: 0.0206, Val Loss: 0.0210\n",
      "Epoch [60/200], Train Loss: 0.0205, Val Loss: 0.0210\n",
      "Epoch [61/200], Train Loss: 0.0205, Val Loss: 0.0210\n",
      "Epoch [62/200], Train Loss: 0.0205, Val Loss: 0.0210\n",
      "Epoch [63/200], Train Loss: 0.0205, Val Loss: 0.0210\n",
      "Epoch [64/200], Train Loss: 0.0205, Val Loss: 0.0209\n",
      "Epoch [65/200], Train Loss: 0.0205, Val Loss: 0.0210\n",
      "Epoch [66/200], Train Loss: 0.0204, Val Loss: 0.0210\n",
      "Epoch [67/200], Train Loss: 0.0204, Val Loss: 0.0209\n",
      "Epoch [68/200], Train Loss: 0.0204, Val Loss: 0.0209\n",
      "Epoch [69/200], Train Loss: 0.0204, Val Loss: 0.0209\n",
      "Epoch [70/200], Train Loss: 0.0204, Val Loss: 0.0209\n",
      "Epoch [71/200], Train Loss: 0.0204, Val Loss: 0.0209\n",
      "Epoch [72/200], Train Loss: 0.0203, Val Loss: 0.0208\n",
      "Epoch [73/200], Train Loss: 0.0203, Val Loss: 0.0209\n",
      "Epoch [74/200], Train Loss: 0.0203, Val Loss: 0.0208\n",
      "Epoch [75/200], Train Loss: 0.0203, Val Loss: 0.0207\n",
      "Epoch [76/200], Train Loss: 0.0203, Val Loss: 0.0209\n",
      "Epoch [77/200], Train Loss: 0.0203, Val Loss: 0.0208\n",
      "Epoch [78/200], Train Loss: 0.0202, Val Loss: 0.0207\n",
      "Epoch [79/200], Train Loss: 0.0202, Val Loss: 0.0207\n",
      "Epoch [80/200], Train Loss: 0.0202, Val Loss: 0.0207\n",
      "Epoch [81/200], Train Loss: 0.0202, Val Loss: 0.0207\n",
      "Epoch [82/200], Train Loss: 0.0202, Val Loss: 0.0207\n",
      "Epoch [83/200], Train Loss: 0.0201, Val Loss: 0.0207\n",
      "Epoch [84/200], Train Loss: 0.0201, Val Loss: 0.0207\n",
      "Epoch [85/200], Train Loss: 0.0201, Val Loss: 0.0206\n",
      "Epoch [86/200], Train Loss: 0.0201, Val Loss: 0.0207\n",
      "Epoch [87/200], Train Loss: 0.0201, Val Loss: 0.0206\n",
      "Epoch [88/200], Train Loss: 0.0200, Val Loss: 0.0206\n",
      "Epoch [89/200], Train Loss: 0.0200, Val Loss: 0.0207\n",
      "Epoch [90/200], Train Loss: 0.0200, Val Loss: 0.0206\n",
      "Epoch [91/200], Train Loss: 0.0200, Val Loss: 0.0206\n",
      "Epoch [92/200], Train Loss: 0.0200, Val Loss: 0.0206\n",
      "Epoch [93/200], Train Loss: 0.0199, Val Loss: 0.0206\n",
      "Epoch [94/200], Train Loss: 0.0199, Val Loss: 0.0206\n",
      "Epoch [95/200], Train Loss: 0.0199, Val Loss: 0.0205\n",
      "Epoch [96/200], Train Loss: 0.0198, Val Loss: 0.0206\n",
      "Epoch [97/200], Train Loss: 0.0198, Val Loss: 0.0204\n",
      "Epoch [98/200], Train Loss: 0.0198, Val Loss: 0.0205\n",
      "Epoch [99/200], Train Loss: 0.0198, Val Loss: 0.0204\n",
      "Epoch [100/200], Train Loss: 0.0198, Val Loss: 0.0205\n",
      "Epoch [101/200], Train Loss: 0.0197, Val Loss: 0.0204\n",
      "Epoch [102/200], Train Loss: 0.0197, Val Loss: 0.0204\n",
      "Epoch [103/200], Train Loss: 0.0197, Val Loss: 0.0204\n",
      "Epoch [104/200], Train Loss: 0.0197, Val Loss: 0.0204\n",
      "Epoch [105/200], Train Loss: 0.0197, Val Loss: 0.0204\n",
      "Epoch [106/200], Train Loss: 0.0197, Val Loss: 0.0204\n",
      "Epoch [107/200], Train Loss: 0.0197, Val Loss: 0.0203\n",
      "Epoch [108/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [109/200], Train Loss: 0.0196, Val Loss: 0.0203\n",
      "Epoch [110/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [111/200], Train Loss: 0.0196, Val Loss: 0.0202\n",
      "Epoch [112/200], Train Loss: 0.0196, Val Loss: 0.0203\n",
      "Epoch [113/200], Train Loss: 0.0196, Val Loss: 0.0203\n",
      "Epoch [114/200], Train Loss: 0.0196, Val Loss: 0.0203\n",
      "Epoch [115/200], Train Loss: 0.0196, Val Loss: 0.0204\n",
      "Epoch [116/200], Train Loss: 0.0196, Val Loss: 0.0203\n",
      "Epoch [117/200], Train Loss: 0.0195, Val Loss: 0.0203\n",
      "Epoch [118/200], Train Loss: 0.0195, Val Loss: 0.0203\n",
      "Epoch [119/200], Train Loss: 0.0195, Val Loss: 0.0202\n",
      "Epoch [120/200], Train Loss: 0.0195, Val Loss: 0.0203\n",
      "Epoch [121/200], Train Loss: 0.0195, Val Loss: 0.0203\n",
      "Early stopping at epoch 121\n",
      "Epoch [1/200], Train Loss: 0.0329, Val Loss: 0.0285\n",
      "Epoch [2/200], Train Loss: 0.0268, Val Loss: 0.0258\n",
      "Epoch [3/200], Train Loss: 0.0249, Val Loss: 0.0243\n",
      "Epoch [4/200], Train Loss: 0.0238, Val Loss: 0.0238\n",
      "Epoch [5/200], Train Loss: 0.0232, Val Loss: 0.0230\n",
      "Epoch [6/200], Train Loss: 0.0225, Val Loss: 0.0225\n",
      "Epoch [7/200], Train Loss: 0.0221, Val Loss: 0.0223\n",
      "Epoch [8/200], Train Loss: 0.0219, Val Loss: 0.0220\n",
      "Epoch [9/200], Train Loss: 0.0216, Val Loss: 0.0218\n",
      "Epoch [10/200], Train Loss: 0.0213, Val Loss: 0.0215\n",
      "Epoch [11/200], Train Loss: 0.0211, Val Loss: 0.0214\n",
      "Epoch [12/200], Train Loss: 0.0210, Val Loss: 0.0213\n",
      "Epoch [13/200], Train Loss: 0.0209, Val Loss: 0.0212\n",
      "Epoch [14/200], Train Loss: 0.0209, Val Loss: 0.0211\n",
      "Epoch [15/200], Train Loss: 0.0208, Val Loss: 0.0211\n",
      "Epoch [16/200], Train Loss: 0.0207, Val Loss: 0.0211\n",
      "Epoch [17/200], Train Loss: 0.0207, Val Loss: 0.0211\n",
      "Epoch [18/200], Train Loss: 0.0206, Val Loss: 0.0210\n",
      "Epoch [19/200], Train Loss: 0.0206, Val Loss: 0.0209\n",
      "Epoch [20/200], Train Loss: 0.0205, Val Loss: 0.0209\n",
      "Epoch [21/200], Train Loss: 0.0205, Val Loss: 0.0208\n",
      "Epoch [22/200], Train Loss: 0.0204, Val Loss: 0.0208\n",
      "Epoch [23/200], Train Loss: 0.0204, Val Loss: 0.0208\n",
      "Epoch [24/200], Train Loss: 0.0203, Val Loss: 0.0207\n",
      "Epoch [25/200], Train Loss: 0.0202, Val Loss: 0.0207\n",
      "Epoch [26/200], Train Loss: 0.0202, Val Loss: 0.0206\n",
      "Epoch [27/200], Train Loss: 0.0201, Val Loss: 0.0206\n",
      "Epoch [28/200], Train Loss: 0.0201, Val Loss: 0.0206\n",
      "Epoch [29/200], Train Loss: 0.0200, Val Loss: 0.0205\n",
      "Epoch [30/200], Train Loss: 0.0200, Val Loss: 0.0205\n",
      "Epoch [31/200], Train Loss: 0.0199, Val Loss: 0.0204\n",
      "Epoch [32/200], Train Loss: 0.0199, Val Loss: 0.0203\n",
      "Epoch [33/200], Train Loss: 0.0199, Val Loss: 0.0203\n",
      "Epoch [34/200], Train Loss: 0.0198, Val Loss: 0.0204\n",
      "Epoch [35/200], Train Loss: 0.0198, Val Loss: 0.0202\n",
      "Epoch [36/200], Train Loss: 0.0198, Val Loss: 0.0202\n",
      "Epoch [37/200], Train Loss: 0.0197, Val Loss: 0.0202\n",
      "Epoch [38/200], Train Loss: 0.0197, Val Loss: 0.0202\n",
      "Epoch [39/200], Train Loss: 0.0197, Val Loss: 0.0201\n",
      "Epoch [40/200], Train Loss: 0.0196, Val Loss: 0.0201\n",
      "Epoch [41/200], Train Loss: 0.0196, Val Loss: 0.0202\n",
      "Epoch [42/200], Train Loss: 0.0196, Val Loss: 0.0200\n",
      "Epoch [43/200], Train Loss: 0.0195, Val Loss: 0.0200\n",
      "Epoch [44/200], Train Loss: 0.0195, Val Loss: 0.0200\n",
      "Epoch [45/200], Train Loss: 0.0195, Val Loss: 0.0199\n",
      "Epoch [46/200], Train Loss: 0.0195, Val Loss: 0.0200\n",
      "Epoch [47/200], Train Loss: 0.0194, Val Loss: 0.0199\n",
      "Epoch [48/200], Train Loss: 0.0194, Val Loss: 0.0199\n",
      "Epoch [49/200], Train Loss: 0.0194, Val Loss: 0.0199\n",
      "Epoch [50/200], Train Loss: 0.0194, Val Loss: 0.0198\n",
      "Epoch [51/200], Train Loss: 0.0194, Val Loss: 0.0199\n",
      "Epoch [52/200], Train Loss: 0.0193, Val Loss: 0.0198\n",
      "Epoch [53/200], Train Loss: 0.0193, Val Loss: 0.0198\n",
      "Epoch [54/200], Train Loss: 0.0193, Val Loss: 0.0198\n",
      "Epoch [55/200], Train Loss: 0.0193, Val Loss: 0.0197\n",
      "Epoch [56/200], Train Loss: 0.0192, Val Loss: 0.0198\n",
      "Epoch [57/200], Train Loss: 0.0192, Val Loss: 0.0197\n",
      "Epoch [58/200], Train Loss: 0.0192, Val Loss: 0.0197\n",
      "Epoch [59/200], Train Loss: 0.0192, Val Loss: 0.0197\n",
      "Epoch [60/200], Train Loss: 0.0192, Val Loss: 0.0197\n",
      "Epoch [61/200], Train Loss: 0.0192, Val Loss: 0.0197\n",
      "Epoch [62/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [63/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [64/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [65/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [66/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [67/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [68/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [69/200], Train Loss: 0.0190, Val Loss: 0.0196\n",
      "Epoch [70/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [71/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [72/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [73/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [74/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [75/200], Train Loss: 0.0190, Val Loss: 0.0196\n",
      "Epoch [76/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [77/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [78/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [79/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [80/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [81/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [82/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [83/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [84/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [85/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [86/200], Train Loss: 0.0188, Val Loss: 0.0195\n",
      "Epoch [87/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [88/200], Train Loss: 0.0188, Val Loss: 0.0193\n",
      "Epoch [89/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [90/200], Train Loss: 0.0188, Val Loss: 0.0195\n",
      "Epoch [91/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [92/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [93/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [94/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [95/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [96/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [97/200], Train Loss: 0.0188, Val Loss: 0.0193\n",
      "Epoch [98/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [99/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [100/200], Train Loss: 0.0187, Val Loss: 0.0194\n",
      "Epoch [101/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [102/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [103/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [104/200], Train Loss: 0.0187, Val Loss: 0.0194\n",
      "Epoch [105/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [106/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [107/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [108/200], Train Loss: 0.0186, Val Loss: 0.0193\n",
      "Epoch [109/200], Train Loss: 0.0186, Val Loss: 0.0193\n",
      "Epoch [110/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [111/200], Train Loss: 0.0186, Val Loss: 0.0193\n",
      "Epoch [112/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [113/200], Train Loss: 0.0186, Val Loss: 0.0193\n",
      "Epoch [114/200], Train Loss: 0.0186, Val Loss: 0.0193\n",
      "Epoch [115/200], Train Loss: 0.0186, Val Loss: 0.0193\n",
      "Epoch [116/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [117/200], Train Loss: 0.0186, Val Loss: 0.0193\n",
      "Epoch [118/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [119/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [120/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [121/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [122/200], Train Loss: 0.0185, Val Loss: 0.0193\n",
      "Epoch [123/200], Train Loss: 0.0185, Val Loss: 0.0193\n",
      "Epoch [124/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [125/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [126/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [127/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [128/200], Train Loss: 0.0185, Val Loss: 0.0191\n",
      "Epoch [129/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [130/200], Train Loss: 0.0185, Val Loss: 0.0191\n",
      "Epoch [131/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [132/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [133/200], Train Loss: 0.0184, Val Loss: 0.0192\n",
      "Epoch [134/200], Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Epoch [135/200], Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Epoch [136/200], Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Epoch [137/200], Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Epoch [138/200], Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Epoch [139/200], Train Loss: 0.0184, Val Loss: 0.0192\n",
      "Epoch [140/200], Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Epoch [141/200], Train Loss: 0.0184, Val Loss: 0.0192\n",
      "Epoch [142/200], Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Epoch [143/200], Train Loss: 0.0184, Val Loss: 0.0191\n",
      "Epoch [144/200], Train Loss: 0.0184, Val Loss: 0.0190\n",
      "Epoch [145/200], Train Loss: 0.0183, Val Loss: 0.0191\n",
      "Epoch [146/200], Train Loss: 0.0183, Val Loss: 0.0191\n",
      "Epoch [147/200], Train Loss: 0.0183, Val Loss: 0.0191\n",
      "Epoch [148/200], Train Loss: 0.0183, Val Loss: 0.0191\n",
      "Epoch [149/200], Train Loss: 0.0183, Val Loss: 0.0191\n",
      "Epoch [150/200], Train Loss: 0.0183, Val Loss: 0.0190\n",
      "Epoch [151/200], Train Loss: 0.0183, Val Loss: 0.0191\n",
      "Epoch [152/200], Train Loss: 0.0183, Val Loss: 0.0190\n",
      "Epoch [153/200], Train Loss: 0.0183, Val Loss: 0.0191\n",
      "Epoch [154/200], Train Loss: 0.0183, Val Loss: 0.0190\n",
      "Epoch [155/200], Train Loss: 0.0183, Val Loss: 0.0190\n",
      "Epoch [156/200], Train Loss: 0.0183, Val Loss: 0.0190\n",
      "Epoch [157/200], Train Loss: 0.0183, Val Loss: 0.0190\n",
      "Epoch [158/200], Train Loss: 0.0183, Val Loss: 0.0190\n",
      "Epoch [159/200], Train Loss: 0.0182, Val Loss: 0.0190\n",
      "Epoch [160/200], Train Loss: 0.0182, Val Loss: 0.0190\n",
      "Epoch [161/200], Train Loss: 0.0182, Val Loss: 0.0190\n",
      "Epoch [162/200], Train Loss: 0.0182, Val Loss: 0.0190\n",
      "Epoch [163/200], Train Loss: 0.0182, Val Loss: 0.0190\n",
      "Epoch [164/200], Train Loss: 0.0182, Val Loss: 0.0190\n",
      "Early stopping at epoch 164\n",
      "Epoch [1/200], Train Loss: 0.0334, Val Loss: 0.0283\n",
      "Epoch [2/200], Train Loss: 0.0263, Val Loss: 0.0249\n",
      "Epoch [3/200], Train Loss: 0.0241, Val Loss: 0.0234\n",
      "Epoch [4/200], Train Loss: 0.0229, Val Loss: 0.0226\n",
      "Epoch [5/200], Train Loss: 0.0221, Val Loss: 0.0220\n",
      "Epoch [6/200], Train Loss: 0.0215, Val Loss: 0.0216\n",
      "Epoch [7/200], Train Loss: 0.0212, Val Loss: 0.0213\n",
      "Epoch [8/200], Train Loss: 0.0210, Val Loss: 0.0211\n",
      "Epoch [9/200], Train Loss: 0.0209, Val Loss: 0.0211\n",
      "Epoch [10/200], Train Loss: 0.0208, Val Loss: 0.0210\n",
      "Epoch [11/200], Train Loss: 0.0207, Val Loss: 0.0210\n",
      "Epoch [12/200], Train Loss: 0.0206, Val Loss: 0.0210\n",
      "Epoch [13/200], Train Loss: 0.0206, Val Loss: 0.0209\n",
      "Epoch [14/200], Train Loss: 0.0205, Val Loss: 0.0208\n",
      "Epoch [15/200], Train Loss: 0.0205, Val Loss: 0.0208\n",
      "Epoch [16/200], Train Loss: 0.0204, Val Loss: 0.0207\n",
      "Epoch [17/200], Train Loss: 0.0203, Val Loss: 0.0206\n",
      "Epoch [18/200], Train Loss: 0.0203, Val Loss: 0.0206\n",
      "Epoch [19/200], Train Loss: 0.0203, Val Loss: 0.0206\n",
      "Epoch [20/200], Train Loss: 0.0202, Val Loss: 0.0205\n",
      "Epoch [21/200], Train Loss: 0.0201, Val Loss: 0.0204\n",
      "Epoch [22/200], Train Loss: 0.0201, Val Loss: 0.0205\n",
      "Epoch [23/200], Train Loss: 0.0201, Val Loss: 0.0203\n",
      "Epoch [24/200], Train Loss: 0.0200, Val Loss: 0.0204\n",
      "Epoch [25/200], Train Loss: 0.0200, Val Loss: 0.0203\n",
      "Epoch [26/200], Train Loss: 0.0200, Val Loss: 0.0203\n",
      "Epoch [27/200], Train Loss: 0.0199, Val Loss: 0.0202\n",
      "Epoch [28/200], Train Loss: 0.0199, Val Loss: 0.0202\n",
      "Epoch [29/200], Train Loss: 0.0198, Val Loss: 0.0201\n",
      "Epoch [30/200], Train Loss: 0.0198, Val Loss: 0.0200\n",
      "Epoch [31/200], Train Loss: 0.0198, Val Loss: 0.0200\n",
      "Epoch [32/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [33/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [34/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [35/200], Train Loss: 0.0196, Val Loss: 0.0200\n",
      "Epoch [36/200], Train Loss: 0.0196, Val Loss: 0.0198\n",
      "Epoch [37/200], Train Loss: 0.0196, Val Loss: 0.0199\n",
      "Epoch [38/200], Train Loss: 0.0195, Val Loss: 0.0199\n",
      "Epoch [39/200], Train Loss: 0.0195, Val Loss: 0.0198\n",
      "Epoch [40/200], Train Loss: 0.0195, Val Loss: 0.0198\n",
      "Epoch [41/200], Train Loss: 0.0195, Val Loss: 0.0199\n",
      "Epoch [42/200], Train Loss: 0.0194, Val Loss: 0.0198\n",
      "Epoch [43/200], Train Loss: 0.0194, Val Loss: 0.0198\n",
      "Epoch [44/200], Train Loss: 0.0194, Val Loss: 0.0197\n",
      "Epoch [45/200], Train Loss: 0.0194, Val Loss: 0.0198\n",
      "Epoch [46/200], Train Loss: 0.0193, Val Loss: 0.0197\n",
      "Epoch [47/200], Train Loss: 0.0193, Val Loss: 0.0197\n",
      "Epoch [48/200], Train Loss: 0.0193, Val Loss: 0.0196\n",
      "Epoch [49/200], Train Loss: 0.0193, Val Loss: 0.0197\n",
      "Epoch [50/200], Train Loss: 0.0193, Val Loss: 0.0196\n",
      "Epoch [51/200], Train Loss: 0.0193, Val Loss: 0.0196\n",
      "Epoch [52/200], Train Loss: 0.0192, Val Loss: 0.0196\n",
      "Epoch [53/200], Train Loss: 0.0192, Val Loss: 0.0196\n",
      "Epoch [54/200], Train Loss: 0.0192, Val Loss: 0.0196\n",
      "Epoch [55/200], Train Loss: 0.0192, Val Loss: 0.0195\n",
      "Epoch [56/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [57/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [58/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [59/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [60/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [61/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [62/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [63/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [64/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [65/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [66/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [67/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [68/200], Train Loss: 0.0190, Val Loss: 0.0193\n",
      "Epoch [69/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [70/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [71/200], Train Loss: 0.0189, Val Loss: 0.0193\n",
      "Epoch [72/200], Train Loss: 0.0189, Val Loss: 0.0195\n",
      "Epoch [73/200], Train Loss: 0.0189, Val Loss: 0.0193\n",
      "Epoch [74/200], Train Loss: 0.0188, Val Loss: 0.0193\n",
      "Epoch [75/200], Train Loss: 0.0188, Val Loss: 0.0194\n",
      "Epoch [76/200], Train Loss: 0.0188, Val Loss: 0.0193\n",
      "Epoch [77/200], Train Loss: 0.0188, Val Loss: 0.0193\n",
      "Epoch [78/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [79/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [80/200], Train Loss: 0.0187, Val Loss: 0.0192\n",
      "Epoch [81/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [82/200], Train Loss: 0.0187, Val Loss: 0.0191\n",
      "Epoch [83/200], Train Loss: 0.0187, Val Loss: 0.0193\n",
      "Epoch [84/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [85/200], Train Loss: 0.0187, Val Loss: 0.0192\n",
      "Epoch [86/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [87/200], Train Loss: 0.0186, Val Loss: 0.0191\n",
      "Epoch [88/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [89/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [90/200], Train Loss: 0.0186, Val Loss: 0.0191\n",
      "Epoch [91/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [92/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [93/200], Train Loss: 0.0185, Val Loss: 0.0191\n",
      "Epoch [94/200], Train Loss: 0.0185, Val Loss: 0.0192\n",
      "Epoch [95/200], Train Loss: 0.0185, Val Loss: 0.0191\n",
      "Epoch [96/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [97/200], Train Loss: 0.0185, Val Loss: 0.0191\n",
      "Epoch [98/200], Train Loss: 0.0185, Val Loss: 0.0191\n",
      "Epoch [99/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [100/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [101/200], Train Loss: 0.0184, Val Loss: 0.0190\n",
      "Epoch [102/200], Train Loss: 0.0184, Val Loss: 0.0190\n",
      "Epoch [103/200], Train Loss: 0.0184, Val Loss: 0.0190\n",
      "Epoch [104/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [105/200], Train Loss: 0.0184, Val Loss: 0.0190\n",
      "Epoch [106/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [107/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [108/200], Train Loss: 0.0184, Val Loss: 0.0190\n",
      "Epoch [109/200], Train Loss: 0.0184, Val Loss: 0.0190\n",
      "Epoch [110/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [111/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [112/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [113/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [114/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [115/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [116/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [117/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [118/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [119/200], Train Loss: 0.0183, Val Loss: 0.0188\n",
      "Epoch [120/200], Train Loss: 0.0182, Val Loss: 0.0189\n",
      "Epoch [121/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [122/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [123/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [124/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [125/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [126/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [127/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [128/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [129/200], Train Loss: 0.0182, Val Loss: 0.0187\n",
      "Epoch [130/200], Train Loss: 0.0182, Val Loss: 0.0187\n",
      "Epoch [131/200], Train Loss: 0.0181, Val Loss: 0.0188\n",
      "Epoch [132/200], Train Loss: 0.0181, Val Loss: 0.0188\n",
      "Epoch [133/200], Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Epoch [134/200], Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Epoch [135/200], Train Loss: 0.0181, Val Loss: 0.0188\n",
      "Epoch [136/200], Train Loss: 0.0181, Val Loss: 0.0188\n",
      "Epoch [137/200], Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Epoch [138/200], Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Epoch [139/200], Train Loss: 0.0181, Val Loss: 0.0188\n",
      "Epoch [140/200], Train Loss: 0.0181, Val Loss: 0.0188\n",
      "Epoch [141/200], Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Epoch [142/200], Train Loss: 0.0181, Val Loss: 0.0188\n",
      "Epoch [143/200], Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Epoch [144/200], Train Loss: 0.0180, Val Loss: 0.0188\n",
      "Epoch [145/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [146/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [147/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [148/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [149/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [150/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [151/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [152/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [153/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [154/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [155/200], Train Loss: 0.0180, Val Loss: 0.0186\n",
      "Epoch [156/200], Train Loss: 0.0179, Val Loss: 0.0187\n",
      "Epoch [157/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [158/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [159/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [160/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [161/200], Train Loss: 0.0179, Val Loss: 0.0187\n",
      "Epoch [162/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [163/200], Train Loss: 0.0179, Val Loss: 0.0187\n",
      "Epoch [164/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [165/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [166/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [167/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [168/200], Train Loss: 0.0178, Val Loss: 0.0186\n",
      "Epoch [169/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Early stopping at epoch 169\n",
      "Epoch [1/200], Train Loss: 0.0330, Val Loss: 0.0283\n",
      "Epoch [2/200], Train Loss: 0.0264, Val Loss: 0.0249\n",
      "Epoch [3/200], Train Loss: 0.0240, Val Loss: 0.0234\n",
      "Epoch [4/200], Train Loss: 0.0227, Val Loss: 0.0225\n",
      "Epoch [5/200], Train Loss: 0.0220, Val Loss: 0.0220\n",
      "Epoch [6/200], Train Loss: 0.0215, Val Loss: 0.0216\n",
      "Epoch [7/200], Train Loss: 0.0212, Val Loss: 0.0213\n",
      "Epoch [8/200], Train Loss: 0.0209, Val Loss: 0.0210\n",
      "Epoch [9/200], Train Loss: 0.0207, Val Loss: 0.0209\n",
      "Epoch [10/200], Train Loss: 0.0206, Val Loss: 0.0207\n",
      "Epoch [11/200], Train Loss: 0.0204, Val Loss: 0.0208\n",
      "Epoch [12/200], Train Loss: 0.0204, Val Loss: 0.0206\n",
      "Epoch [13/200], Train Loss: 0.0203, Val Loss: 0.0206\n",
      "Epoch [14/200], Train Loss: 0.0203, Val Loss: 0.0206\n",
      "Epoch [15/200], Train Loss: 0.0203, Val Loss: 0.0205\n",
      "Epoch [16/200], Train Loss: 0.0202, Val Loss: 0.0205\n",
      "Epoch [17/200], Train Loss: 0.0202, Val Loss: 0.0205\n",
      "Epoch [18/200], Train Loss: 0.0202, Val Loss: 0.0205\n",
      "Epoch [19/200], Train Loss: 0.0201, Val Loss: 0.0205\n",
      "Epoch [20/200], Train Loss: 0.0201, Val Loss: 0.0204\n",
      "Epoch [21/200], Train Loss: 0.0201, Val Loss: 0.0204\n",
      "Epoch [22/200], Train Loss: 0.0200, Val Loss: 0.0203\n",
      "Epoch [23/200], Train Loss: 0.0199, Val Loss: 0.0203\n",
      "Epoch [24/200], Train Loss: 0.0199, Val Loss: 0.0203\n",
      "Epoch [25/200], Train Loss: 0.0198, Val Loss: 0.0201\n",
      "Epoch [26/200], Train Loss: 0.0198, Val Loss: 0.0201\n",
      "Epoch [27/200], Train Loss: 0.0198, Val Loss: 0.0201\n",
      "Epoch [28/200], Train Loss: 0.0197, Val Loss: 0.0201\n",
      "Epoch [29/200], Train Loss: 0.0197, Val Loss: 0.0201\n",
      "Epoch [30/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [31/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [32/200], Train Loss: 0.0196, Val Loss: 0.0200\n",
      "Epoch [33/200], Train Loss: 0.0196, Val Loss: 0.0200\n",
      "Epoch [34/200], Train Loss: 0.0196, Val Loss: 0.0200\n",
      "Epoch [35/200], Train Loss: 0.0196, Val Loss: 0.0199\n",
      "Epoch [36/200], Train Loss: 0.0195, Val Loss: 0.0200\n",
      "Epoch [37/200], Train Loss: 0.0195, Val Loss: 0.0199\n",
      "Epoch [38/200], Train Loss: 0.0195, Val Loss: 0.0199\n",
      "Epoch [39/200], Train Loss: 0.0195, Val Loss: 0.0199\n",
      "Epoch [40/200], Train Loss: 0.0195, Val Loss: 0.0198\n",
      "Epoch [41/200], Train Loss: 0.0194, Val Loss: 0.0199\n",
      "Epoch [42/200], Train Loss: 0.0194, Val Loss: 0.0198\n",
      "Epoch [43/200], Train Loss: 0.0194, Val Loss: 0.0197\n",
      "Epoch [44/200], Train Loss: 0.0194, Val Loss: 0.0197\n",
      "Epoch [45/200], Train Loss: 0.0194, Val Loss: 0.0198\n",
      "Epoch [46/200], Train Loss: 0.0193, Val Loss: 0.0197\n",
      "Epoch [47/200], Train Loss: 0.0193, Val Loss: 0.0197\n",
      "Epoch [48/200], Train Loss: 0.0193, Val Loss: 0.0197\n",
      "Epoch [49/200], Train Loss: 0.0193, Val Loss: 0.0196\n",
      "Epoch [50/200], Train Loss: 0.0192, Val Loss: 0.0196\n",
      "Epoch [51/200], Train Loss: 0.0192, Val Loss: 0.0197\n",
      "Epoch [52/200], Train Loss: 0.0192, Val Loss: 0.0196\n",
      "Epoch [53/200], Train Loss: 0.0192, Val Loss: 0.0196\n",
      "Epoch [54/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [55/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [56/200], Train Loss: 0.0191, Val Loss: 0.0196\n",
      "Epoch [57/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [58/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [59/200], Train Loss: 0.0190, Val Loss: 0.0195\n",
      "Epoch [60/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [61/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [62/200], Train Loss: 0.0189, Val Loss: 0.0194\n",
      "Epoch [63/200], Train Loss: 0.0189, Val Loss: 0.0193\n",
      "Epoch [64/200], Train Loss: 0.0189, Val Loss: 0.0193\n",
      "Epoch [65/200], Train Loss: 0.0188, Val Loss: 0.0193\n",
      "Epoch [66/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [67/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [68/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [69/200], Train Loss: 0.0188, Val Loss: 0.0193\n",
      "Epoch [70/200], Train Loss: 0.0187, Val Loss: 0.0192\n",
      "Epoch [71/200], Train Loss: 0.0187, Val Loss: 0.0192\n",
      "Epoch [72/200], Train Loss: 0.0187, Val Loss: 0.0192\n",
      "Epoch [73/200], Train Loss: 0.0186, Val Loss: 0.0191\n",
      "Epoch [74/200], Train Loss: 0.0186, Val Loss: 0.0191\n",
      "Epoch [75/200], Train Loss: 0.0186, Val Loss: 0.0191\n",
      "Epoch [76/200], Train Loss: 0.0186, Val Loss: 0.0191\n",
      "Epoch [77/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [78/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [79/200], Train Loss: 0.0185, Val Loss: 0.0191\n",
      "Epoch [80/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [81/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [82/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [83/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [84/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [85/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [86/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [87/200], Train Loss: 0.0183, Val Loss: 0.0188\n",
      "Epoch [88/200], Train Loss: 0.0183, Val Loss: 0.0188\n",
      "Epoch [89/200], Train Loss: 0.0183, Val Loss: 0.0188\n",
      "Epoch [90/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [91/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [92/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [93/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [94/200], Train Loss: 0.0182, Val Loss: 0.0187\n",
      "Epoch [95/200], Train Loss: 0.0182, Val Loss: 0.0186\n",
      "Epoch [96/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [97/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [98/200], Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Epoch [99/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [100/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [101/200], Train Loss: 0.0180, Val Loss: 0.0187\n",
      "Epoch [102/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [103/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [104/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [105/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [106/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [107/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [108/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [109/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [110/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [111/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [112/200], Train Loss: 0.0179, Val Loss: 0.0185\n",
      "Epoch [113/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [114/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [115/200], Train Loss: 0.0178, Val Loss: 0.0184\n",
      "Epoch [116/200], Train Loss: 0.0178, Val Loss: 0.0184\n",
      "Epoch [117/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [118/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [119/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [120/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [121/200], Train Loss: 0.0177, Val Loss: 0.0183\n",
      "Epoch [122/200], Train Loss: 0.0177, Val Loss: 0.0183\n",
      "Epoch [123/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [124/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [125/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [126/200], Train Loss: 0.0176, Val Loss: 0.0182\n",
      "Epoch [127/200], Train Loss: 0.0176, Val Loss: 0.0182\n",
      "Epoch [128/200], Train Loss: 0.0176, Val Loss: 0.0182\n",
      "Epoch [129/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [130/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [131/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [132/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [133/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [134/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [135/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [136/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [137/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [138/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [139/200], Train Loss: 0.0174, Val Loss: 0.0181\n",
      "Epoch [140/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [141/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [142/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [143/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [144/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [145/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [146/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [147/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [148/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [149/200], Train Loss: 0.0173, Val Loss: 0.0180\n",
      "Epoch [150/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [151/200], Train Loss: 0.0173, Val Loss: 0.0180\n",
      "Epoch [152/200], Train Loss: 0.0173, Val Loss: 0.0178\n",
      "Epoch [153/200], Train Loss: 0.0173, Val Loss: 0.0180\n",
      "Epoch [154/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [155/200], Train Loss: 0.0173, Val Loss: 0.0178\n",
      "Epoch [156/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [157/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [158/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [159/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [160/200], Train Loss: 0.0172, Val Loss: 0.0179\n",
      "Epoch [161/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [162/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [163/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [164/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [165/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [166/200], Train Loss: 0.0172, Val Loss: 0.0179\n",
      "Epoch [167/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [168/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [169/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [170/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [171/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [172/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [173/200], Train Loss: 0.0172, Val Loss: 0.0177\n",
      "Epoch [174/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [175/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [176/200], Train Loss: 0.0172, Val Loss: 0.0177\n",
      "Epoch [177/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [178/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [179/200], Train Loss: 0.0171, Val Loss: 0.0179\n",
      "Epoch [180/200], Train Loss: 0.0171, Val Loss: 0.0179\n",
      "Epoch [181/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [182/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [183/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [184/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [185/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [186/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [187/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [188/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [189/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [190/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [191/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [192/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [193/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [194/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [195/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [196/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [197/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [198/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [199/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [200/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [1/200], Train Loss: 0.0323, Val Loss: 0.0275\n",
      "Epoch [2/200], Train Loss: 0.0263, Val Loss: 0.0254\n",
      "Epoch [3/200], Train Loss: 0.0245, Val Loss: 0.0239\n",
      "Epoch [4/200], Train Loss: 0.0230, Val Loss: 0.0225\n",
      "Epoch [5/200], Train Loss: 0.0219, Val Loss: 0.0219\n",
      "Epoch [6/200], Train Loss: 0.0215, Val Loss: 0.0215\n",
      "Epoch [7/200], Train Loss: 0.0211, Val Loss: 0.0211\n",
      "Epoch [8/200], Train Loss: 0.0209, Val Loss: 0.0210\n",
      "Epoch [9/200], Train Loss: 0.0207, Val Loss: 0.0207\n",
      "Epoch [10/200], Train Loss: 0.0204, Val Loss: 0.0205\n",
      "Epoch [11/200], Train Loss: 0.0203, Val Loss: 0.0205\n",
      "Epoch [12/200], Train Loss: 0.0202, Val Loss: 0.0203\n",
      "Epoch [13/200], Train Loss: 0.0201, Val Loss: 0.0203\n",
      "Epoch [14/200], Train Loss: 0.0200, Val Loss: 0.0202\n",
      "Epoch [15/200], Train Loss: 0.0200, Val Loss: 0.0202\n",
      "Epoch [16/200], Train Loss: 0.0200, Val Loss: 0.0202\n",
      "Epoch [17/200], Train Loss: 0.0199, Val Loss: 0.0202\n",
      "Epoch [18/200], Train Loss: 0.0199, Val Loss: 0.0201\n",
      "Epoch [19/200], Train Loss: 0.0199, Val Loss: 0.0201\n",
      "Epoch [20/200], Train Loss: 0.0198, Val Loss: 0.0201\n",
      "Epoch [21/200], Train Loss: 0.0198, Val Loss: 0.0200\n",
      "Epoch [22/200], Train Loss: 0.0198, Val Loss: 0.0201\n",
      "Epoch [23/200], Train Loss: 0.0198, Val Loss: 0.0200\n",
      "Epoch [24/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [25/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [26/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [27/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [28/200], Train Loss: 0.0196, Val Loss: 0.0198\n",
      "Epoch [29/200], Train Loss: 0.0196, Val Loss: 0.0198\n",
      "Epoch [30/200], Train Loss: 0.0195, Val Loss: 0.0198\n",
      "Epoch [31/200], Train Loss: 0.0195, Val Loss: 0.0197\n",
      "Epoch [32/200], Train Loss: 0.0194, Val Loss: 0.0197\n",
      "Epoch [33/200], Train Loss: 0.0194, Val Loss: 0.0197\n",
      "Epoch [34/200], Train Loss: 0.0194, Val Loss: 0.0196\n",
      "Epoch [35/200], Train Loss: 0.0193, Val Loss: 0.0196\n",
      "Epoch [36/200], Train Loss: 0.0193, Val Loss: 0.0195\n",
      "Epoch [37/200], Train Loss: 0.0193, Val Loss: 0.0195\n",
      "Epoch [38/200], Train Loss: 0.0192, Val Loss: 0.0195\n",
      "Epoch [39/200], Train Loss: 0.0192, Val Loss: 0.0195\n",
      "Epoch [40/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [41/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [42/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [43/200], Train Loss: 0.0191, Val Loss: 0.0195\n",
      "Epoch [44/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [45/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [46/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [47/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [48/200], Train Loss: 0.0190, Val Loss: 0.0193\n",
      "Epoch [49/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [50/200], Train Loss: 0.0189, Val Loss: 0.0193\n",
      "Epoch [51/200], Train Loss: 0.0189, Val Loss: 0.0193\n",
      "Epoch [52/200], Train Loss: 0.0189, Val Loss: 0.0193\n",
      "Epoch [53/200], Train Loss: 0.0189, Val Loss: 0.0193\n",
      "Epoch [54/200], Train Loss: 0.0189, Val Loss: 0.0192\n",
      "Epoch [55/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [56/200], Train Loss: 0.0188, Val Loss: 0.0193\n",
      "Epoch [57/200], Train Loss: 0.0188, Val Loss: 0.0193\n",
      "Epoch [58/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [59/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [60/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [61/200], Train Loss: 0.0187, Val Loss: 0.0191\n",
      "Epoch [62/200], Train Loss: 0.0187, Val Loss: 0.0191\n",
      "Epoch [63/200], Train Loss: 0.0187, Val Loss: 0.0191\n",
      "Epoch [64/200], Train Loss: 0.0187, Val Loss: 0.0191\n",
      "Epoch [65/200], Train Loss: 0.0187, Val Loss: 0.0191\n",
      "Epoch [66/200], Train Loss: 0.0186, Val Loss: 0.0190\n",
      "Epoch [67/200], Train Loss: 0.0186, Val Loss: 0.0191\n",
      "Epoch [68/200], Train Loss: 0.0186, Val Loss: 0.0190\n",
      "Epoch [69/200], Train Loss: 0.0186, Val Loss: 0.0190\n",
      "Epoch [70/200], Train Loss: 0.0186, Val Loss: 0.0190\n",
      "Epoch [71/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [72/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [73/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [74/200], Train Loss: 0.0185, Val Loss: 0.0189\n",
      "Epoch [75/200], Train Loss: 0.0185, Val Loss: 0.0189\n",
      "Epoch [76/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [77/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [78/200], Train Loss: 0.0184, Val Loss: 0.0188\n",
      "Epoch [79/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [80/200], Train Loss: 0.0184, Val Loss: 0.0188\n",
      "Epoch [81/200], Train Loss: 0.0183, Val Loss: 0.0189\n",
      "Epoch [82/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [83/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [84/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [85/200], Train Loss: 0.0183, Val Loss: 0.0188\n",
      "Epoch [86/200], Train Loss: 0.0182, Val Loss: 0.0188\n",
      "Epoch [87/200], Train Loss: 0.0182, Val Loss: 0.0187\n",
      "Epoch [88/200], Train Loss: 0.0182, Val Loss: 0.0187\n",
      "Epoch [89/200], Train Loss: 0.0182, Val Loss: 0.0186\n",
      "Epoch [90/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [91/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [92/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [93/200], Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Epoch [94/200], Train Loss: 0.0181, Val Loss: 0.0187\n",
      "Epoch [95/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [96/200], Train Loss: 0.0181, Val Loss: 0.0185\n",
      "Epoch [97/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [98/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [99/200], Train Loss: 0.0180, Val Loss: 0.0186\n",
      "Epoch [100/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [101/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [102/200], Train Loss: 0.0180, Val Loss: 0.0186\n",
      "Epoch [103/200], Train Loss: 0.0180, Val Loss: 0.0186\n",
      "Epoch [104/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [105/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [106/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [107/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [108/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [109/200], Train Loss: 0.0179, Val Loss: 0.0185\n",
      "Epoch [110/200], Train Loss: 0.0179, Val Loss: 0.0185\n",
      "Epoch [111/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [112/200], Train Loss: 0.0179, Val Loss: 0.0185\n",
      "Epoch [113/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [114/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [115/200], Train Loss: 0.0178, Val Loss: 0.0184\n",
      "Epoch [116/200], Train Loss: 0.0178, Val Loss: 0.0184\n",
      "Epoch [117/200], Train Loss: 0.0178, Val Loss: 0.0184\n",
      "Epoch [118/200], Train Loss: 0.0178, Val Loss: 0.0184\n",
      "Epoch [119/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [120/200], Train Loss: 0.0178, Val Loss: 0.0184\n",
      "Epoch [121/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [122/200], Train Loss: 0.0177, Val Loss: 0.0183\n",
      "Epoch [123/200], Train Loss: 0.0177, Val Loss: 0.0183\n",
      "Epoch [124/200], Train Loss: 0.0177, Val Loss: 0.0183\n",
      "Epoch [125/200], Train Loss: 0.0177, Val Loss: 0.0183\n",
      "Epoch [126/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [127/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [128/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [129/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [130/200], Train Loss: 0.0176, Val Loss: 0.0182\n",
      "Epoch [131/200], Train Loss: 0.0176, Val Loss: 0.0182\n",
      "Epoch [132/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [133/200], Train Loss: 0.0176, Val Loss: 0.0182\n",
      "Epoch [134/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [135/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [136/200], Train Loss: 0.0176, Val Loss: 0.0183\n",
      "Epoch [137/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [138/200], Train Loss: 0.0175, Val Loss: 0.0182\n",
      "Epoch [139/200], Train Loss: 0.0176, Val Loss: 0.0182\n",
      "Epoch [140/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [141/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [142/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [143/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [144/200], Train Loss: 0.0175, Val Loss: 0.0182\n",
      "Epoch [145/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [146/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [147/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [148/200], Train Loss: 0.0175, Val Loss: 0.0181\n",
      "Epoch [149/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [150/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [151/200], Train Loss: 0.0174, Val Loss: 0.0181\n",
      "Epoch [152/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [153/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [154/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [155/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [156/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [157/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [158/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [159/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [160/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [161/200], Train Loss: 0.0174, Val Loss: 0.0180\n",
      "Epoch [162/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [163/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [164/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [165/200], Train Loss: 0.0173, Val Loss: 0.0180\n",
      "Epoch [166/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [167/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [168/200], Train Loss: 0.0173, Val Loss: 0.0178\n",
      "Epoch [169/200], Train Loss: 0.0173, Val Loss: 0.0178\n",
      "Epoch [170/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [171/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [172/200], Train Loss: 0.0172, Val Loss: 0.0179\n",
      "Epoch [173/200], Train Loss: 0.0172, Val Loss: 0.0179\n",
      "Epoch [174/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [175/200], Train Loss: 0.0172, Val Loss: 0.0179\n",
      "Epoch [176/200], Train Loss: 0.0172, Val Loss: 0.0179\n",
      "Epoch [177/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [178/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [179/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [180/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [181/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [182/200], Train Loss: 0.0171, Val Loss: 0.0179\n",
      "Epoch [183/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [184/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [185/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [186/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [187/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [188/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [189/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [190/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [191/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [192/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [193/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [194/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [195/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [196/200], Train Loss: 0.0170, Val Loss: 0.0176\n",
      "Epoch [197/200], Train Loss: 0.0170, Val Loss: 0.0178\n",
      "Epoch [198/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [199/200], Train Loss: 0.0170, Val Loss: 0.0176\n",
      "Epoch [200/200], Train Loss: 0.0170, Val Loss: 0.0176\n",
      "Epoch [1/200], Train Loss: 0.0333, Val Loss: 0.0286\n",
      "Epoch [2/200], Train Loss: 0.0267, Val Loss: 0.0254\n",
      "Epoch [3/200], Train Loss: 0.0247, Val Loss: 0.0240\n",
      "Epoch [4/200], Train Loss: 0.0235, Val Loss: 0.0231\n",
      "Epoch [5/200], Train Loss: 0.0226, Val Loss: 0.0224\n",
      "Epoch [6/200], Train Loss: 0.0220, Val Loss: 0.0220\n",
      "Epoch [7/200], Train Loss: 0.0216, Val Loss: 0.0216\n",
      "Epoch [8/200], Train Loss: 0.0212, Val Loss: 0.0213\n",
      "Epoch [9/200], Train Loss: 0.0210, Val Loss: 0.0212\n",
      "Epoch [10/200], Train Loss: 0.0208, Val Loss: 0.0210\n",
      "Epoch [11/200], Train Loss: 0.0206, Val Loss: 0.0206\n",
      "Epoch [12/200], Train Loss: 0.0204, Val Loss: 0.0205\n",
      "Epoch [13/200], Train Loss: 0.0203, Val Loss: 0.0205\n",
      "Epoch [14/200], Train Loss: 0.0202, Val Loss: 0.0204\n",
      "Epoch [15/200], Train Loss: 0.0201, Val Loss: 0.0203\n",
      "Epoch [16/200], Train Loss: 0.0199, Val Loss: 0.0201\n",
      "Epoch [17/200], Train Loss: 0.0197, Val Loss: 0.0200\n",
      "Epoch [18/200], Train Loss: 0.0196, Val Loss: 0.0200\n",
      "Epoch [19/200], Train Loss: 0.0196, Val Loss: 0.0199\n",
      "Epoch [20/200], Train Loss: 0.0195, Val Loss: 0.0199\n",
      "Epoch [21/200], Train Loss: 0.0195, Val Loss: 0.0198\n",
      "Epoch [22/200], Train Loss: 0.0194, Val Loss: 0.0198\n",
      "Epoch [23/200], Train Loss: 0.0193, Val Loss: 0.0197\n",
      "Epoch [24/200], Train Loss: 0.0192, Val Loss: 0.0194\n",
      "Epoch [25/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [26/200], Train Loss: 0.0190, Val Loss: 0.0193\n",
      "Epoch [27/200], Train Loss: 0.0189, Val Loss: 0.0193\n",
      "Epoch [28/200], Train Loss: 0.0189, Val Loss: 0.0192\n",
      "Epoch [29/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [30/200], Train Loss: 0.0188, Val Loss: 0.0192\n",
      "Epoch [31/200], Train Loss: 0.0187, Val Loss: 0.0191\n",
      "Epoch [32/200], Train Loss: 0.0187, Val Loss: 0.0191\n",
      "Epoch [33/200], Train Loss: 0.0187, Val Loss: 0.0191\n",
      "Epoch [34/200], Train Loss: 0.0187, Val Loss: 0.0190\n",
      "Epoch [35/200], Train Loss: 0.0186, Val Loss: 0.0192\n",
      "Epoch [36/200], Train Loss: 0.0186, Val Loss: 0.0189\n",
      "Epoch [37/200], Train Loss: 0.0186, Val Loss: 0.0189\n",
      "Epoch [38/200], Train Loss: 0.0185, Val Loss: 0.0189\n",
      "Epoch [39/200], Train Loss: 0.0185, Val Loss: 0.0189\n",
      "Epoch [40/200], Train Loss: 0.0185, Val Loss: 0.0189\n",
      "Epoch [41/200], Train Loss: 0.0185, Val Loss: 0.0190\n",
      "Epoch [42/200], Train Loss: 0.0185, Val Loss: 0.0188\n",
      "Epoch [43/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [44/200], Train Loss: 0.0184, Val Loss: 0.0188\n",
      "Epoch [45/200], Train Loss: 0.0184, Val Loss: 0.0187\n",
      "Epoch [46/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [47/200], Train Loss: 0.0183, Val Loss: 0.0186\n",
      "Epoch [48/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [49/200], Train Loss: 0.0182, Val Loss: 0.0187\n",
      "Epoch [50/200], Train Loss: 0.0182, Val Loss: 0.0186\n",
      "Epoch [51/200], Train Loss: 0.0182, Val Loss: 0.0186\n",
      "Epoch [52/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [53/200], Train Loss: 0.0181, Val Loss: 0.0185\n",
      "Epoch [54/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [55/200], Train Loss: 0.0181, Val Loss: 0.0185\n",
      "Epoch [56/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [57/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [58/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [59/200], Train Loss: 0.0179, Val Loss: 0.0186\n",
      "Epoch [60/200], Train Loss: 0.0180, Val Loss: 0.0183\n",
      "Epoch [61/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [62/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [63/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [64/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [65/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [66/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [67/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [68/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [69/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [70/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [71/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [72/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [73/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [74/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [75/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [76/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [77/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [78/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [79/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [80/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [81/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [82/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [83/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [84/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [85/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [86/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [87/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [88/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [89/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [90/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [91/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [92/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [93/200], Train Loss: 0.0173, Val Loss: 0.0178\n",
      "Epoch [94/200], Train Loss: 0.0173, Val Loss: 0.0178\n",
      "Epoch [95/200], Train Loss: 0.0173, Val Loss: 0.0178\n",
      "Epoch [96/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [97/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [98/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [99/200], Train Loss: 0.0172, Val Loss: 0.0177\n",
      "Epoch [100/200], Train Loss: 0.0172, Val Loss: 0.0178\n",
      "Epoch [101/200], Train Loss: 0.0172, Val Loss: 0.0177\n",
      "Epoch [102/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [103/200], Train Loss: 0.0172, Val Loss: 0.0177\n",
      "Epoch [104/200], Train Loss: 0.0171, Val Loss: 0.0176\n",
      "Epoch [105/200], Train Loss: 0.0171, Val Loss: 0.0178\n",
      "Epoch [106/200], Train Loss: 0.0171, Val Loss: 0.0176\n",
      "Epoch [107/200], Train Loss: 0.0171, Val Loss: 0.0176\n",
      "Epoch [108/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [109/200], Train Loss: 0.0171, Val Loss: 0.0176\n",
      "Epoch [110/200], Train Loss: 0.0171, Val Loss: 0.0176\n",
      "Epoch [111/200], Train Loss: 0.0171, Val Loss: 0.0177\n",
      "Epoch [112/200], Train Loss: 0.0170, Val Loss: 0.0176\n",
      "Epoch [113/200], Train Loss: 0.0170, Val Loss: 0.0176\n",
      "Epoch [114/200], Train Loss: 0.0170, Val Loss: 0.0176\n",
      "Epoch [115/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [116/200], Train Loss: 0.0170, Val Loss: 0.0176\n",
      "Epoch [117/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [118/200], Train Loss: 0.0170, Val Loss: 0.0177\n",
      "Epoch [119/200], Train Loss: 0.0169, Val Loss: 0.0176\n",
      "Epoch [120/200], Train Loss: 0.0169, Val Loss: 0.0177\n",
      "Epoch [121/200], Train Loss: 0.0169, Val Loss: 0.0175\n",
      "Epoch [122/200], Train Loss: 0.0169, Val Loss: 0.0176\n",
      "Epoch [123/200], Train Loss: 0.0169, Val Loss: 0.0175\n",
      "Epoch [124/200], Train Loss: 0.0169, Val Loss: 0.0176\n",
      "Epoch [125/200], Train Loss: 0.0169, Val Loss: 0.0174\n",
      "Epoch [126/200], Train Loss: 0.0169, Val Loss: 0.0174\n",
      "Epoch [127/200], Train Loss: 0.0169, Val Loss: 0.0175\n",
      "Epoch [128/200], Train Loss: 0.0169, Val Loss: 0.0174\n",
      "Epoch [129/200], Train Loss: 0.0169, Val Loss: 0.0175\n",
      "Epoch [130/200], Train Loss: 0.0168, Val Loss: 0.0174\n",
      "Epoch [131/200], Train Loss: 0.0168, Val Loss: 0.0174\n",
      "Epoch [132/200], Train Loss: 0.0169, Val Loss: 0.0175\n",
      "Epoch [133/200], Train Loss: 0.0168, Val Loss: 0.0175\n",
      "Epoch [134/200], Train Loss: 0.0168, Val Loss: 0.0174\n",
      "Epoch [135/200], Train Loss: 0.0168, Val Loss: 0.0175\n",
      "Epoch [136/200], Train Loss: 0.0168, Val Loss: 0.0175\n",
      "Epoch [137/200], Train Loss: 0.0168, Val Loss: 0.0174\n",
      "Epoch [138/200], Train Loss: 0.0168, Val Loss: 0.0174\n",
      "Epoch [139/200], Train Loss: 0.0168, Val Loss: 0.0174\n",
      "Epoch [140/200], Train Loss: 0.0168, Val Loss: 0.0175\n",
      "Epoch [141/200], Train Loss: 0.0167, Val Loss: 0.0174\n",
      "Epoch [142/200], Train Loss: 0.0168, Val Loss: 0.0174\n",
      "Epoch [143/200], Train Loss: 0.0167, Val Loss: 0.0173\n",
      "Epoch [144/200], Train Loss: 0.0167, Val Loss: 0.0173\n",
      "Epoch [145/200], Train Loss: 0.0167, Val Loss: 0.0173\n",
      "Epoch [146/200], Train Loss: 0.0167, Val Loss: 0.0174\n",
      "Epoch [147/200], Train Loss: 0.0167, Val Loss: 0.0173\n",
      "Epoch [148/200], Train Loss: 0.0167, Val Loss: 0.0175\n",
      "Epoch [149/200], Train Loss: 0.0167, Val Loss: 0.0174\n",
      "Epoch [150/200], Train Loss: 0.0167, Val Loss: 0.0173\n",
      "Epoch [151/200], Train Loss: 0.0166, Val Loss: 0.0173\n",
      "Epoch [152/200], Train Loss: 0.0166, Val Loss: 0.0173\n",
      "Epoch [153/200], Train Loss: 0.0166, Val Loss: 0.0173\n",
      "Epoch [154/200], Train Loss: 0.0166, Val Loss: 0.0173\n",
      "Epoch [155/200], Train Loss: 0.0166, Val Loss: 0.0173\n",
      "Epoch [156/200], Train Loss: 0.0166, Val Loss: 0.0172\n",
      "Epoch [157/200], Train Loss: 0.0166, Val Loss: 0.0172\n",
      "Epoch [158/200], Train Loss: 0.0166, Val Loss: 0.0172\n",
      "Epoch [159/200], Train Loss: 0.0166, Val Loss: 0.0173\n",
      "Epoch [160/200], Train Loss: 0.0166, Val Loss: 0.0172\n",
      "Epoch [161/200], Train Loss: 0.0165, Val Loss: 0.0171\n",
      "Epoch [162/200], Train Loss: 0.0165, Val Loss: 0.0172\n",
      "Epoch [163/200], Train Loss: 0.0165, Val Loss: 0.0172\n",
      "Epoch [164/200], Train Loss: 0.0165, Val Loss: 0.0173\n",
      "Epoch [165/200], Train Loss: 0.0165, Val Loss: 0.0172\n",
      "Epoch [166/200], Train Loss: 0.0165, Val Loss: 0.0171\n",
      "Epoch [167/200], Train Loss: 0.0165, Val Loss: 0.0173\n",
      "Epoch [168/200], Train Loss: 0.0165, Val Loss: 0.0171\n",
      "Epoch [169/200], Train Loss: 0.0165, Val Loss: 0.0171\n",
      "Epoch [170/200], Train Loss: 0.0165, Val Loss: 0.0171\n",
      "Epoch [171/200], Train Loss: 0.0164, Val Loss: 0.0172\n",
      "Epoch [172/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [173/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [174/200], Train Loss: 0.0165, Val Loss: 0.0172\n",
      "Epoch [175/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [176/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [177/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [178/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [179/200], Train Loss: 0.0164, Val Loss: 0.0170\n",
      "Epoch [180/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [181/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [182/200], Train Loss: 0.0164, Val Loss: 0.0170\n",
      "Epoch [183/200], Train Loss: 0.0164, Val Loss: 0.0170\n",
      "Epoch [184/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [185/200], Train Loss: 0.0164, Val Loss: 0.0171\n",
      "Epoch [186/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [187/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [188/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [189/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [190/200], Train Loss: 0.0163, Val Loss: 0.0171\n",
      "Epoch [191/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [192/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [193/200], Train Loss: 0.0163, Val Loss: 0.0169\n",
      "Epoch [194/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [195/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [196/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [197/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [198/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [199/200], Train Loss: 0.0163, Val Loss: 0.0170\n",
      "Epoch [200/200], Train Loss: 0.0162, Val Loss: 0.0169\n",
      "Epoch [1/200], Train Loss: 0.0327, Val Loss: 0.0276\n",
      "Epoch [2/200], Train Loss: 0.0261, Val Loss: 0.0248\n",
      "Epoch [3/200], Train Loss: 0.0239, Val Loss: 0.0233\n",
      "Epoch [4/200], Train Loss: 0.0226, Val Loss: 0.0223\n",
      "Epoch [5/200], Train Loss: 0.0218, Val Loss: 0.0215\n",
      "Epoch [6/200], Train Loss: 0.0212, Val Loss: 0.0212\n",
      "Epoch [7/200], Train Loss: 0.0209, Val Loss: 0.0209\n",
      "Epoch [8/200], Train Loss: 0.0206, Val Loss: 0.0206\n",
      "Epoch [9/200], Train Loss: 0.0203, Val Loss: 0.0203\n",
      "Epoch [10/200], Train Loss: 0.0201, Val Loss: 0.0203\n",
      "Epoch [11/200], Train Loss: 0.0199, Val Loss: 0.0198\n",
      "Epoch [12/200], Train Loss: 0.0196, Val Loss: 0.0198\n",
      "Epoch [13/200], Train Loss: 0.0194, Val Loss: 0.0195\n",
      "Epoch [14/200], Train Loss: 0.0191, Val Loss: 0.0193\n",
      "Epoch [15/200], Train Loss: 0.0190, Val Loss: 0.0193\n",
      "Epoch [16/200], Train Loss: 0.0189, Val Loss: 0.0192\n",
      "Epoch [17/200], Train Loss: 0.0188, Val Loss: 0.0190\n",
      "Epoch [18/200], Train Loss: 0.0187, Val Loss: 0.0190\n",
      "Epoch [19/200], Train Loss: 0.0187, Val Loss: 0.0190\n",
      "Epoch [20/200], Train Loss: 0.0187, Val Loss: 0.0190\n",
      "Epoch [21/200], Train Loss: 0.0186, Val Loss: 0.0188\n",
      "Epoch [22/200], Train Loss: 0.0186, Val Loss: 0.0189\n",
      "Epoch [23/200], Train Loss: 0.0185, Val Loss: 0.0189\n",
      "Epoch [24/200], Train Loss: 0.0185, Val Loss: 0.0188\n",
      "Epoch [25/200], Train Loss: 0.0185, Val Loss: 0.0188\n",
      "Epoch [26/200], Train Loss: 0.0184, Val Loss: 0.0187\n",
      "Epoch [27/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [28/200], Train Loss: 0.0184, Val Loss: 0.0189\n",
      "Epoch [29/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [30/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [31/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [32/200], Train Loss: 0.0182, Val Loss: 0.0186\n",
      "Epoch [33/200], Train Loss: 0.0182, Val Loss: 0.0186\n",
      "Epoch [34/200], Train Loss: 0.0182, Val Loss: 0.0186\n",
      "Epoch [35/200], Train Loss: 0.0182, Val Loss: 0.0185\n",
      "Epoch [36/200], Train Loss: 0.0182, Val Loss: 0.0185\n",
      "Epoch [37/200], Train Loss: 0.0182, Val Loss: 0.0185\n",
      "Epoch [38/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [39/200], Train Loss: 0.0181, Val Loss: 0.0185\n",
      "Epoch [40/200], Train Loss: 0.0181, Val Loss: 0.0186\n",
      "Epoch [41/200], Train Loss: 0.0181, Val Loss: 0.0185\n",
      "Epoch [42/200], Train Loss: 0.0181, Val Loss: 0.0185\n",
      "Epoch [43/200], Train Loss: 0.0181, Val Loss: 0.0184\n",
      "Epoch [44/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [45/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [46/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [47/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [48/200], Train Loss: 0.0180, Val Loss: 0.0185\n",
      "Epoch [49/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [50/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [51/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [52/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [53/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [54/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [55/200], Train Loss: 0.0179, Val Loss: 0.0184\n",
      "Epoch [56/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [57/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [58/200], Train Loss: 0.0179, Val Loss: 0.0182\n",
      "Epoch [59/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [60/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [61/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [62/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [63/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [64/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [65/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [66/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [67/200], Train Loss: 0.0178, Val Loss: 0.0183\n",
      "Epoch [68/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [69/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [70/200], Train Loss: 0.0177, Val Loss: 0.0182\n",
      "Epoch [71/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [72/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [73/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [74/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [75/200], Train Loss: 0.0176, Val Loss: 0.0180\n",
      "Epoch [76/200], Train Loss: 0.0176, Val Loss: 0.0180\n",
      "Epoch [77/200], Train Loss: 0.0176, Val Loss: 0.0180\n",
      "Epoch [78/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [79/200], Train Loss: 0.0176, Val Loss: 0.0180\n",
      "Epoch [80/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [81/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [82/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [83/200], Train Loss: 0.0175, Val Loss: 0.0179\n",
      "Epoch [84/200], Train Loss: 0.0175, Val Loss: 0.0180\n",
      "Epoch [85/200], Train Loss: 0.0175, Val Loss: 0.0179\n",
      "Epoch [86/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [87/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [88/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [89/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [90/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [91/200], Train Loss: 0.0174, Val Loss: 0.0179\n",
      "Epoch [92/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [93/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [94/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [95/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [96/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [97/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [98/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [99/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [100/200], Train Loss: 0.0172, Val Loss: 0.0177\n",
      "Epoch [101/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [102/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [103/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [104/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [105/200], Train Loss: 0.0171, Val Loss: 0.0176\n",
      "Epoch [106/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [107/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [108/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [109/200], Train Loss: 0.0171, Val Loss: 0.0174\n",
      "Epoch [110/200], Train Loss: 0.0170, Val Loss: 0.0175\n",
      "Epoch [111/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [112/200], Train Loss: 0.0170, Val Loss: 0.0176\n",
      "Epoch [113/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [114/200], Train Loss: 0.0170, Val Loss: 0.0175\n",
      "Epoch [115/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [116/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [117/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [118/200], Train Loss: 0.0170, Val Loss: 0.0175\n",
      "Epoch [119/200], Train Loss: 0.0169, Val Loss: 0.0174\n",
      "Epoch [120/200], Train Loss: 0.0169, Val Loss: 0.0174\n",
      "Epoch [121/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [122/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [123/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [124/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [125/200], Train Loss: 0.0168, Val Loss: 0.0173\n",
      "Epoch [126/200], Train Loss: 0.0168, Val Loss: 0.0174\n",
      "Epoch [127/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [128/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [129/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [130/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [131/200], Train Loss: 0.0167, Val Loss: 0.0173\n",
      "Epoch [132/200], Train Loss: 0.0167, Val Loss: 0.0172\n",
      "Epoch [133/200], Train Loss: 0.0167, Val Loss: 0.0172\n",
      "Epoch [134/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [135/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [136/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [137/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [138/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [139/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [140/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [141/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [142/200], Train Loss: 0.0165, Val Loss: 0.0170\n",
      "Epoch [143/200], Train Loss: 0.0165, Val Loss: 0.0171\n",
      "Epoch [144/200], Train Loss: 0.0165, Val Loss: 0.0170\n",
      "Epoch [145/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [146/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [147/200], Train Loss: 0.0165, Val Loss: 0.0170\n",
      "Epoch [148/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [149/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [150/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [151/200], Train Loss: 0.0164, Val Loss: 0.0170\n",
      "Epoch [152/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [153/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [154/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [155/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [156/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [157/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [158/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [159/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [160/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [161/200], Train Loss: 0.0162, Val Loss: 0.0168\n",
      "Epoch [162/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [163/200], Train Loss: 0.0162, Val Loss: 0.0168\n",
      "Epoch [164/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [165/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [166/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [167/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [168/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [169/200], Train Loss: 0.0162, Val Loss: 0.0169\n",
      "Epoch [170/200], Train Loss: 0.0161, Val Loss: 0.0168\n",
      "Epoch [171/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [172/200], Train Loss: 0.0161, Val Loss: 0.0167\n",
      "Epoch [173/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [174/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [175/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [176/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [177/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [178/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [179/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [180/200], Train Loss: 0.0160, Val Loss: 0.0166\n",
      "Epoch [181/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [182/200], Train Loss: 0.0160, Val Loss: 0.0166\n",
      "Epoch [183/200], Train Loss: 0.0160, Val Loss: 0.0166\n",
      "Epoch [184/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [185/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [186/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [187/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [188/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [189/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [190/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [191/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [192/200], Train Loss: 0.0159, Val Loss: 0.0166\n",
      "Epoch [193/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [194/200], Train Loss: 0.0159, Val Loss: 0.0166\n",
      "Epoch [195/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [196/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [197/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [198/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [199/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [200/200], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [1/200], Train Loss: 0.0328, Val Loss: 0.0278\n",
      "Epoch [2/200], Train Loss: 0.0256, Val Loss: 0.0242\n",
      "Epoch [3/200], Train Loss: 0.0231, Val Loss: 0.0224\n",
      "Epoch [4/200], Train Loss: 0.0218, Val Loss: 0.0215\n",
      "Epoch [5/200], Train Loss: 0.0211, Val Loss: 0.0210\n",
      "Epoch [6/200], Train Loss: 0.0207, Val Loss: 0.0206\n",
      "Epoch [7/200], Train Loss: 0.0201, Val Loss: 0.0201\n",
      "Epoch [8/200], Train Loss: 0.0195, Val Loss: 0.0197\n",
      "Epoch [9/200], Train Loss: 0.0193, Val Loss: 0.0195\n",
      "Epoch [10/200], Train Loss: 0.0191, Val Loss: 0.0194\n",
      "Epoch [11/200], Train Loss: 0.0190, Val Loss: 0.0194\n",
      "Epoch [12/200], Train Loss: 0.0189, Val Loss: 0.0191\n",
      "Epoch [13/200], Train Loss: 0.0188, Val Loss: 0.0190\n",
      "Epoch [14/200], Train Loss: 0.0187, Val Loss: 0.0190\n",
      "Epoch [15/200], Train Loss: 0.0186, Val Loss: 0.0189\n",
      "Epoch [16/200], Train Loss: 0.0185, Val Loss: 0.0188\n",
      "Epoch [17/200], Train Loss: 0.0183, Val Loss: 0.0187\n",
      "Epoch [18/200], Train Loss: 0.0182, Val Loss: 0.0185\n",
      "Epoch [19/200], Train Loss: 0.0181, Val Loss: 0.0185\n",
      "Epoch [20/200], Train Loss: 0.0181, Val Loss: 0.0184\n",
      "Epoch [21/200], Train Loss: 0.0180, Val Loss: 0.0183\n",
      "Epoch [22/200], Train Loss: 0.0180, Val Loss: 0.0183\n",
      "Epoch [23/200], Train Loss: 0.0180, Val Loss: 0.0183\n",
      "Epoch [24/200], Train Loss: 0.0180, Val Loss: 0.0183\n",
      "Epoch [25/200], Train Loss: 0.0179, Val Loss: 0.0182\n",
      "Epoch [26/200], Train Loss: 0.0179, Val Loss: 0.0183\n",
      "Epoch [27/200], Train Loss: 0.0179, Val Loss: 0.0182\n",
      "Epoch [28/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [29/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [30/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [31/200], Train Loss: 0.0178, Val Loss: 0.0181\n",
      "Epoch [32/200], Train Loss: 0.0178, Val Loss: 0.0181\n",
      "Epoch [33/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [34/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [35/200], Train Loss: 0.0177, Val Loss: 0.0180\n",
      "Epoch [36/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [37/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [38/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [39/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [40/200], Train Loss: 0.0176, Val Loss: 0.0181\n",
      "Epoch [41/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [42/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [43/200], Train Loss: 0.0175, Val Loss: 0.0179\n",
      "Epoch [44/200], Train Loss: 0.0175, Val Loss: 0.0179\n",
      "Epoch [45/200], Train Loss: 0.0175, Val Loss: 0.0179\n",
      "Epoch [46/200], Train Loss: 0.0175, Val Loss: 0.0178\n",
      "Epoch [47/200], Train Loss: 0.0175, Val Loss: 0.0178\n",
      "Epoch [48/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [49/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [50/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [51/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [52/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [53/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [54/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [55/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [56/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [57/200], Train Loss: 0.0173, Val Loss: 0.0179\n",
      "Epoch [58/200], Train Loss: 0.0173, Val Loss: 0.0176\n",
      "Epoch [59/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [60/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [61/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [62/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [63/200], Train Loss: 0.0171, Val Loss: 0.0176\n",
      "Epoch [64/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [65/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [66/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [67/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [68/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [69/200], Train Loss: 0.0170, Val Loss: 0.0175\n",
      "Epoch [70/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [71/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [72/200], Train Loss: 0.0169, Val Loss: 0.0174\n",
      "Epoch [73/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [74/200], Train Loss: 0.0169, Val Loss: 0.0174\n",
      "Epoch [75/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [76/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [77/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [78/200], Train Loss: 0.0168, Val Loss: 0.0171\n",
      "Epoch [79/200], Train Loss: 0.0167, Val Loss: 0.0172\n",
      "Epoch [80/200], Train Loss: 0.0167, Val Loss: 0.0172\n",
      "Epoch [81/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [82/200], Train Loss: 0.0167, Val Loss: 0.0170\n",
      "Epoch [83/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [84/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [85/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [86/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [87/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [88/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [89/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [90/200], Train Loss: 0.0165, Val Loss: 0.0170\n",
      "Epoch [91/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [92/200], Train Loss: 0.0165, Val Loss: 0.0168\n",
      "Epoch [93/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [94/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [95/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [96/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [97/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [98/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [99/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [100/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [101/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [102/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [103/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [104/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [105/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [106/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [107/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [108/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [109/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [110/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [111/200], Train Loss: 0.0162, Val Loss: 0.0168\n",
      "Epoch [112/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [113/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [114/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [115/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [116/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [117/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [118/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [119/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [120/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [121/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [122/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [123/200], Train Loss: 0.0160, Val Loss: 0.0166\n",
      "Epoch [124/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [125/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [126/200], Train Loss: 0.0160, Val Loss: 0.0167\n",
      "Epoch [127/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [128/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [129/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [130/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [131/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [132/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [133/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [134/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [135/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [136/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [137/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [138/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [139/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [140/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [141/200], Train Loss: 0.0158, Val Loss: 0.0164\n",
      "Epoch [142/200], Train Loss: 0.0158, Val Loss: 0.0164\n",
      "Epoch [143/200], Train Loss: 0.0158, Val Loss: 0.0164\n",
      "Epoch [144/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [145/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [146/200], Train Loss: 0.0158, Val Loss: 0.0164\n",
      "Epoch [147/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [148/200], Train Loss: 0.0158, Val Loss: 0.0165\n",
      "Epoch [149/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [150/200], Train Loss: 0.0158, Val Loss: 0.0164\n",
      "Epoch [151/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [152/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [153/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [154/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [155/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [156/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [157/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [158/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [159/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [160/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [161/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [162/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [163/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [164/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [165/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [166/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [167/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [168/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [169/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [170/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [171/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [172/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [173/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [174/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [175/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [176/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [177/200], Train Loss: 0.0155, Val Loss: 0.0162\n",
      "Epoch [178/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [179/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [180/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [181/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [182/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [183/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [184/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [185/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [186/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [187/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [188/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [189/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [190/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [191/200], Train Loss: 0.0154, Val Loss: 0.0161\n",
      "Epoch [192/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [193/200], Train Loss: 0.0154, Val Loss: 0.0160\n",
      "Epoch [194/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [195/200], Train Loss: 0.0154, Val Loss: 0.0160\n",
      "Epoch [196/200], Train Loss: 0.0154, Val Loss: 0.0160\n",
      "Epoch [197/200], Train Loss: 0.0154, Val Loss: 0.0162\n",
      "Epoch [198/200], Train Loss: 0.0154, Val Loss: 0.0160\n",
      "Epoch [199/200], Train Loss: 0.0154, Val Loss: 0.0161\n",
      "Early stopping at epoch 199\n",
      "Epoch [1/200], Train Loss: 0.0332, Val Loss: 0.0281\n",
      "Epoch [2/200], Train Loss: 0.0261, Val Loss: 0.0246\n",
      "Epoch [3/200], Train Loss: 0.0236, Val Loss: 0.0229\n",
      "Epoch [4/200], Train Loss: 0.0223, Val Loss: 0.0219\n",
      "Epoch [5/200], Train Loss: 0.0215, Val Loss: 0.0212\n",
      "Epoch [6/200], Train Loss: 0.0208, Val Loss: 0.0206\n",
      "Epoch [7/200], Train Loss: 0.0202, Val Loss: 0.0202\n",
      "Epoch [8/200], Train Loss: 0.0198, Val Loss: 0.0198\n",
      "Epoch [9/200], Train Loss: 0.0195, Val Loss: 0.0195\n",
      "Epoch [10/200], Train Loss: 0.0192, Val Loss: 0.0191\n",
      "Epoch [11/200], Train Loss: 0.0189, Val Loss: 0.0190\n",
      "Epoch [12/200], Train Loss: 0.0186, Val Loss: 0.0187\n",
      "Epoch [13/200], Train Loss: 0.0184, Val Loss: 0.0186\n",
      "Epoch [14/200], Train Loss: 0.0182, Val Loss: 0.0185\n",
      "Epoch [15/200], Train Loss: 0.0181, Val Loss: 0.0184\n",
      "Epoch [16/200], Train Loss: 0.0180, Val Loss: 0.0184\n",
      "Epoch [17/200], Train Loss: 0.0180, Val Loss: 0.0183\n",
      "Epoch [18/200], Train Loss: 0.0180, Val Loss: 0.0183\n",
      "Epoch [19/200], Train Loss: 0.0179, Val Loss: 0.0182\n",
      "Epoch [20/200], Train Loss: 0.0179, Val Loss: 0.0182\n",
      "Epoch [21/200], Train Loss: 0.0179, Val Loss: 0.0182\n",
      "Epoch [22/200], Train Loss: 0.0179, Val Loss: 0.0181\n",
      "Epoch [23/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [24/200], Train Loss: 0.0178, Val Loss: 0.0181\n",
      "Epoch [25/200], Train Loss: 0.0178, Val Loss: 0.0180\n",
      "Epoch [26/200], Train Loss: 0.0178, Val Loss: 0.0182\n",
      "Epoch [27/200], Train Loss: 0.0178, Val Loss: 0.0181\n",
      "Epoch [28/200], Train Loss: 0.0178, Val Loss: 0.0181\n",
      "Epoch [29/200], Train Loss: 0.0178, Val Loss: 0.0181\n",
      "Epoch [30/200], Train Loss: 0.0177, Val Loss: 0.0180\n",
      "Epoch [31/200], Train Loss: 0.0177, Val Loss: 0.0180\n",
      "Epoch [32/200], Train Loss: 0.0177, Val Loss: 0.0181\n",
      "Epoch [33/200], Train Loss: 0.0177, Val Loss: 0.0180\n",
      "Epoch [34/200], Train Loss: 0.0177, Val Loss: 0.0180\n",
      "Epoch [35/200], Train Loss: 0.0177, Val Loss: 0.0180\n",
      "Epoch [36/200], Train Loss: 0.0177, Val Loss: 0.0180\n",
      "Epoch [37/200], Train Loss: 0.0177, Val Loss: 0.0180\n",
      "Epoch [38/200], Train Loss: 0.0177, Val Loss: 0.0179\n",
      "Epoch [39/200], Train Loss: 0.0176, Val Loss: 0.0180\n",
      "Epoch [40/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [41/200], Train Loss: 0.0176, Val Loss: 0.0180\n",
      "Epoch [42/200], Train Loss: 0.0176, Val Loss: 0.0180\n",
      "Epoch [43/200], Train Loss: 0.0176, Val Loss: 0.0180\n",
      "Epoch [44/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [45/200], Train Loss: 0.0176, Val Loss: 0.0180\n",
      "Epoch [46/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [47/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [48/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [49/200], Train Loss: 0.0175, Val Loss: 0.0179\n",
      "Epoch [50/200], Train Loss: 0.0175, Val Loss: 0.0178\n",
      "Epoch [51/200], Train Loss: 0.0175, Val Loss: 0.0178\n",
      "Epoch [52/200], Train Loss: 0.0175, Val Loss: 0.0178\n",
      "Epoch [53/200], Train Loss: 0.0175, Val Loss: 0.0179\n",
      "Epoch [54/200], Train Loss: 0.0175, Val Loss: 0.0178\n",
      "Epoch [55/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [56/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [57/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [58/200], Train Loss: 0.0174, Val Loss: 0.0178\n",
      "Epoch [59/200], Train Loss: 0.0174, Val Loss: 0.0177\n",
      "Epoch [60/200], Train Loss: 0.0174, Val Loss: 0.0177\n",
      "Epoch [61/200], Train Loss: 0.0174, Val Loss: 0.0177\n",
      "Epoch [62/200], Train Loss: 0.0173, Val Loss: 0.0178\n",
      "Epoch [63/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [64/200], Train Loss: 0.0173, Val Loss: 0.0176\n",
      "Epoch [65/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [66/200], Train Loss: 0.0173, Val Loss: 0.0176\n",
      "Epoch [67/200], Train Loss: 0.0173, Val Loss: 0.0176\n",
      "Epoch [68/200], Train Loss: 0.0173, Val Loss: 0.0176\n",
      "Epoch [69/200], Train Loss: 0.0173, Val Loss: 0.0177\n",
      "Epoch [70/200], Train Loss: 0.0173, Val Loss: 0.0176\n",
      "Epoch [71/200], Train Loss: 0.0172, Val Loss: 0.0177\n",
      "Epoch [72/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [73/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [74/200], Train Loss: 0.0172, Val Loss: 0.0177\n",
      "Epoch [75/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [76/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [77/200], Train Loss: 0.0172, Val Loss: 0.0175\n",
      "Epoch [78/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [79/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [80/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [81/200], Train Loss: 0.0171, Val Loss: 0.0175\n",
      "Epoch [82/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [83/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [84/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [85/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [86/200], Train Loss: 0.0170, Val Loss: 0.0172\n",
      "Epoch [87/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [88/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [89/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [90/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [91/200], Train Loss: 0.0169, Val Loss: 0.0173\n",
      "Epoch [92/200], Train Loss: 0.0169, Val Loss: 0.0172\n",
      "Epoch [93/200], Train Loss: 0.0168, Val Loss: 0.0173\n",
      "Epoch [94/200], Train Loss: 0.0168, Val Loss: 0.0173\n",
      "Epoch [95/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [96/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [97/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [98/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [99/200], Train Loss: 0.0168, Val Loss: 0.0171\n",
      "Epoch [100/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [101/200], Train Loss: 0.0167, Val Loss: 0.0172\n",
      "Epoch [102/200], Train Loss: 0.0167, Val Loss: 0.0170\n",
      "Epoch [103/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [104/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [105/200], Train Loss: 0.0167, Val Loss: 0.0170\n",
      "Epoch [106/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [107/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [108/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [109/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [110/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [111/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [112/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [113/200], Train Loss: 0.0165, Val Loss: 0.0170\n",
      "Epoch [114/200], Train Loss: 0.0165, Val Loss: 0.0170\n",
      "Epoch [115/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [116/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [117/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [118/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [119/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [120/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [121/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [122/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [123/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [124/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [125/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [126/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [127/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [128/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [129/200], Train Loss: 0.0163, Val Loss: 0.0169\n",
      "Epoch [130/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [131/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [132/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [133/200], Train Loss: 0.0163, Val Loss: 0.0169\n",
      "Epoch [134/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [135/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [136/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [137/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [138/200], Train Loss: 0.0162, Val Loss: 0.0168\n",
      "Epoch [139/200], Train Loss: 0.0162, Val Loss: 0.0168\n",
      "Epoch [140/200], Train Loss: 0.0162, Val Loss: 0.0168\n",
      "Epoch [141/200], Train Loss: 0.0162, Val Loss: 0.0168\n",
      "Epoch [142/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [143/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [144/200], Train Loss: 0.0162, Val Loss: 0.0168\n",
      "Epoch [145/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [146/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [147/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [148/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [149/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [150/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [151/200], Train Loss: 0.0161, Val Loss: 0.0167\n",
      "Epoch [152/200], Train Loss: 0.0161, Val Loss: 0.0168\n",
      "Epoch [153/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [154/200], Train Loss: 0.0161, Val Loss: 0.0167\n",
      "Epoch [155/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [156/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [157/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [158/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [159/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [160/200], Train Loss: 0.0160, Val Loss: 0.0166\n",
      "Epoch [161/200], Train Loss: 0.0160, Val Loss: 0.0166\n",
      "Epoch [162/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [163/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [164/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [165/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [166/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [167/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [168/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [169/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [170/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [171/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [172/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [173/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [174/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [175/200], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [176/200], Train Loss: 0.0158, Val Loss: 0.0164\n",
      "Epoch [177/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [178/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [179/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [180/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [181/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [182/200], Train Loss: 0.0158, Val Loss: 0.0162\n",
      "Epoch [183/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [184/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [185/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [186/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [187/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [188/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [189/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [190/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [191/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [192/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [193/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [194/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [195/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [196/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [197/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [198/200], Train Loss: 0.0156, Val Loss: 0.0160\n",
      "Epoch [199/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [200/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [1/200], Train Loss: 0.0327, Val Loss: 0.0282\n",
      "Epoch [2/200], Train Loss: 0.0263, Val Loss: 0.0247\n",
      "Epoch [3/200], Train Loss: 0.0235, Val Loss: 0.0230\n",
      "Epoch [4/200], Train Loss: 0.0221, Val Loss: 0.0217\n",
      "Epoch [5/200], Train Loss: 0.0212, Val Loss: 0.0210\n",
      "Epoch [6/200], Train Loss: 0.0207, Val Loss: 0.0206\n",
      "Epoch [7/200], Train Loss: 0.0202, Val Loss: 0.0201\n",
      "Epoch [8/200], Train Loss: 0.0197, Val Loss: 0.0198\n",
      "Epoch [9/200], Train Loss: 0.0194, Val Loss: 0.0196\n",
      "Epoch [10/200], Train Loss: 0.0192, Val Loss: 0.0194\n",
      "Epoch [11/200], Train Loss: 0.0189, Val Loss: 0.0191\n",
      "Epoch [12/200], Train Loss: 0.0187, Val Loss: 0.0187\n",
      "Epoch [13/200], Train Loss: 0.0184, Val Loss: 0.0185\n",
      "Epoch [14/200], Train Loss: 0.0181, Val Loss: 0.0183\n",
      "Epoch [15/200], Train Loss: 0.0179, Val Loss: 0.0181\n",
      "Epoch [16/200], Train Loss: 0.0178, Val Loss: 0.0180\n",
      "Epoch [17/200], Train Loss: 0.0177, Val Loss: 0.0180\n",
      "Epoch [18/200], Train Loss: 0.0176, Val Loss: 0.0179\n",
      "Epoch [19/200], Train Loss: 0.0175, Val Loss: 0.0178\n",
      "Epoch [20/200], Train Loss: 0.0175, Val Loss: 0.0177\n",
      "Epoch [21/200], Train Loss: 0.0174, Val Loss: 0.0177\n",
      "Epoch [22/200], Train Loss: 0.0173, Val Loss: 0.0176\n",
      "Epoch [23/200], Train Loss: 0.0173, Val Loss: 0.0178\n",
      "Epoch [24/200], Train Loss: 0.0172, Val Loss: 0.0176\n",
      "Epoch [25/200], Train Loss: 0.0172, Val Loss: 0.0175\n",
      "Epoch [26/200], Train Loss: 0.0171, Val Loss: 0.0174\n",
      "Epoch [27/200], Train Loss: 0.0171, Val Loss: 0.0174\n",
      "Epoch [28/200], Train Loss: 0.0170, Val Loss: 0.0174\n",
      "Epoch [29/200], Train Loss: 0.0170, Val Loss: 0.0173\n",
      "Epoch [30/200], Train Loss: 0.0169, Val Loss: 0.0172\n",
      "Epoch [31/200], Train Loss: 0.0169, Val Loss: 0.0172\n",
      "Epoch [32/200], Train Loss: 0.0168, Val Loss: 0.0172\n",
      "Epoch [33/200], Train Loss: 0.0168, Val Loss: 0.0171\n",
      "Epoch [34/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [35/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [36/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [37/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [38/200], Train Loss: 0.0167, Val Loss: 0.0171\n",
      "Epoch [39/200], Train Loss: 0.0166, Val Loss: 0.0171\n",
      "Epoch [40/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [41/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [42/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [43/200], Train Loss: 0.0166, Val Loss: 0.0170\n",
      "Epoch [44/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [45/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [46/200], Train Loss: 0.0165, Val Loss: 0.0170\n",
      "Epoch [47/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [48/200], Train Loss: 0.0165, Val Loss: 0.0169\n",
      "Epoch [49/200], Train Loss: 0.0164, Val Loss: 0.0169\n",
      "Epoch [50/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [51/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [52/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [53/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [54/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [55/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [56/200], Train Loss: 0.0164, Val Loss: 0.0168\n",
      "Epoch [57/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [58/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [59/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [60/200], Train Loss: 0.0163, Val Loss: 0.0166\n",
      "Epoch [61/200], Train Loss: 0.0163, Val Loss: 0.0168\n",
      "Epoch [62/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [63/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [64/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [65/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [66/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [67/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [68/200], Train Loss: 0.0163, Val Loss: 0.0166\n",
      "Epoch [69/200], Train Loss: 0.0163, Val Loss: 0.0167\n",
      "Epoch [70/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [71/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [72/200], Train Loss: 0.0162, Val Loss: 0.0167\n",
      "Epoch [73/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [74/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [75/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [76/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [77/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [78/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [79/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [80/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [81/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [82/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [83/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [84/200], Train Loss: 0.0162, Val Loss: 0.0166\n",
      "Epoch [85/200], Train Loss: 0.0162, Val Loss: 0.0165\n",
      "Epoch [86/200], Train Loss: 0.0162, Val Loss: 0.0165\n",
      "Epoch [87/200], Train Loss: 0.0162, Val Loss: 0.0165\n",
      "Epoch [88/200], Train Loss: 0.0161, Val Loss: 0.0167\n",
      "Epoch [89/200], Train Loss: 0.0162, Val Loss: 0.0165\n",
      "Epoch [90/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [91/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [92/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [93/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [94/200], Train Loss: 0.0161, Val Loss: 0.0166\n",
      "Epoch [95/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [96/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [97/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [98/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [99/200], Train Loss: 0.0161, Val Loss: 0.0164\n",
      "Epoch [100/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [101/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [102/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [103/200], Train Loss: 0.0160, Val Loss: 0.0164\n",
      "Epoch [104/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [105/200], Train Loss: 0.0161, Val Loss: 0.0165\n",
      "Epoch [106/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [107/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [108/200], Train Loss: 0.0160, Val Loss: 0.0164\n",
      "Epoch [109/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [110/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [111/200], Train Loss: 0.0160, Val Loss: 0.0164\n",
      "Epoch [112/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [113/200], Train Loss: 0.0160, Val Loss: 0.0164\n",
      "Epoch [114/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [115/200], Train Loss: 0.0160, Val Loss: 0.0164\n",
      "Epoch [116/200], Train Loss: 0.0160, Val Loss: 0.0164\n",
      "Epoch [117/200], Train Loss: 0.0160, Val Loss: 0.0165\n",
      "Epoch [118/200], Train Loss: 0.0160, Val Loss: 0.0164\n",
      "Epoch [119/200], Train Loss: 0.0160, Val Loss: 0.0164\n",
      "Epoch [120/200], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [121/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [122/200], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [123/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [124/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [125/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [126/200], Train Loss: 0.0159, Val Loss: 0.0165\n",
      "Epoch [127/200], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [128/200], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [129/200], Train Loss: 0.0159, Val Loss: 0.0164\n",
      "Epoch [130/200], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [131/200], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [132/200], Train Loss: 0.0159, Val Loss: 0.0163\n",
      "Epoch [133/200], Train Loss: 0.0159, Val Loss: 0.0162\n",
      "Epoch [134/200], Train Loss: 0.0158, Val Loss: 0.0162\n",
      "Epoch [135/200], Train Loss: 0.0158, Val Loss: 0.0162\n",
      "Epoch [136/200], Train Loss: 0.0158, Val Loss: 0.0164\n",
      "Epoch [137/200], Train Loss: 0.0158, Val Loss: 0.0162\n",
      "Epoch [138/200], Train Loss: 0.0158, Val Loss: 0.0162\n",
      "Epoch [139/200], Train Loss: 0.0158, Val Loss: 0.0163\n",
      "Epoch [140/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [141/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [142/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [143/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [144/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [145/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [146/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [147/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [148/200], Train Loss: 0.0157, Val Loss: 0.0161\n",
      "Epoch [149/200], Train Loss: 0.0157, Val Loss: 0.0163\n",
      "Epoch [150/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [151/200], Train Loss: 0.0157, Val Loss: 0.0162\n",
      "Epoch [152/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [153/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [154/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [155/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [156/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [157/200], Train Loss: 0.0156, Val Loss: 0.0162\n",
      "Epoch [158/200], Train Loss: 0.0156, Val Loss: 0.0163\n",
      "Epoch [159/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [160/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [161/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [162/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [163/200], Train Loss: 0.0155, Val Loss: 0.0162\n",
      "Epoch [164/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [165/200], Train Loss: 0.0155, Val Loss: 0.0162\n",
      "Epoch [166/200], Train Loss: 0.0156, Val Loss: 0.0161\n",
      "Epoch [167/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [168/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [169/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [170/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [171/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [172/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [173/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [174/200], Train Loss: 0.0155, Val Loss: 0.0160\n",
      "Epoch [175/200], Train Loss: 0.0155, Val Loss: 0.0161\n",
      "Epoch [176/200], Train Loss: 0.0154, Val Loss: 0.0159\n",
      "Epoch [177/200], Train Loss: 0.0154, Val Loss: 0.0160\n",
      "Epoch [178/200], Train Loss: 0.0154, Val Loss: 0.0160\n",
      "Epoch [179/200], Train Loss: 0.0154, Val Loss: 0.0159\n",
      "Epoch [180/200], Train Loss: 0.0154, Val Loss: 0.0159\n",
      "Epoch [181/200], Train Loss: 0.0154, Val Loss: 0.0159\n",
      "Epoch [182/200], Train Loss: 0.0154, Val Loss: 0.0160\n",
      "Epoch [183/200], Train Loss: 0.0154, Val Loss: 0.0159\n",
      "Epoch [184/200], Train Loss: 0.0154, Val Loss: 0.0159\n",
      "Epoch [185/200], Train Loss: 0.0154, Val Loss: 0.0160\n",
      "Epoch [186/200], Train Loss: 0.0154, Val Loss: 0.0159\n",
      "Epoch [187/200], Train Loss: 0.0154, Val Loss: 0.0159\n",
      "Epoch [188/200], Train Loss: 0.0154, Val Loss: 0.0159\n",
      "Epoch [189/200], Train Loss: 0.0154, Val Loss: 0.0160\n",
      "Epoch [190/200], Train Loss: 0.0153, Val Loss: 0.0159\n",
      "Early stopping at epoch 190\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming your questionnaire items are in columns named 'Q1' to 'Q100'\n",
    "# questionnaire_columns = [col for col in data.columns if col.startswith('Q')]\n",
    "X = qns.iloc[:,2:].values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "class QuestionnaireDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.data[idx]  # Input and target are the same\n",
    "\n",
    "train_dataset = QuestionnaireDataset(X_train)\n",
    "val_dataset = QuestionnaireDataset(X_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Step 2: Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, latent_dim),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, input_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 5  # Reduce to 5 dimensions\n",
    "latent_dims = range(1, 21)\n",
    "val_losses = []\n",
    "\n",
    "for latent_dim in latent_dims:\n",
    "    model = Autoencoder(input_dim, latent_dim)\n",
    "\n",
    "    # Step 3: Train the autoencoder\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 200\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_features, _ in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_features)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_features.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_features, _ in val_loader:\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_features)\n",
    "                val_loss += loss.item() * batch_features.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "\n",
    "        # 检查 Early Stopping 条件\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0  # 重置计数器\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "plt.plot(latent_dims, val_losses, marker='o')\n",
    "plt.xlabel('Number of Latent Dimensions')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss vs. Number of Latent Dimensions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_val=[]\n",
    "for val, dim in zip(val_losses, latent_dims):\n",
    "    avg_val.append(val/dim)\n",
    "\n",
    "plt.plot(latent_dims, avg_val, marker='o')\n",
    "plt.xlabel('Number of Latent Dimensions')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss vs. Number of Latent Dimensions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.0342, Val Loss: 0.0302\n",
      "Epoch [2/200], Train Loss: 0.0298, Val Loss: 0.0296\n",
      "Epoch [3/200], Train Loss: 0.0295, Val Loss: 0.0294\n",
      "Epoch [4/200], Train Loss: 0.0293, Val Loss: 0.0293\n",
      "Epoch [5/200], Train Loss: 0.0291, Val Loss: 0.0291\n",
      "Epoch [6/200], Train Loss: 0.0290, Val Loss: 0.0291\n",
      "Epoch [7/200], Train Loss: 0.0289, Val Loss: 0.0290\n",
      "Epoch [8/200], Train Loss: 0.0288, Val Loss: 0.0289\n",
      "Epoch [9/200], Train Loss: 0.0287, Val Loss: 0.0289\n",
      "Epoch [10/200], Train Loss: 0.0286, Val Loss: 0.0287\n",
      "Epoch [11/200], Train Loss: 0.0285, Val Loss: 0.0287\n",
      "Epoch [12/200], Train Loss: 0.0284, Val Loss: 0.0287\n",
      "Epoch [13/200], Train Loss: 0.0284, Val Loss: 0.0287\n",
      "Epoch [14/200], Train Loss: 0.0283, Val Loss: 0.0286\n",
      "Epoch [15/200], Train Loss: 0.0283, Val Loss: 0.0286\n",
      "Epoch [16/200], Train Loss: 0.0282, Val Loss: 0.0286\n",
      "Epoch [17/200], Train Loss: 0.0282, Val Loss: 0.0286\n",
      "Epoch [18/200], Train Loss: 0.0282, Val Loss: 0.0286\n",
      "Epoch [19/200], Train Loss: 0.0282, Val Loss: 0.0285\n",
      "Epoch [20/200], Train Loss: 0.0282, Val Loss: 0.0284\n",
      "Epoch [21/200], Train Loss: 0.0282, Val Loss: 0.0285\n",
      "Epoch [22/200], Train Loss: 0.0282, Val Loss: 0.0285\n",
      "Epoch [23/200], Train Loss: 0.0281, Val Loss: 0.0284\n",
      "Epoch [24/200], Train Loss: 0.0282, Val Loss: 0.0285\n",
      "Epoch [25/200], Train Loss: 0.0281, Val Loss: 0.0284\n",
      "Epoch [26/200], Train Loss: 0.0280, Val Loss: 0.0284\n",
      "Epoch [27/200], Train Loss: 0.0280, Val Loss: 0.0283\n",
      "Epoch [28/200], Train Loss: 0.0280, Val Loss: 0.0284\n",
      "Epoch [29/200], Train Loss: 0.0280, Val Loss: 0.0284\n",
      "Epoch [30/200], Train Loss: 0.0279, Val Loss: 0.0283\n",
      "Epoch [31/200], Train Loss: 0.0280, Val Loss: 0.0283\n",
      "Epoch [32/200], Train Loss: 0.0279, Val Loss: 0.0284\n",
      "Epoch [33/200], Train Loss: 0.0279, Val Loss: 0.0283\n",
      "Epoch [34/200], Train Loss: 0.0279, Val Loss: 0.0283\n",
      "Epoch [35/200], Train Loss: 0.0279, Val Loss: 0.0283\n",
      "Epoch [36/200], Train Loss: 0.0279, Val Loss: 0.0283\n",
      "Epoch [37/200], Train Loss: 0.0279, Val Loss: 0.0282\n",
      "Epoch [38/200], Train Loss: 0.0278, Val Loss: 0.0283\n",
      "Epoch [39/200], Train Loss: 0.0278, Val Loss: 0.0283\n",
      "Epoch [40/200], Train Loss: 0.0278, Val Loss: 0.0282\n",
      "Epoch [41/200], Train Loss: 0.0278, Val Loss: 0.0283\n",
      "Epoch [42/200], Train Loss: 0.0278, Val Loss: 0.0282\n",
      "Epoch [43/200], Train Loss: 0.0278, Val Loss: 0.0282\n",
      "Epoch [44/200], Train Loss: 0.0278, Val Loss: 0.0282\n",
      "Epoch [45/200], Train Loss: 0.0277, Val Loss: 0.0282\n",
      "Epoch [46/200], Train Loss: 0.0277, Val Loss: 0.0282\n",
      "Epoch [47/200], Train Loss: 0.0277, Val Loss: 0.0282\n",
      "Epoch [48/200], Train Loss: 0.0277, Val Loss: 0.0282\n",
      "Epoch [49/200], Train Loss: 0.0277, Val Loss: 0.0282\n",
      "Epoch [50/200], Train Loss: 0.0277, Val Loss: 0.0282\n",
      "Early stopping at epoch 50\n",
      "Reconstruction Error: 0.027815207839012146\n",
      "Epoch [1/200], Train Loss: 0.0333, Val Loss: 0.0300\n",
      "Epoch [2/200], Train Loss: 0.0292, Val Loss: 0.0283\n",
      "Epoch [3/200], Train Loss: 0.0282, Val Loss: 0.0281\n",
      "Epoch [4/200], Train Loss: 0.0280, Val Loss: 0.0279\n",
      "Epoch [5/200], Train Loss: 0.0279, Val Loss: 0.0278\n",
      "Epoch [6/200], Train Loss: 0.0277, Val Loss: 0.0277\n",
      "Epoch [7/200], Train Loss: 0.0276, Val Loss: 0.0276\n",
      "Epoch [8/200], Train Loss: 0.0275, Val Loss: 0.0276\n",
      "Epoch [9/200], Train Loss: 0.0274, Val Loss: 0.0275\n",
      "Epoch [10/200], Train Loss: 0.0274, Val Loss: 0.0275\n",
      "Epoch [11/200], Train Loss: 0.0273, Val Loss: 0.0275\n",
      "Epoch [12/200], Train Loss: 0.0272, Val Loss: 0.0274\n",
      "Epoch [13/200], Train Loss: 0.0272, Val Loss: 0.0274\n",
      "Epoch [14/200], Train Loss: 0.0272, Val Loss: 0.0274\n",
      "Epoch [15/200], Train Loss: 0.0271, Val Loss: 0.0273\n",
      "Epoch [16/200], Train Loss: 0.0271, Val Loss: 0.0273\n",
      "Epoch [17/200], Train Loss: 0.0270, Val Loss: 0.0273\n",
      "Epoch [18/200], Train Loss: 0.0270, Val Loss: 0.0273\n",
      "Epoch [19/200], Train Loss: 0.0269, Val Loss: 0.0273\n",
      "Epoch [20/200], Train Loss: 0.0269, Val Loss: 0.0273\n",
      "Epoch [21/200], Train Loss: 0.0268, Val Loss: 0.0273\n",
      "Epoch [22/200], Train Loss: 0.0268, Val Loss: 0.0272\n",
      "Epoch [23/200], Train Loss: 0.0268, Val Loss: 0.0272\n",
      "Epoch [24/200], Train Loss: 0.0268, Val Loss: 0.0272\n",
      "Epoch [25/200], Train Loss: 0.0267, Val Loss: 0.0272\n",
      "Epoch [26/200], Train Loss: 0.0267, Val Loss: 0.0272\n",
      "Epoch [27/200], Train Loss: 0.0267, Val Loss: 0.0272\n",
      "Epoch [28/200], Train Loss: 0.0266, Val Loss: 0.0272\n",
      "Epoch [29/200], Train Loss: 0.0266, Val Loss: 0.0272\n",
      "Epoch [30/200], Train Loss: 0.0266, Val Loss: 0.0271\n",
      "Epoch [31/200], Train Loss: 0.0266, Val Loss: 0.0272\n",
      "Epoch [32/200], Train Loss: 0.0265, Val Loss: 0.0271\n",
      "Epoch [33/200], Train Loss: 0.0265, Val Loss: 0.0272\n",
      "Epoch [34/200], Train Loss: 0.0265, Val Loss: 0.0271\n",
      "Epoch [35/200], Train Loss: 0.0265, Val Loss: 0.0271\n",
      "Epoch [36/200], Train Loss: 0.0265, Val Loss: 0.0272\n",
      "Epoch [37/200], Train Loss: 0.0264, Val Loss: 0.0272\n",
      "Epoch [38/200], Train Loss: 0.0264, Val Loss: 0.0271\n",
      "Epoch [39/200], Train Loss: 0.0264, Val Loss: 0.0272\n",
      "Epoch [40/200], Train Loss: 0.0264, Val Loss: 0.0272\n",
      "Early stopping at epoch 40\n",
      "Reconstruction Error: 0.026459017768502235\n",
      "Epoch [1/200], Train Loss: 0.0334, Val Loss: 0.0301\n",
      "Epoch [2/200], Train Loss: 0.0282, Val Loss: 0.0274\n",
      "Epoch [3/200], Train Loss: 0.0271, Val Loss: 0.0270\n",
      "Epoch [4/200], Train Loss: 0.0268, Val Loss: 0.0268\n",
      "Epoch [5/200], Train Loss: 0.0267, Val Loss: 0.0267\n",
      "Epoch [6/200], Train Loss: 0.0266, Val Loss: 0.0266\n",
      "Epoch [7/200], Train Loss: 0.0264, Val Loss: 0.0265\n",
      "Epoch [8/200], Train Loss: 0.0264, Val Loss: 0.0264\n",
      "Epoch [9/200], Train Loss: 0.0263, Val Loss: 0.0265\n",
      "Epoch [10/200], Train Loss: 0.0262, Val Loss: 0.0264\n",
      "Epoch [11/200], Train Loss: 0.0261, Val Loss: 0.0264\n",
      "Epoch [12/200], Train Loss: 0.0260, Val Loss: 0.0263\n",
      "Epoch [13/200], Train Loss: 0.0260, Val Loss: 0.0263\n",
      "Epoch [14/200], Train Loss: 0.0259, Val Loss: 0.0262\n",
      "Epoch [15/200], Train Loss: 0.0259, Val Loss: 0.0262\n",
      "Epoch [16/200], Train Loss: 0.0258, Val Loss: 0.0262\n",
      "Epoch [17/200], Train Loss: 0.0258, Val Loss: 0.0262\n",
      "Epoch [18/200], Train Loss: 0.0257, Val Loss: 0.0261\n",
      "Epoch [19/200], Train Loss: 0.0257, Val Loss: 0.0262\n",
      "Epoch [20/200], Train Loss: 0.0257, Val Loss: 0.0261\n",
      "Epoch [21/200], Train Loss: 0.0256, Val Loss: 0.0261\n",
      "Epoch [22/200], Train Loss: 0.0256, Val Loss: 0.0261\n",
      "Epoch [23/200], Train Loss: 0.0256, Val Loss: 0.0261\n",
      "Epoch [24/200], Train Loss: 0.0256, Val Loss: 0.0261\n",
      "Epoch [25/200], Train Loss: 0.0255, Val Loss: 0.0260\n",
      "Epoch [26/200], Train Loss: 0.0255, Val Loss: 0.0260\n",
      "Epoch [27/200], Train Loss: 0.0255, Val Loss: 0.0260\n",
      "Epoch [28/200], Train Loss: 0.0254, Val Loss: 0.0260\n",
      "Epoch [29/200], Train Loss: 0.0254, Val Loss: 0.0261\n",
      "Epoch [30/200], Train Loss: 0.0254, Val Loss: 0.0260\n",
      "Epoch [31/200], Train Loss: 0.0254, Val Loss: 0.0260\n",
      "Epoch [32/200], Train Loss: 0.0254, Val Loss: 0.0260\n",
      "Epoch [33/200], Train Loss: 0.0253, Val Loss: 0.0259\n",
      "Epoch [34/200], Train Loss: 0.0253, Val Loss: 0.0260\n",
      "Epoch [35/200], Train Loss: 0.0253, Val Loss: 0.0260\n",
      "Epoch [36/200], Train Loss: 0.0253, Val Loss: 0.0259\n",
      "Epoch [37/200], Train Loss: 0.0253, Val Loss: 0.0260\n",
      "Epoch [38/200], Train Loss: 0.0252, Val Loss: 0.0260\n",
      "Epoch [39/200], Train Loss: 0.0252, Val Loss: 0.0259\n",
      "Epoch [40/200], Train Loss: 0.0252, Val Loss: 0.0260\n",
      "Epoch [41/200], Train Loss: 0.0252, Val Loss: 0.0259\n",
      "Epoch [42/200], Train Loss: 0.0252, Val Loss: 0.0259\n",
      "Epoch [43/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [44/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [45/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [46/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [47/200], Train Loss: 0.0251, Val Loss: 0.0260\n",
      "Epoch [48/200], Train Loss: 0.0251, Val Loss: 0.0259\n",
      "Epoch [49/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [50/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [51/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [52/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [53/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [54/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [55/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [56/200], Train Loss: 0.0250, Val Loss: 0.0259\n",
      "Epoch [57/200], Train Loss: 0.0249, Val Loss: 0.0259\n",
      "Epoch [58/200], Train Loss: 0.0249, Val Loss: 0.0259\n",
      "Epoch [59/200], Train Loss: 0.0249, Val Loss: 0.0259\n",
      "Epoch [60/200], Train Loss: 0.0249, Val Loss: 0.0258\n",
      "Epoch [61/200], Train Loss: 0.0248, Val Loss: 0.0258\n",
      "Epoch [62/200], Train Loss: 0.0248, Val Loss: 0.0258\n",
      "Epoch [63/200], Train Loss: 0.0248, Val Loss: 0.0259\n",
      "Epoch [64/200], Train Loss: 0.0248, Val Loss: 0.0259\n",
      "Epoch [65/200], Train Loss: 0.0248, Val Loss: 0.0258\n",
      "Epoch [66/200], Train Loss: 0.0248, Val Loss: 0.0259\n",
      "Epoch [67/200], Train Loss: 0.0248, Val Loss: 0.0258\n",
      "Epoch [68/200], Train Loss: 0.0248, Val Loss: 0.0259\n",
      "Epoch [69/200], Train Loss: 0.0248, Val Loss: 0.0259\n",
      "Epoch [70/200], Train Loss: 0.0247, Val Loss: 0.0258\n",
      "Epoch [71/200], Train Loss: 0.0247, Val Loss: 0.0258\n",
      "Epoch [72/200], Train Loss: 0.0247, Val Loss: 0.0259\n",
      "Epoch [73/200], Train Loss: 0.0247, Val Loss: 0.0259\n",
      "Epoch [74/200], Train Loss: 0.0247, Val Loss: 0.0258\n",
      "Epoch [75/200], Train Loss: 0.0247, Val Loss: 0.0258\n",
      "Epoch [76/200], Train Loss: 0.0247, Val Loss: 0.0259\n",
      "Epoch [77/200], Train Loss: 0.0247, Val Loss: 0.0259\n",
      "Epoch [78/200], Train Loss: 0.0246, Val Loss: 0.0259\n",
      "Epoch [79/200], Train Loss: 0.0246, Val Loss: 0.0258\n",
      "Epoch [80/200], Train Loss: 0.0246, Val Loss: 0.0258\n",
      "Epoch [81/200], Train Loss: 0.0247, Val Loss: 0.0258\n",
      "Early stopping at epoch 81\n",
      "Reconstruction Error: 0.02479897066950798\n",
      "Epoch [1/200], Train Loss: 0.0337, Val Loss: 0.0293\n",
      "Epoch [2/200], Train Loss: 0.0282, Val Loss: 0.0272\n",
      "Epoch [3/200], Train Loss: 0.0266, Val Loss: 0.0263\n",
      "Epoch [4/200], Train Loss: 0.0262, Val Loss: 0.0262\n",
      "Epoch [5/200], Train Loss: 0.0260, Val Loss: 0.0260\n",
      "Epoch [6/200], Train Loss: 0.0259, Val Loss: 0.0259\n",
      "Epoch [7/200], Train Loss: 0.0257, Val Loss: 0.0258\n",
      "Epoch [8/200], Train Loss: 0.0256, Val Loss: 0.0258\n",
      "Epoch [9/200], Train Loss: 0.0255, Val Loss: 0.0256\n",
      "Epoch [10/200], Train Loss: 0.0254, Val Loss: 0.0256\n",
      "Epoch [11/200], Train Loss: 0.0254, Val Loss: 0.0256\n",
      "Epoch [12/200], Train Loss: 0.0253, Val Loss: 0.0255\n",
      "Epoch [13/200], Train Loss: 0.0252, Val Loss: 0.0254\n",
      "Epoch [14/200], Train Loss: 0.0252, Val Loss: 0.0254\n",
      "Epoch [15/200], Train Loss: 0.0251, Val Loss: 0.0253\n",
      "Epoch [16/200], Train Loss: 0.0251, Val Loss: 0.0253\n",
      "Epoch [17/200], Train Loss: 0.0250, Val Loss: 0.0253\n",
      "Epoch [18/200], Train Loss: 0.0250, Val Loss: 0.0252\n",
      "Epoch [19/200], Train Loss: 0.0249, Val Loss: 0.0252\n",
      "Epoch [20/200], Train Loss: 0.0249, Val Loss: 0.0252\n",
      "Epoch [21/200], Train Loss: 0.0249, Val Loss: 0.0251\n",
      "Epoch [22/200], Train Loss: 0.0248, Val Loss: 0.0251\n",
      "Epoch [23/200], Train Loss: 0.0248, Val Loss: 0.0251\n",
      "Epoch [24/200], Train Loss: 0.0248, Val Loss: 0.0251\n",
      "Epoch [25/200], Train Loss: 0.0247, Val Loss: 0.0251\n",
      "Epoch [26/200], Train Loss: 0.0247, Val Loss: 0.0251\n",
      "Epoch [27/200], Train Loss: 0.0247, Val Loss: 0.0250\n",
      "Epoch [28/200], Train Loss: 0.0246, Val Loss: 0.0251\n",
      "Epoch [29/200], Train Loss: 0.0246, Val Loss: 0.0250\n",
      "Epoch [30/200], Train Loss: 0.0246, Val Loss: 0.0250\n",
      "Epoch [31/200], Train Loss: 0.0245, Val Loss: 0.0249\n",
      "Epoch [32/200], Train Loss: 0.0245, Val Loss: 0.0249\n",
      "Epoch [33/200], Train Loss: 0.0245, Val Loss: 0.0249\n",
      "Epoch [34/200], Train Loss: 0.0244, Val Loss: 0.0249\n",
      "Epoch [35/200], Train Loss: 0.0244, Val Loss: 0.0248\n",
      "Epoch [36/200], Train Loss: 0.0243, Val Loss: 0.0248\n",
      "Epoch [37/200], Train Loss: 0.0243, Val Loss: 0.0248\n",
      "Epoch [38/200], Train Loss: 0.0243, Val Loss: 0.0248\n",
      "Epoch [39/200], Train Loss: 0.0242, Val Loss: 0.0247\n",
      "Epoch [40/200], Train Loss: 0.0242, Val Loss: 0.0247\n",
      "Epoch [41/200], Train Loss: 0.0242, Val Loss: 0.0247\n",
      "Epoch [42/200], Train Loss: 0.0242, Val Loss: 0.0247\n",
      "Epoch [43/200], Train Loss: 0.0241, Val Loss: 0.0247\n",
      "Epoch [44/200], Train Loss: 0.0241, Val Loss: 0.0246\n",
      "Epoch [45/200], Train Loss: 0.0241, Val Loss: 0.0247\n",
      "Epoch [46/200], Train Loss: 0.0241, Val Loss: 0.0247\n",
      "Epoch [47/200], Train Loss: 0.0241, Val Loss: 0.0246\n",
      "Epoch [48/200], Train Loss: 0.0240, Val Loss: 0.0246\n",
      "Epoch [49/200], Train Loss: 0.0240, Val Loss: 0.0247\n",
      "Epoch [50/200], Train Loss: 0.0240, Val Loss: 0.0246\n",
      "Epoch [51/200], Train Loss: 0.0240, Val Loss: 0.0246\n",
      "Epoch [52/200], Train Loss: 0.0240, Val Loss: 0.0246\n",
      "Epoch [53/200], Train Loss: 0.0239, Val Loss: 0.0245\n",
      "Epoch [54/200], Train Loss: 0.0239, Val Loss: 0.0246\n",
      "Epoch [55/200], Train Loss: 0.0239, Val Loss: 0.0246\n",
      "Epoch [56/200], Train Loss: 0.0239, Val Loss: 0.0246\n",
      "Epoch [57/200], Train Loss: 0.0239, Val Loss: 0.0246\n",
      "Epoch [58/200], Train Loss: 0.0239, Val Loss: 0.0246\n",
      "Epoch [59/200], Train Loss: 0.0239, Val Loss: 0.0246\n",
      "Epoch [60/200], Train Loss: 0.0238, Val Loss: 0.0245\n",
      "Epoch [61/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [62/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [63/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [64/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [65/200], Train Loss: 0.0238, Val Loss: 0.0246\n",
      "Epoch [66/200], Train Loss: 0.0237, Val Loss: 0.0245\n",
      "Epoch [67/200], Train Loss: 0.0237, Val Loss: 0.0246\n",
      "Epoch [68/200], Train Loss: 0.0237, Val Loss: 0.0245\n",
      "Epoch [69/200], Train Loss: 0.0237, Val Loss: 0.0245\n",
      "Epoch [70/200], Train Loss: 0.0237, Val Loss: 0.0245\n",
      "Early stopping at epoch 70\n",
      "Reconstruction Error: 0.02376101352274418\n",
      "Epoch [1/200], Train Loss: 0.0337, Val Loss: 0.0289\n",
      "Epoch [2/200], Train Loss: 0.0274, Val Loss: 0.0267\n",
      "Epoch [3/200], Train Loss: 0.0260, Val Loss: 0.0256\n",
      "Epoch [4/200], Train Loss: 0.0254, Val Loss: 0.0253\n",
      "Epoch [5/200], Train Loss: 0.0252, Val Loss: 0.0252\n",
      "Epoch [6/200], Train Loss: 0.0250, Val Loss: 0.0250\n",
      "Epoch [7/200], Train Loss: 0.0248, Val Loss: 0.0250\n",
      "Epoch [8/200], Train Loss: 0.0247, Val Loss: 0.0248\n",
      "Epoch [9/200], Train Loss: 0.0246, Val Loss: 0.0248\n",
      "Epoch [10/200], Train Loss: 0.0245, Val Loss: 0.0247\n",
      "Epoch [11/200], Train Loss: 0.0244, Val Loss: 0.0246\n",
      "Epoch [12/200], Train Loss: 0.0244, Val Loss: 0.0246\n",
      "Epoch [13/200], Train Loss: 0.0243, Val Loss: 0.0246\n",
      "Epoch [14/200], Train Loss: 0.0242, Val Loss: 0.0245\n",
      "Epoch [15/200], Train Loss: 0.0241, Val Loss: 0.0245\n",
      "Epoch [16/200], Train Loss: 0.0241, Val Loss: 0.0244\n",
      "Epoch [17/200], Train Loss: 0.0240, Val Loss: 0.0244\n",
      "Epoch [18/200], Train Loss: 0.0240, Val Loss: 0.0244\n",
      "Epoch [19/200], Train Loss: 0.0240, Val Loss: 0.0243\n",
      "Epoch [20/200], Train Loss: 0.0239, Val Loss: 0.0243\n",
      "Epoch [21/200], Train Loss: 0.0239, Val Loss: 0.0243\n",
      "Epoch [22/200], Train Loss: 0.0238, Val Loss: 0.0242\n",
      "Epoch [23/200], Train Loss: 0.0238, Val Loss: 0.0242\n",
      "Epoch [24/200], Train Loss: 0.0238, Val Loss: 0.0242\n",
      "Epoch [25/200], Train Loss: 0.0237, Val Loss: 0.0242\n",
      "Epoch [26/200], Train Loss: 0.0237, Val Loss: 0.0241\n",
      "Epoch [27/200], Train Loss: 0.0237, Val Loss: 0.0241\n",
      "Epoch [28/200], Train Loss: 0.0236, Val Loss: 0.0241\n",
      "Epoch [29/200], Train Loss: 0.0236, Val Loss: 0.0241\n",
      "Epoch [30/200], Train Loss: 0.0236, Val Loss: 0.0241\n",
      "Epoch [31/200], Train Loss: 0.0235, Val Loss: 0.0241\n",
      "Epoch [32/200], Train Loss: 0.0235, Val Loss: 0.0241\n",
      "Epoch [33/200], Train Loss: 0.0235, Val Loss: 0.0241\n",
      "Epoch [34/200], Train Loss: 0.0235, Val Loss: 0.0240\n",
      "Epoch [35/200], Train Loss: 0.0234, Val Loss: 0.0241\n",
      "Epoch [36/200], Train Loss: 0.0234, Val Loss: 0.0240\n",
      "Epoch [37/200], Train Loss: 0.0234, Val Loss: 0.0240\n",
      "Epoch [38/200], Train Loss: 0.0234, Val Loss: 0.0240\n",
      "Epoch [39/200], Train Loss: 0.0233, Val Loss: 0.0239\n",
      "Epoch [40/200], Train Loss: 0.0234, Val Loss: 0.0239\n",
      "Epoch [41/200], Train Loss: 0.0233, Val Loss: 0.0239\n",
      "Epoch [42/200], Train Loss: 0.0233, Val Loss: 0.0240\n",
      "Epoch [43/200], Train Loss: 0.0233, Val Loss: 0.0240\n",
      "Epoch [44/200], Train Loss: 0.0233, Val Loss: 0.0239\n",
      "Epoch [45/200], Train Loss: 0.0232, Val Loss: 0.0240\n",
      "Epoch [46/200], Train Loss: 0.0232, Val Loss: 0.0239\n",
      "Epoch [47/200], Train Loss: 0.0232, Val Loss: 0.0239\n",
      "Epoch [48/200], Train Loss: 0.0232, Val Loss: 0.0239\n",
      "Epoch [49/200], Train Loss: 0.0232, Val Loss: 0.0239\n",
      "Epoch [50/200], Train Loss: 0.0232, Val Loss: 0.0239\n",
      "Epoch [51/200], Train Loss: 0.0231, Val Loss: 0.0239\n",
      "Epoch [52/200], Train Loss: 0.0231, Val Loss: 0.0239\n",
      "Epoch [53/200], Train Loss: 0.0231, Val Loss: 0.0239\n",
      "Epoch [54/200], Train Loss: 0.0231, Val Loss: 0.0239\n",
      "Epoch [55/200], Train Loss: 0.0231, Val Loss: 0.0239\n",
      "Epoch [56/200], Train Loss: 0.0231, Val Loss: 0.0238\n",
      "Epoch [57/200], Train Loss: 0.0231, Val Loss: 0.0239\n",
      "Epoch [58/200], Train Loss: 0.0230, Val Loss: 0.0238\n",
      "Epoch [59/200], Train Loss: 0.0230, Val Loss: 0.0239\n",
      "Epoch [60/200], Train Loss: 0.0230, Val Loss: 0.0239\n",
      "Epoch [61/200], Train Loss: 0.0230, Val Loss: 0.0238\n",
      "Epoch [62/200], Train Loss: 0.0230, Val Loss: 0.0239\n",
      "Epoch [63/200], Train Loss: 0.0230, Val Loss: 0.0239\n",
      "Epoch [64/200], Train Loss: 0.0230, Val Loss: 0.0238\n",
      "Epoch [65/200], Train Loss: 0.0230, Val Loss: 0.0238\n",
      "Epoch [66/200], Train Loss: 0.0229, Val Loss: 0.0239\n",
      "Epoch [67/200], Train Loss: 0.0229, Val Loss: 0.0238\n",
      "Epoch [68/200], Train Loss: 0.0229, Val Loss: 0.0239\n",
      "Epoch [69/200], Train Loss: 0.0229, Val Loss: 0.0238\n",
      "Epoch [70/200], Train Loss: 0.0229, Val Loss: 0.0238\n",
      "Epoch [71/200], Train Loss: 0.0229, Val Loss: 0.0238\n",
      "Epoch [72/200], Train Loss: 0.0229, Val Loss: 0.0239\n",
      "Epoch [73/200], Train Loss: 0.0229, Val Loss: 0.0238\n",
      "Epoch [74/200], Train Loss: 0.0229, Val Loss: 0.0238\n",
      "Epoch [75/200], Train Loss: 0.0228, Val Loss: 0.0238\n",
      "Epoch [76/200], Train Loss: 0.0228, Val Loss: 0.0239\n",
      "Epoch [77/200], Train Loss: 0.0228, Val Loss: 0.0238\n",
      "Epoch [78/200], Train Loss: 0.0228, Val Loss: 0.0239\n",
      "Epoch [79/200], Train Loss: 0.0228, Val Loss: 0.0238\n",
      "Epoch [80/200], Train Loss: 0.0228, Val Loss: 0.0238\n",
      "Epoch [81/200], Train Loss: 0.0228, Val Loss: 0.0238\n",
      "Epoch [82/200], Train Loss: 0.0228, Val Loss: 0.0238\n",
      "Epoch [83/200], Train Loss: 0.0228, Val Loss: 0.0238\n",
      "Epoch [84/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [85/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [86/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [87/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [88/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [89/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [90/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [91/200], Train Loss: 0.0227, Val Loss: 0.0237\n",
      "Epoch [92/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [93/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [94/200], Train Loss: 0.0227, Val Loss: 0.0237\n",
      "Epoch [95/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [96/200], Train Loss: 0.0227, Val Loss: 0.0238\n",
      "Epoch [97/200], Train Loss: 0.0226, Val Loss: 0.0237\n",
      "Epoch [98/200], Train Loss: 0.0226, Val Loss: 0.0238\n",
      "Epoch [99/200], Train Loss: 0.0226, Val Loss: 0.0238\n",
      "Epoch [100/200], Train Loss: 0.0226, Val Loss: 0.0237\n",
      "Epoch [101/200], Train Loss: 0.0226, Val Loss: 0.0237\n",
      "Epoch [102/200], Train Loss: 0.0226, Val Loss: 0.0237\n",
      "Epoch [103/200], Train Loss: 0.0226, Val Loss: 0.0238\n",
      "Epoch [104/200], Train Loss: 0.0226, Val Loss: 0.0238\n",
      "Early stopping at epoch 104\n",
      "Reconstruction Error: 0.022709418088197708\n",
      "Epoch [1/200], Train Loss: 0.0336, Val Loss: 0.0286\n",
      "Epoch [2/200], Train Loss: 0.0268, Val Loss: 0.0257\n",
      "Epoch [3/200], Train Loss: 0.0252, Val Loss: 0.0250\n",
      "Epoch [4/200], Train Loss: 0.0248, Val Loss: 0.0248\n",
      "Epoch [5/200], Train Loss: 0.0246, Val Loss: 0.0246\n",
      "Epoch [6/200], Train Loss: 0.0244, Val Loss: 0.0244\n",
      "Epoch [7/200], Train Loss: 0.0242, Val Loss: 0.0243\n",
      "Epoch [8/200], Train Loss: 0.0241, Val Loss: 0.0241\n",
      "Epoch [9/200], Train Loss: 0.0240, Val Loss: 0.0241\n",
      "Epoch [10/200], Train Loss: 0.0239, Val Loss: 0.0240\n",
      "Epoch [11/200], Train Loss: 0.0238, Val Loss: 0.0239\n",
      "Epoch [12/200], Train Loss: 0.0237, Val Loss: 0.0239\n",
      "Epoch [13/200], Train Loss: 0.0236, Val Loss: 0.0238\n",
      "Epoch [14/200], Train Loss: 0.0235, Val Loss: 0.0237\n",
      "Epoch [15/200], Train Loss: 0.0235, Val Loss: 0.0237\n",
      "Epoch [16/200], Train Loss: 0.0234, Val Loss: 0.0237\n",
      "Epoch [17/200], Train Loss: 0.0234, Val Loss: 0.0237\n",
      "Epoch [18/200], Train Loss: 0.0233, Val Loss: 0.0237\n",
      "Epoch [19/200], Train Loss: 0.0233, Val Loss: 0.0236\n",
      "Epoch [20/200], Train Loss: 0.0232, Val Loss: 0.0236\n",
      "Epoch [21/200], Train Loss: 0.0232, Val Loss: 0.0235\n",
      "Epoch [22/200], Train Loss: 0.0231, Val Loss: 0.0235\n",
      "Epoch [23/200], Train Loss: 0.0231, Val Loss: 0.0235\n",
      "Epoch [24/200], Train Loss: 0.0230, Val Loss: 0.0234\n",
      "Epoch [25/200], Train Loss: 0.0230, Val Loss: 0.0234\n",
      "Epoch [26/200], Train Loss: 0.0230, Val Loss: 0.0234\n",
      "Epoch [27/200], Train Loss: 0.0229, Val Loss: 0.0234\n",
      "Epoch [28/200], Train Loss: 0.0229, Val Loss: 0.0234\n",
      "Epoch [29/200], Train Loss: 0.0229, Val Loss: 0.0233\n",
      "Epoch [30/200], Train Loss: 0.0228, Val Loss: 0.0233\n",
      "Epoch [31/200], Train Loss: 0.0228, Val Loss: 0.0233\n",
      "Epoch [32/200], Train Loss: 0.0228, Val Loss: 0.0233\n",
      "Epoch [33/200], Train Loss: 0.0227, Val Loss: 0.0232\n",
      "Epoch [34/200], Train Loss: 0.0227, Val Loss: 0.0232\n",
      "Epoch [35/200], Train Loss: 0.0227, Val Loss: 0.0232\n",
      "Epoch [36/200], Train Loss: 0.0227, Val Loss: 0.0232\n",
      "Epoch [37/200], Train Loss: 0.0227, Val Loss: 0.0232\n",
      "Epoch [38/200], Train Loss: 0.0226, Val Loss: 0.0232\n",
      "Epoch [39/200], Train Loss: 0.0226, Val Loss: 0.0232\n",
      "Epoch [40/200], Train Loss: 0.0226, Val Loss: 0.0232\n",
      "Epoch [41/200], Train Loss: 0.0226, Val Loss: 0.0231\n",
      "Epoch [42/200], Train Loss: 0.0226, Val Loss: 0.0231\n",
      "Epoch [43/200], Train Loss: 0.0225, Val Loss: 0.0232\n",
      "Epoch [44/200], Train Loss: 0.0225, Val Loss: 0.0231\n",
      "Epoch [45/200], Train Loss: 0.0225, Val Loss: 0.0231\n",
      "Epoch [46/200], Train Loss: 0.0225, Val Loss: 0.0231\n",
      "Epoch [47/200], Train Loss: 0.0225, Val Loss: 0.0232\n",
      "Epoch [48/200], Train Loss: 0.0225, Val Loss: 0.0232\n",
      "Epoch [49/200], Train Loss: 0.0225, Val Loss: 0.0231\n",
      "Epoch [50/200], Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Epoch [51/200], Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Epoch [52/200], Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Epoch [53/200], Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Epoch [54/200], Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Epoch [55/200], Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Epoch [56/200], Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Epoch [57/200], Train Loss: 0.0224, Val Loss: 0.0232\n",
      "Epoch [58/200], Train Loss: 0.0224, Val Loss: 0.0231\n",
      "Epoch [59/200], Train Loss: 0.0224, Val Loss: 0.0230\n",
      "Epoch [60/200], Train Loss: 0.0223, Val Loss: 0.0231\n",
      "Epoch [61/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [62/200], Train Loss: 0.0223, Val Loss: 0.0231\n",
      "Epoch [63/200], Train Loss: 0.0223, Val Loss: 0.0231\n",
      "Epoch [64/200], Train Loss: 0.0223, Val Loss: 0.0231\n",
      "Epoch [65/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [66/200], Train Loss: 0.0223, Val Loss: 0.0231\n",
      "Epoch [67/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [68/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [69/200], Train Loss: 0.0223, Val Loss: 0.0230\n",
      "Epoch [70/200], Train Loss: 0.0223, Val Loss: 0.0231\n",
      "Epoch [71/200], Train Loss: 0.0222, Val Loss: 0.0231\n",
      "Epoch [72/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [73/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [74/200], Train Loss: 0.0222, Val Loss: 0.0231\n",
      "Epoch [75/200], Train Loss: 0.0222, Val Loss: 0.0231\n",
      "Epoch [76/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [77/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [78/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [79/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [80/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [81/200], Train Loss: 0.0222, Val Loss: 0.0231\n",
      "Epoch [82/200], Train Loss: 0.0222, Val Loss: 0.0230\n",
      "Epoch [83/200], Train Loss: 0.0221, Val Loss: 0.0231\n",
      "Epoch [84/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [85/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [86/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [87/200], Train Loss: 0.0221, Val Loss: 0.0231\n",
      "Epoch [88/200], Train Loss: 0.0221, Val Loss: 0.0231\n",
      "Epoch [89/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [90/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [91/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [92/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [93/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [94/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [95/200], Train Loss: 0.0221, Val Loss: 0.0230\n",
      "Epoch [96/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [97/200], Train Loss: 0.0220, Val Loss: 0.0231\n",
      "Epoch [98/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [99/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [100/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [101/200], Train Loss: 0.0220, Val Loss: 0.0231\n",
      "Epoch [102/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [103/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [104/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [105/200], Train Loss: 0.0220, Val Loss: 0.0229\n",
      "Epoch [106/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [107/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [108/200], Train Loss: 0.0220, Val Loss: 0.0230\n",
      "Epoch [109/200], Train Loss: 0.0219, Val Loss: 0.0230\n",
      "Epoch [110/200], Train Loss: 0.0219, Val Loss: 0.0229\n",
      "Epoch [111/200], Train Loss: 0.0219, Val Loss: 0.0229\n",
      "Epoch [112/200], Train Loss: 0.0219, Val Loss: 0.0229\n",
      "Epoch [113/200], Train Loss: 0.0219, Val Loss: 0.0230\n",
      "Epoch [114/200], Train Loss: 0.0219, Val Loss: 0.0230\n",
      "Epoch [115/200], Train Loss: 0.0219, Val Loss: 0.0229\n",
      "Epoch [116/200], Train Loss: 0.0219, Val Loss: 0.0230\n",
      "Epoch [117/200], Train Loss: 0.0219, Val Loss: 0.0229\n",
      "Epoch [118/200], Train Loss: 0.0219, Val Loss: 0.0230\n",
      "Epoch [119/200], Train Loss: 0.0219, Val Loss: 0.0229\n",
      "Epoch [120/200], Train Loss: 0.0219, Val Loss: 0.0229\n",
      "Epoch [121/200], Train Loss: 0.0219, Val Loss: 0.0230\n",
      "Epoch [122/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Epoch [123/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Epoch [124/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Epoch [125/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Epoch [126/200], Train Loss: 0.0218, Val Loss: 0.0230\n",
      "Epoch [127/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Epoch [128/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Epoch [129/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Epoch [130/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Epoch [131/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Epoch [132/200], Train Loss: 0.0218, Val Loss: 0.0229\n",
      "Early stopping at epoch 132\n",
      "Reconstruction Error: 0.02192023955285549\n",
      "Epoch [1/200], Train Loss: 0.0331, Val Loss: 0.0286\n",
      "Epoch [2/200], Train Loss: 0.0270, Val Loss: 0.0257\n",
      "Epoch [3/200], Train Loss: 0.0249, Val Loss: 0.0245\n",
      "Epoch [4/200], Train Loss: 0.0243, Val Loss: 0.0242\n",
      "Epoch [5/200], Train Loss: 0.0240, Val Loss: 0.0242\n",
      "Epoch [6/200], Train Loss: 0.0239, Val Loss: 0.0239\n",
      "Epoch [7/200], Train Loss: 0.0237, Val Loss: 0.0238\n",
      "Epoch [8/200], Train Loss: 0.0236, Val Loss: 0.0236\n",
      "Epoch [9/200], Train Loss: 0.0235, Val Loss: 0.0236\n",
      "Epoch [10/200], Train Loss: 0.0234, Val Loss: 0.0235\n",
      "Epoch [11/200], Train Loss: 0.0233, Val Loss: 0.0235\n",
      "Epoch [12/200], Train Loss: 0.0232, Val Loss: 0.0234\n",
      "Epoch [13/200], Train Loss: 0.0232, Val Loss: 0.0233\n",
      "Epoch [14/200], Train Loss: 0.0231, Val Loss: 0.0233\n",
      "Epoch [15/200], Train Loss: 0.0231, Val Loss: 0.0233\n",
      "Epoch [16/200], Train Loss: 0.0230, Val Loss: 0.0232\n",
      "Epoch [17/200], Train Loss: 0.0230, Val Loss: 0.0232\n",
      "Epoch [18/200], Train Loss: 0.0229, Val Loss: 0.0232\n",
      "Epoch [19/200], Train Loss: 0.0229, Val Loss: 0.0231\n",
      "Epoch [20/200], Train Loss: 0.0228, Val Loss: 0.0231\n",
      "Epoch [21/200], Train Loss: 0.0228, Val Loss: 0.0231\n",
      "Epoch [22/200], Train Loss: 0.0228, Val Loss: 0.0230\n",
      "Epoch [23/200], Train Loss: 0.0227, Val Loss: 0.0230\n",
      "Epoch [24/200], Train Loss: 0.0227, Val Loss: 0.0230\n",
      "Epoch [25/200], Train Loss: 0.0227, Val Loss: 0.0230\n",
      "Epoch [26/200], Train Loss: 0.0226, Val Loss: 0.0229\n",
      "Epoch [27/200], Train Loss: 0.0226, Val Loss: 0.0229\n",
      "Epoch [28/200], Train Loss: 0.0226, Val Loss: 0.0229\n",
      "Epoch [29/200], Train Loss: 0.0225, Val Loss: 0.0229\n",
      "Epoch [30/200], Train Loss: 0.0225, Val Loss: 0.0228\n",
      "Epoch [31/200], Train Loss: 0.0225, Val Loss: 0.0228\n",
      "Epoch [32/200], Train Loss: 0.0225, Val Loss: 0.0228\n",
      "Epoch [33/200], Train Loss: 0.0224, Val Loss: 0.0228\n",
      "Epoch [34/200], Train Loss: 0.0224, Val Loss: 0.0228\n",
      "Epoch [35/200], Train Loss: 0.0224, Val Loss: 0.0228\n",
      "Epoch [36/200], Train Loss: 0.0224, Val Loss: 0.0227\n",
      "Epoch [37/200], Train Loss: 0.0223, Val Loss: 0.0227\n",
      "Epoch [38/200], Train Loss: 0.0223, Val Loss: 0.0227\n",
      "Epoch [39/200], Train Loss: 0.0223, Val Loss: 0.0228\n",
      "Epoch [40/200], Train Loss: 0.0223, Val Loss: 0.0226\n",
      "Epoch [41/200], Train Loss: 0.0222, Val Loss: 0.0227\n",
      "Epoch [42/200], Train Loss: 0.0222, Val Loss: 0.0227\n",
      "Epoch [43/200], Train Loss: 0.0222, Val Loss: 0.0227\n",
      "Epoch [44/200], Train Loss: 0.0222, Val Loss: 0.0227\n",
      "Epoch [45/200], Train Loss: 0.0222, Val Loss: 0.0226\n",
      "Epoch [46/200], Train Loss: 0.0222, Val Loss: 0.0226\n",
      "Epoch [47/200], Train Loss: 0.0221, Val Loss: 0.0225\n",
      "Epoch [48/200], Train Loss: 0.0221, Val Loss: 0.0226\n",
      "Epoch [49/200], Train Loss: 0.0221, Val Loss: 0.0226\n",
      "Epoch [50/200], Train Loss: 0.0221, Val Loss: 0.0226\n",
      "Epoch [51/200], Train Loss: 0.0221, Val Loss: 0.0225\n",
      "Epoch [52/200], Train Loss: 0.0220, Val Loss: 0.0226\n",
      "Epoch [53/200], Train Loss: 0.0220, Val Loss: 0.0225\n",
      "Epoch [54/200], Train Loss: 0.0220, Val Loss: 0.0225\n",
      "Epoch [55/200], Train Loss: 0.0220, Val Loss: 0.0225\n",
      "Epoch [56/200], Train Loss: 0.0220, Val Loss: 0.0225\n",
      "Epoch [57/200], Train Loss: 0.0220, Val Loss: 0.0225\n",
      "Epoch [58/200], Train Loss: 0.0220, Val Loss: 0.0225\n",
      "Epoch [59/200], Train Loss: 0.0220, Val Loss: 0.0225\n",
      "Epoch [60/200], Train Loss: 0.0219, Val Loss: 0.0225\n",
      "Epoch [61/200], Train Loss: 0.0219, Val Loss: 0.0224\n",
      "Epoch [62/200], Train Loss: 0.0219, Val Loss: 0.0225\n",
      "Epoch [63/200], Train Loss: 0.0219, Val Loss: 0.0225\n",
      "Epoch [64/200], Train Loss: 0.0219, Val Loss: 0.0224\n",
      "Epoch [65/200], Train Loss: 0.0219, Val Loss: 0.0224\n",
      "Epoch [66/200], Train Loss: 0.0219, Val Loss: 0.0224\n",
      "Epoch [67/200], Train Loss: 0.0218, Val Loss: 0.0224\n",
      "Epoch [68/200], Train Loss: 0.0219, Val Loss: 0.0224\n",
      "Epoch [69/200], Train Loss: 0.0218, Val Loss: 0.0224\n",
      "Epoch [70/200], Train Loss: 0.0218, Val Loss: 0.0224\n",
      "Epoch [71/200], Train Loss: 0.0218, Val Loss: 0.0224\n",
      "Epoch [72/200], Train Loss: 0.0218, Val Loss: 0.0223\n",
      "Epoch [73/200], Train Loss: 0.0218, Val Loss: 0.0224\n",
      "Epoch [74/200], Train Loss: 0.0218, Val Loss: 0.0224\n",
      "Epoch [75/200], Train Loss: 0.0217, Val Loss: 0.0224\n",
      "Epoch [76/200], Train Loss: 0.0217, Val Loss: 0.0224\n",
      "Epoch [77/200], Train Loss: 0.0217, Val Loss: 0.0224\n",
      "Epoch [78/200], Train Loss: 0.0217, Val Loss: 0.0224\n",
      "Epoch [79/200], Train Loss: 0.0217, Val Loss: 0.0223\n",
      "Epoch [80/200], Train Loss: 0.0217, Val Loss: 0.0223\n",
      "Epoch [81/200], Train Loss: 0.0217, Val Loss: 0.0223\n",
      "Epoch [82/200], Train Loss: 0.0217, Val Loss: 0.0224\n",
      "Epoch [83/200], Train Loss: 0.0217, Val Loss: 0.0223\n",
      "Epoch [84/200], Train Loss: 0.0217, Val Loss: 0.0223\n",
      "Epoch [85/200], Train Loss: 0.0216, Val Loss: 0.0223\n",
      "Epoch [86/200], Train Loss: 0.0216, Val Loss: 0.0223\n",
      "Epoch [87/200], Train Loss: 0.0216, Val Loss: 0.0223\n",
      "Epoch [88/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [89/200], Train Loss: 0.0216, Val Loss: 0.0223\n",
      "Epoch [90/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [91/200], Train Loss: 0.0216, Val Loss: 0.0223\n",
      "Epoch [92/200], Train Loss: 0.0216, Val Loss: 0.0223\n",
      "Epoch [93/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [94/200], Train Loss: 0.0215, Val Loss: 0.0222\n",
      "Epoch [95/200], Train Loss: 0.0215, Val Loss: 0.0223\n",
      "Epoch [96/200], Train Loss: 0.0216, Val Loss: 0.0222\n",
      "Epoch [97/200], Train Loss: 0.0215, Val Loss: 0.0223\n",
      "Epoch [98/200], Train Loss: 0.0215, Val Loss: 0.0223\n",
      "Epoch [99/200], Train Loss: 0.0215, Val Loss: 0.0223\n",
      "Epoch [100/200], Train Loss: 0.0215, Val Loss: 0.0222\n",
      "Epoch [101/200], Train Loss: 0.0215, Val Loss: 0.0222\n",
      "Epoch [102/200], Train Loss: 0.0215, Val Loss: 0.0222\n",
      "Epoch [103/200], Train Loss: 0.0215, Val Loss: 0.0222\n",
      "Epoch [104/200], Train Loss: 0.0215, Val Loss: 0.0223\n",
      "Epoch [105/200], Train Loss: 0.0215, Val Loss: 0.0222\n",
      "Epoch [106/200], Train Loss: 0.0215, Val Loss: 0.0223\n",
      "Epoch [107/200], Train Loss: 0.0214, Val Loss: 0.0222\n",
      "Epoch [108/200], Train Loss: 0.0215, Val Loss: 0.0222\n",
      "Epoch [109/200], Train Loss: 0.0214, Val Loss: 0.0222\n",
      "Epoch [110/200], Train Loss: 0.0214, Val Loss: 0.0223\n",
      "Epoch [111/200], Train Loss: 0.0214, Val Loss: 0.0222\n",
      "Epoch [112/200], Train Loss: 0.0214, Val Loss: 0.0222\n",
      "Epoch [113/200], Train Loss: 0.0214, Val Loss: 0.0222\n",
      "Early stopping at epoch 113\n",
      "Reconstruction Error: 0.021480005234479904\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming your questionnaire items are in columns named 'Q1' to 'Q100'\n",
    "# questionnaire_columns = [col for col in data.columns if col.startswith('Q')]\n",
    "X = qns.iloc[:,2:].values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "class QuestionnaireDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.data[idx]  # Input and target are the same\n",
    "\n",
    "train_dataset = QuestionnaireDataset(X_train)\n",
    "val_dataset = QuestionnaireDataset(X_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Step 2: Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, latent_dim),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, input_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "# latent_dim = 5  # Reduce to 5 dimensions\n",
    "latent_dims = range(2, 9)\n",
    "val_losses = []\n",
    "explained_variance_ratios = []\n",
    "for latent_dim in latent_dims:\n",
    "    model = Autoencoder(input_dim, latent_dim)\n",
    "\n",
    "    # Step 3: Train the autoencoder\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 200\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_features, _ in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_features)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_features.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_features, _ in val_loader:\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_features)\n",
    "                val_loss += loss.item() * batch_features.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "\n",
    "\n",
    "        # 检查 Early Stopping 条件\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0  # 重置计数器\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    reconstructed = model(torch.tensor(X_scaled, dtype=torch.float32))\n",
    "    mse = nn.MSELoss(reduction='none')\n",
    "    reconstruction_error = mse(reconstructed, torch.tensor(X_scaled, dtype=torch.float32)).mean(dim=1).detach().numpy()\n",
    "    print(f'Reconstruction Error: {reconstruction_error.mean()}')\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    total_variance = np.var(X_scaled, axis=0).sum()\n",
    "    # 计算每个因子贡献的方差\n",
    "    # 通过重建数据的方差贡献，计算解释率\n",
    "    reconstruction_variance = np.var(reconstructed.detach().numpy(), axis=0).sum()\n",
    "    # 计算方差解释率\n",
    "    explained_variance_ratio = reconstruction_variance / total_variance\n",
    "    explained_variance_ratios.append(explained_variance_ratio)\n",
    "plt.plot(latent_dims, val_losses, marker='o')\n",
    "plt.xlabel('Number of Latent Dimensions')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss vs. Number of Latent Dimensions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(latent_dims, explained_variance_ratios, marker='o')\n",
    "plt.xlabel('Number of Latent Dimensions')\n",
    "plt.ylabel('Varience Explained ratio')\n",
    "plt.title('Varience Explained ratio vs. Number of Latent Dimensions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "avg_var=[]\n",
    "for val, dim in zip(explained_variance_ratios, latent_dims):\n",
    "    avg_var.append(val/dim)\n",
    "\n",
    "plt.plot(latent_dims, avg_var, marker='o')\n",
    "plt.xlabel('Number of Latent Dimensions')\n",
    "plt.ylabel('average Varience Explained ratio')\n",
    "plt.title('average Varience Explained ratio vs. Number of Latent Dimensions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6124903649433933,\n",
       " 0.6124903649433933,\n",
       " 0.6124903649433933,\n",
       " 0.6124903649433933,\n",
       " 0.6124903649433933,\n",
       " 0.6124903649433933,\n",
       " 0.6124903649433933,\n",
       " 0.6124903649433933,\n",
       " 0.6124903649433933]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改自编码器，移除非线性激活函数\n",
    "class LinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(LinearAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Linear(input_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "model = LinearAutoencoder(input_dim, latent_dim)\n",
    "# 获取编码器的权重矩阵\n",
    "weight_matrix = model.encoder.weight.data.numpy()\n",
    "# 对权重矩阵进行奇异值分解\n",
    "U, S, Vt = np.linalg.svd(weight_matrix, full_matrices=False)\n",
    "# S 包含奇异值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, len(S)+1), S, marker='o')\n",
    "plt.xlabel('Latent Dimension Index')\n",
    "plt.ylabel('Singular Value')\n",
    "plt.title('Singular Values of Encoder Weight Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 获取潜在表示\n",
    "latent_features = model.encoder(torch.tensor(X_scaled, dtype=torch.float32)).detach().numpy()\n",
    "# 对潜在表示进行 PCA\n",
    "pca = PCA()\n",
    "pca.fit(latent_features)\n",
    "# 获取解释方差比例\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# 绘制解释方差比例图\n",
    "plt.plot(range(1, len(explained_variance_ratio)+1), explained_variance_ratio, marker='o')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA on Latent Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 21)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# 计算所有样本的重构误差\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(torch.tensor(X_scaled, dtype=torch.float32))\n",
    "    reconstruction_errors = torch.mean((torch.tensor(X_scaled, dtype=torch.float32) - reconstructed) ** 2, dim=1).numpy()\n",
    "\n",
    "sns.histplot(reconstruction_errors, kde=True)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\801679093.py:10: DtypeWarning: Columns (124,128,132,136,140,144,148,152,156,160,164,168,172,176,180,184,188,192,196,200) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  labels = pd.read_csv(r'G:/ABCD/data/mh_p_cbcl.csv')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# load data and drop the first column and the subject id\n",
    "data = pd.read_csv(r\"G:/ABCD/data/mri_y_rsfmr_cor_gp_gp.csv\")\n",
    "# only keep eventname = baseline_year_1_arm_1\n",
    "data = data[data['eventname'] == 'baseline_year_1_arm_1'].drop(columns=['eventname']).dropna()\n",
    "\n",
    "# labels = pd.read_csv(r'G:\\ABCD\\script\\trail\\trail_tsne_RF\\factor analysis\\output\\NA\\EFA.csv').iloc[:, 1:]\n",
    "labels = pd.read_csv(r'G:/ABCD/data/mh_p_cbcl.csv')\n",
    "# only keep the  column.('cbcl_scr_syn_anxdep_t')\n",
    "labels = labels[labels['eventname'] == 'baseline_year_1_arm_1'].drop(columns=['eventname'])\n",
    "labels = labels[['src_subject_id', 'cbcl_scr_syn_anxdep_t']].dropna()\n",
    "\n",
    "# only keep the subjects that have labels\n",
    "data = data[data['src_subject_id'].isin(labels[\"src_subject_id\"])]\n",
    "labels = labels[labels['src_subject_id'].isin(data[\"src_subject_id\"])]\n",
    "\n",
    "# data = data.drop(columns=['src_subject_id'])\n",
    "# labels = labels.drop(columns=['src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>rsfmri_c_ngd_ad_ngd_ad</th>\n",
       "      <th>rsfmri_c_ngd_ad_ngd_cgc</th>\n",
       "      <th>rsfmri_c_ngd_ad_ngd_ca</th>\n",
       "      <th>rsfmri_c_ngd_ad_ngd_dt</th>\n",
       "      <th>rsfmri_c_ngd_ad_ngd_dla</th>\n",
       "      <th>rsfmri_c_ngd_ad_ngd_fo</th>\n",
       "      <th>rsfmri_c_ngd_ad_ngd_n</th>\n",
       "      <th>rsfmri_c_ngd_ad_ngd_rspltp</th>\n",
       "      <th>rsfmri_c_ngd_ad_ngd_sa</th>\n",
       "      <th>...</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_dt</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_dla</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_fo</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_n</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_rspltp</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_sa</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_smh</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_smm</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_vta</th>\n",
       "      <th>rsfmri_c_ngd_vs_ngd_vs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>0.471330</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>-0.076960</td>\n",
       "      <td>-0.116451</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>-0.036302</td>\n",
       "      <td>-0.057183</td>\n",
       "      <td>-0.048132</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088894</td>\n",
       "      <td>0.055355</td>\n",
       "      <td>-0.076806</td>\n",
       "      <td>-0.006004</td>\n",
       "      <td>0.228366</td>\n",
       "      <td>-0.150867</td>\n",
       "      <td>-0.064416</td>\n",
       "      <td>0.098679</td>\n",
       "      <td>-0.155134</td>\n",
       "      <td>0.371960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.116256</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>-0.024781</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.023421</td>\n",
       "      <td>-0.016284</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.056241</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136596</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>-0.111623</td>\n",
       "      <td>-0.077958</td>\n",
       "      <td>0.049159</td>\n",
       "      <td>-0.065420</td>\n",
       "      <td>-0.061459</td>\n",
       "      <td>0.075145</td>\n",
       "      <td>-0.071943</td>\n",
       "      <td>0.346746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>-0.138693</td>\n",
       "      <td>-0.035168</td>\n",
       "      <td>0.044412</td>\n",
       "      <td>-0.041321</td>\n",
       "      <td>-0.185939</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136426</td>\n",
       "      <td>-0.018266</td>\n",
       "      <td>-0.096822</td>\n",
       "      <td>-0.050623</td>\n",
       "      <td>0.002644</td>\n",
       "      <td>-0.017955</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>0.070566</td>\n",
       "      <td>-0.076156</td>\n",
       "      <td>0.400397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>0.241918</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>-0.044039</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>-0.022455</td>\n",
       "      <td>-0.005482</td>\n",
       "      <td>-0.124627</td>\n",
       "      <td>0.095889</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117847</td>\n",
       "      <td>0.007997</td>\n",
       "      <td>-0.117152</td>\n",
       "      <td>-0.028143</td>\n",
       "      <td>0.079898</td>\n",
       "      <td>-0.066724</td>\n",
       "      <td>-0.029856</td>\n",
       "      <td>-0.038823</td>\n",
       "      <td>-0.092095</td>\n",
       "      <td>0.334313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>-0.142808</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>-0.049414</td>\n",
       "      <td>-0.144481</td>\n",
       "      <td>-0.024881</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>-0.036592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202610</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>-0.200163</td>\n",
       "      <td>-0.091514</td>\n",
       "      <td>0.154572</td>\n",
       "      <td>-0.210839</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.159728</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>0.485745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>0.315367</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.030765</td>\n",
       "      <td>-0.062264</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>-0.043990</td>\n",
       "      <td>0.140031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075473</td>\n",
       "      <td>-0.008800</td>\n",
       "      <td>-0.103751</td>\n",
       "      <td>-0.035379</td>\n",
       "      <td>0.183831</td>\n",
       "      <td>-0.019576</td>\n",
       "      <td>-0.045539</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>-0.054182</td>\n",
       "      <td>0.333367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>0.343187</td>\n",
       "      <td>0.142774</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>-0.046064</td>\n",
       "      <td>-0.006279</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>-0.064605</td>\n",
       "      <td>-0.014693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164360</td>\n",
       "      <td>0.059505</td>\n",
       "      <td>-0.157277</td>\n",
       "      <td>-0.037063</td>\n",
       "      <td>0.264306</td>\n",
       "      <td>-0.143545</td>\n",
       "      <td>-0.082002</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>-0.126353</td>\n",
       "      <td>0.484629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>0.404070</td>\n",
       "      <td>0.300714</td>\n",
       "      <td>-0.167735</td>\n",
       "      <td>-0.093816</td>\n",
       "      <td>-0.044867</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>-0.157262</td>\n",
       "      <td>0.192685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183933</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>-0.130174</td>\n",
       "      <td>-0.066710</td>\n",
       "      <td>0.179635</td>\n",
       "      <td>-0.012360</td>\n",
       "      <td>-0.098269</td>\n",
       "      <td>0.069790</td>\n",
       "      <td>-0.126898</td>\n",
       "      <td>0.462496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>0.411105</td>\n",
       "      <td>0.209193</td>\n",
       "      <td>-0.041473</td>\n",
       "      <td>-0.063504</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>-0.046049</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>-0.162120</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152191</td>\n",
       "      <td>0.022720</td>\n",
       "      <td>-0.132211</td>\n",
       "      <td>-0.046361</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>-0.094498</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>-0.153170</td>\n",
       "      <td>0.351791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>0.270290</td>\n",
       "      <td>0.147113</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.005676</td>\n",
       "      <td>-0.083171</td>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097686</td>\n",
       "      <td>0.024468</td>\n",
       "      <td>-0.047959</td>\n",
       "      <td>-0.020547</td>\n",
       "      <td>0.245821</td>\n",
       "      <td>-0.019414</td>\n",
       "      <td>-0.040876</td>\n",
       "      <td>-0.019973</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>0.381383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11180 rows × 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id  rsfmri_c_ngd_ad_ngd_ad  rsfmri_c_ngd_ad_ngd_cgc  \\\n",
       "0      NDAR_INV003RTV85                0.471330                 0.256267   \n",
       "1      NDAR_INV005V6D2C                0.279435                 0.116256   \n",
       "2      NDAR_INV007W6H7B                0.294463                 0.209772   \n",
       "3      NDAR_INV00BD7VDC                0.241918                 0.163942   \n",
       "6      NDAR_INV00CY2MDM                0.395300                 0.180940   \n",
       "...                 ...                     ...                      ...   \n",
       "22117  NDAR_INVZZLZCKAY                0.315367                 0.176642   \n",
       "22120  NDAR_INVZZNX6W2P                0.343187                 0.142774   \n",
       "22123  NDAR_INVZZPKBDAC                0.404070                 0.300714   \n",
       "22125  NDAR_INVZZZ2ALR6                0.411105                 0.209193   \n",
       "22127  NDAR_INVZZZNB0XC                0.270290                 0.147113   \n",
       "\n",
       "       rsfmri_c_ngd_ad_ngd_ca  rsfmri_c_ngd_ad_ngd_dt  \\\n",
       "0                   -0.076960               -0.116451   \n",
       "1                    0.063664               -0.024781   \n",
       "2                   -0.071834               -0.138693   \n",
       "3                   -0.090651               -0.044039   \n",
       "6                   -0.142808               -0.073424   \n",
       "...                       ...                     ...   \n",
       "22117               -0.010926               -0.019208   \n",
       "22120               -0.055901               -0.046064   \n",
       "22123               -0.167735               -0.093816   \n",
       "22125               -0.041473               -0.063504   \n",
       "22127               -0.111270               -0.093982   \n",
       "\n",
       "       rsfmri_c_ngd_ad_ngd_dla  rsfmri_c_ngd_ad_ngd_fo  rsfmri_c_ngd_ad_ngd_n  \\\n",
       "0                     0.022202               -0.036302              -0.057183   \n",
       "1                    -0.000840               -0.023421              -0.016284   \n",
       "2                    -0.035168                0.044412              -0.041321   \n",
       "3                     0.012523               -0.022455              -0.005482   \n",
       "6                    -0.049414               -0.144481              -0.024881   \n",
       "...                        ...                     ...                    ...   \n",
       "22117                -0.030765               -0.062264               0.021828   \n",
       "22120                -0.006279                0.013171               0.017289   \n",
       "22123                -0.044867                0.019305               0.049103   \n",
       "22125                -0.102791               -0.046049              -0.009828   \n",
       "22127                -0.005676               -0.083171              -0.018443   \n",
       "\n",
       "       rsfmri_c_ngd_ad_ngd_rspltp  rsfmri_c_ngd_ad_ngd_sa  ...  \\\n",
       "0                       -0.048132                0.006416  ...   \n",
       "1                        0.022696                0.056241  ...   \n",
       "2                       -0.185939                0.071985  ...   \n",
       "3                       -0.124627                0.095889  ...   \n",
       "6                       -0.012894               -0.036592  ...   \n",
       "...                           ...                     ...  ...   \n",
       "22117                   -0.043990                0.140031  ...   \n",
       "22120                   -0.064605               -0.014693  ...   \n",
       "22123                   -0.157262                0.192685  ...   \n",
       "22125                   -0.162120                0.026045  ...   \n",
       "22127                   -0.074797                0.052438  ...   \n",
       "\n",
       "       rsfmri_c_ngd_vs_ngd_dt  rsfmri_c_ngd_vs_ngd_dla  \\\n",
       "0                   -0.088894                 0.055355   \n",
       "1                   -0.136596                 0.017415   \n",
       "2                   -0.136426                -0.018266   \n",
       "3                   -0.117847                 0.007997   \n",
       "6                   -0.202610                 0.014283   \n",
       "...                       ...                      ...   \n",
       "22117               -0.075473                -0.008800   \n",
       "22120               -0.164360                 0.059505   \n",
       "22123               -0.183933                 0.022202   \n",
       "22125               -0.152191                 0.022720   \n",
       "22127               -0.097686                 0.024468   \n",
       "\n",
       "       rsfmri_c_ngd_vs_ngd_fo  rsfmri_c_ngd_vs_ngd_n  \\\n",
       "0                   -0.076806              -0.006004   \n",
       "1                   -0.111623              -0.077958   \n",
       "2                   -0.096822              -0.050623   \n",
       "3                   -0.117152              -0.028143   \n",
       "6                   -0.200163              -0.091514   \n",
       "...                       ...                    ...   \n",
       "22117               -0.103751              -0.035379   \n",
       "22120               -0.157277              -0.037063   \n",
       "22123               -0.130174              -0.066710   \n",
       "22125               -0.132211              -0.046361   \n",
       "22127               -0.047959              -0.020547   \n",
       "\n",
       "       rsfmri_c_ngd_vs_ngd_rspltp  rsfmri_c_ngd_vs_ngd_sa  \\\n",
       "0                        0.228366               -0.150867   \n",
       "1                        0.049159               -0.065420   \n",
       "2                        0.002644               -0.017955   \n",
       "3                        0.079898               -0.066724   \n",
       "6                        0.154572               -0.210839   \n",
       "...                           ...                     ...   \n",
       "22117                    0.183831               -0.019576   \n",
       "22120                    0.264306               -0.143545   \n",
       "22123                    0.179635               -0.012360   \n",
       "22125                    0.170122               -0.094498   \n",
       "22127                    0.245821               -0.019414   \n",
       "\n",
       "       rsfmri_c_ngd_vs_ngd_smh  rsfmri_c_ngd_vs_ngd_smm  \\\n",
       "0                    -0.064416                 0.098679   \n",
       "1                    -0.061459                 0.075145   \n",
       "2                    -0.060259                 0.070566   \n",
       "3                    -0.029856                -0.038823   \n",
       "6                     0.033060                 0.159728   \n",
       "...                        ...                      ...   \n",
       "22117                -0.045539                -0.012037   \n",
       "22120                -0.082002                 0.015764   \n",
       "22123                -0.098269                 0.069790   \n",
       "22125                 0.000854                 0.030236   \n",
       "22127                -0.040876                -0.019973   \n",
       "\n",
       "       rsfmri_c_ngd_vs_ngd_vta  rsfmri_c_ngd_vs_ngd_vs  \n",
       "0                    -0.155134                0.371960  \n",
       "1                    -0.071943                0.346746  \n",
       "2                    -0.076156                0.400397  \n",
       "3                    -0.092095                0.334313  \n",
       "6                    -0.125210                0.485745  \n",
       "...                        ...                     ...  \n",
       "22117                -0.054182                0.333367  \n",
       "22120                -0.126353                0.484629  \n",
       "22123                -0.126898                0.462496  \n",
       "22125                -0.153170                0.351791  \n",
       "22127                -0.105656                0.381383  \n",
       "\n",
       "[11180 rows x 170 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Computing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1418452100.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_fnc(x):\n",
    "    return [c[:-4] for c in x.split('_vs_')]\n",
    "\n",
    "def compute_fnc_agg(df, pair_cols):\n",
    "    df_res = pd.DataFrame()\n",
    "    \n",
    "    for c in pair_cols.keys():\n",
    "        df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
    "        df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
    "        df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
    "        df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "# load data\n",
    "print('Loading data...')\n",
    "# df = pd.read_csv('./data/raw/fnc.csv')\n",
    "df = data\n",
    "fnc_cols = sorted(df.columns[1:])\n",
    "\n",
    "# generate pair of cols\n",
    "pair_cols = {}\n",
    "colz = ['SCN', 'ADN', 'SMN', 'VSN', 'CON', 'DMN', 'CBN']\n",
    "for pair in itertools.combinations_with_replacement(colz, 2):\n",
    "    pair_cols[pair] = [c for c in fnc_cols if set(parse_fnc(c)) == set(pair)]\n",
    "\n",
    "# compute features\n",
    "print('Computing features...')\n",
    "agg_feats = compute_fnc_agg(df, pair_cols)\n",
    "agg_feats = agg_feats[agg_feats.std()[agg_feats.std() != 0].index]\n",
    "agg_feats.columns = ['AGG_' + c.lower() for c in agg_feats.columns]\n",
    "#agg_feats['Id'] = df['Id']\n",
    "agg_feats = pd.concat((df[['src_subject_id']], agg_feats), axis=1)\n",
    "# agg_feats.to_csv('./data/features/agg_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Original: rsfmri_c_ngd_ad_ngd_ad, Parsed: ('ngd', 'ad')\n",
      "Original: rsfmri_c_ngd_ad_ngd_cgc, Parsed: ('ngd', 'ad')\n",
      "Original: rsfmri_c_ngd_ad_ngd_ca, Parsed: ('ngd', 'ad')\n",
      "Original: rsfmri_c_ngd_ad_ngd_dt, Parsed: ('ngd', 'ad')\n",
      "Original: rsfmri_c_ngd_ad_ngd_dla, Parsed: ('ngd', 'ad')\n",
      "Detected network names: ['ad', 'ca', 'cgc', 'dla', 'dt', 'fo', 'n', 'ngd', 'rspltp', 'sa', 'smh', 'smm', 'vs', 'vta']\n",
      "Example of pair_cols content: {('ad', 'ad'): [], ('ad', 'ca'): [], ('ad', 'cgc'): [], ('ad', 'dla'): [], ('ad', 'dt'): []}\n",
      "Computing features...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# 加载数据\n",
    "print('Loading data...')\n",
    "df = data\n",
    "\n",
    "# 提取所有列名\n",
    "fnc_cols = df.columns[1:]  # 排除 'src_subject_id' 列\n",
    "\n",
    "# 自动解析并提取网络名称\n",
    "def parse_fnc(x):\n",
    "    parts = x.split('_')\n",
    "    return parts[2], parts[3]  # 假设网络名称总是在第3和第4个位置\n",
    "\n",
    "# 检查解析结果\n",
    "for col in fnc_cols[:5]:  # 打印前5个列名的解析结果进行检查\n",
    "    print(f\"Original: {col}, Parsed: {parse_fnc(col)}\")\n",
    "\n",
    "# 生成网络名称集合\n",
    "network_names = set()\n",
    "for col in fnc_cols:\n",
    "    network1, network2 = parse_fnc(col)\n",
    "    network_names.add(network1)\n",
    "    network_names.add(network2)\n",
    "\n",
    "# 将网络名称转换为列表并排序\n",
    "colz = sorted(network_names)\n",
    "print(\"Detected network names:\", colz)\n",
    "\n",
    "# 生成网络对列\n",
    "pair_cols = {}\n",
    "for pair in itertools.combinations_with_replacement(colz, 2):\n",
    "    pair_cols[pair] = [c for c in fnc_cols if set(parse_fnc(c)) == set(pair)]\n",
    "\n",
    "# 检查 pair_cols 是否正确匹配\n",
    "print(\"Example of pair_cols content:\", {k: v for k, v in list(pair_cols.items())[:5]})\n",
    "\n",
    "# 定义聚合特征计算函数\n",
    "def compute_fnc_agg(df, pair_cols):\n",
    "    df_res = pd.DataFrame()\n",
    "    for c in pair_cols.keys():\n",
    "        if pair_cols[c]:  # 检查是否有匹配的列\n",
    "            df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
    "            df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
    "            df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
    "            df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
    "    return df_res\n",
    "\n",
    "# 计算特征\n",
    "print('Computing features...')\n",
    "agg_feats = compute_fnc_agg(df, pair_cols)\n",
    "\n",
    "# 过滤掉标准差为0的列并重命名\n",
    "agg_feats = agg_feats[agg_feats.std()[agg_feats.std() != 0].index]\n",
    "agg_feats.columns = ['AGG_' + c.lower() for c in agg_feats.columns]\n",
    "\n",
    "# 合并 'src_subject_id' 列\n",
    "agg_feats = pd.concat((df[['src_subject_id']], agg_feats), axis=1)\n",
    "\n",
    "# # 保存到 CSV 文件\n",
    "# agg_feats.to_csv('./data/features/agg_feats.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Computing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\2611962364.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_fnc(x):\n",
    "    return [c[:-4] for c in x.split('_vs_')]\n",
    "\n",
    "def compute_fnc_agg(df, pair_cols):\n",
    "    df_res = pd.DataFrame()\n",
    "    \n",
    "    for c in pair_cols.keys():\n",
    "        df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
    "        df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
    "        df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
    "        df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
    "\n",
    "    return df_res\n",
    "\n",
    "# load data\n",
    "print('Loading data...')\n",
    "df = data\n",
    "\n",
    "fnc_cols = df.columns[1:]  # 排除 'src_subject_id' 列\n",
    "\n",
    "# generate pair of cols\n",
    "pair_cols = {}\n",
    "colz = ['ad', 'ca', 'cgc', 'dla', 'dt', 'fo', 'n', 'rspltp', 'sa', 'smh', 'smm', 'vs', 'vta']\n",
    "for pair in itertools.combinations_with_replacement(colz, 2):\n",
    "    pair_cols[pair] = [c for c in fnc_cols if set(parse_fnc(c)) == set(pair)]\n",
    "\n",
    "# compute features\n",
    "print('Computing features...')\n",
    "agg_feats = compute_fnc_agg(df, pair_cols)\n",
    "agg_feats = agg_feats[agg_feats.std()[agg_feats.std() != 0].index]\n",
    "agg_feats.columns = ['AGG_' + c.lower() for c in agg_feats.columns]\n",
    "#agg_feats['Id'] = df['Id']\n",
    "agg_feats = pd.concat((df[['src_subject_id']], agg_feats), axis=1)\n",
    "# agg_feats.to_csv('./data/features/agg_feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11180 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id\n",
       "0      NDAR_INV003RTV85\n",
       "1      NDAR_INV005V6D2C\n",
       "2      NDAR_INV007W6H7B\n",
       "3      NDAR_INV00BD7VDC\n",
       "6      NDAR_INV00CY2MDM\n",
       "...                 ...\n",
       "22117  NDAR_INVZZLZCKAY\n",
       "22120  NDAR_INVZZNX6W2P\n",
       "22123  NDAR_INVZZPKBDAC\n",
       "22125  NDAR_INVZZZ2ALR6\n",
       "22127  NDAR_INVZZZNB0XC\n",
       "\n",
       "[11180 rows x 1 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rsfmri_c_ngd_ad_ngd_ad', 'rsfmri_c_ngd_ad_ngd_cgc',\n",
       "       'rsfmri_c_ngd_ad_ngd_ca', 'rsfmri_c_ngd_ad_ngd_dt',\n",
       "       'rsfmri_c_ngd_ad_ngd_dla', 'rsfmri_c_ngd_ad_ngd_fo',\n",
       "       'rsfmri_c_ngd_ad_ngd_n', 'rsfmri_c_ngd_ad_ngd_rspltp',\n",
       "       'rsfmri_c_ngd_ad_ngd_sa', 'rsfmri_c_ngd_ad_ngd_smh',\n",
       "       ...\n",
       "       'rsfmri_c_ngd_vs_ngd_dt', 'rsfmri_c_ngd_vs_ngd_dla',\n",
       "       'rsfmri_c_ngd_vs_ngd_fo', 'rsfmri_c_ngd_vs_ngd_n',\n",
       "       'rsfmri_c_ngd_vs_ngd_rspltp', 'rsfmri_c_ngd_vs_ngd_sa',\n",
       "       'rsfmri_c_ngd_vs_ngd_smh', 'rsfmri_c_ngd_vs_ngd_smm',\n",
       "       'rsfmri_c_ngd_vs_ngd_vta', 'rsfmri_c_ngd_vs_ngd_vs'],\n",
       "      dtype='object', length=169)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rsfmri', 'c', 'ngd', 'vs', 'ngd', 'vs']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ngd'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取的网络名称: ['ngd']\n"
     ]
    }
   ],
   "source": [
    "# 提取所有列名\n",
    "fnc_cols = df.columns[1:]\n",
    "\n",
    "# 创建一个集合来存储唯一的网络名称\n",
    "network_names = set()\n",
    "\n",
    "# 遍历每个列名并提取网络名称\n",
    "for col in fnc_cols:\n",
    "    parts = col.split('_')  # 使用下划线分割列名\n",
    "    # 提取网络名称并添加到集合中\n",
    "    network_names.add(parts[2])  # 第一个网络名\n",
    "    network_names.add(parts[4])  # 第二个网络名\n",
    "\n",
    "# 将集合转换为排序后的列表\n",
    "network_names_list = sorted(network_names)\n",
    "\n",
    "print(\"提取的网络名称:\", network_names_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_names.add(parts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取的网络名称: ['ad', 'ca', 'cgc', 'dla', 'dt', 'fo', 'n', 'rspltp', 'sa', 'smh', 'smm', 'vs', 'vta']\n"
     ]
    }
   ],
   "source": [
    "# 提取从第二个开始的列名，跳过第一个列名\n",
    "fnc_cols = df.columns[1:]\n",
    "\n",
    "# 创建一个集合来存储唯一的网络名称\n",
    "network_names = set()\n",
    "\n",
    "# 遍历每个列名并提取网络名称\n",
    "for col in fnc_cols:\n",
    "    parts = col.split('_')\n",
    "    # 正确提取网络名称\n",
    "    network_names.add(parts[3])  # 第一个网络名\n",
    "    network_names.add(parts[5])  # 第二个网络名\n",
    "\n",
    "# 将集合转换为排序后的列表\n",
    "network_names_list = sorted(network_names)\n",
    "\n",
    "print(\"提取的网络名称:\", network_names_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "提取的网络名称: ['ad', 'ca', 'cgc', 'dla', 'dt', 'fo', 'n', 'rspltp', 'sa', 'smh', 'smm', 'vs', 'vta']\n",
      "Computing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
      "C:\\Users\\Makka Papa\\AppData\\Local\\Temp\\ipykernel_5648\\1714619537.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 加载数据\n",
    "print('Loading data...')\n",
    "df = data\n",
    "\n",
    "# 提取从第二个开始的列名，跳过第一个列名\n",
    "fnc_cols = df.columns[1:]  # 排除 'src_subject_id' 列\n",
    "\n",
    "# 创建一个集合来存储唯一的网络名称\n",
    "network_names = set()\n",
    "\n",
    "# 遍历每个列名并提取网络名称\n",
    "for col in fnc_cols:\n",
    "    parts = col.split('_')\n",
    "    # 正确提取网络名称\n",
    "    network_names.add(parts[3])  # 第一个网络名\n",
    "    network_names.add(parts[5])  # 第二个网络名\n",
    "\n",
    "# 将集合转换为排序后的列表\n",
    "colz = sorted(network_names)\n",
    "print(\"提取的网络名称:\", colz)\n",
    "\n",
    "# 修改后的解析函数，使用分割好的网络名称\n",
    "def parse_fnc(x):\n",
    "    parts = x.split('_')\n",
    "    return [parts[3], parts[5]]  # 返回正确的网络名称\n",
    "\n",
    "# 生成网络对列\n",
    "pair_cols = {}\n",
    "for pair in itertools.combinations_with_replacement(colz, 2):\n",
    "    # 匹配列名，并检查是否有匹配的列\n",
    "    pair_cols[pair] = [c for c in fnc_cols if set(parse_fnc(c)) == set(pair)]\n",
    "    if len(pair_cols[pair]) == 0:  # 打印未匹配的网络对\n",
    "        print(f\"No matching columns for pair: {pair}\")\n",
    "\n",
    "# 计算聚合特征\n",
    "def compute_fnc_agg(df, pair_cols):\n",
    "    df_res = pd.DataFrame()\n",
    "    for c in pair_cols.keys():\n",
    "        if len(pair_cols[c]) > 0:  # 仅对有匹配列的网络对计算特征\n",
    "            df_res[c[0] + '_' + c[1] + '_mean'] = df[pair_cols[c]].mean(axis=1)\n",
    "            df_res[c[0] + '_' + c[1] + '_min'] = df[pair_cols[c]].min(axis=1)\n",
    "            df_res[c[0] + '_' + c[1] + '_max'] = df[pair_cols[c]].max(axis=1)\n",
    "            df_res[c[0] + '_' + c[1] + '_std'] = df[pair_cols[c]].std(axis=1).fillna(0)\n",
    "    return df_res\n",
    "\n",
    "# 计算特征\n",
    "print('Computing features...')\n",
    "agg_feats = compute_fnc_agg(df, pair_cols)\n",
    "\n",
    "# 检查 agg_feats 是否为空\n",
    "if agg_feats.empty:\n",
    "    print(\"No features were generated. Check the matching logic.\")\n",
    "\n",
    "# 过滤掉标准差为0的列并重命名\n",
    "agg_feats = agg_feats[agg_feats.std()[agg_feats.std() != 0].index]\n",
    "agg_feats.columns = ['AGG_' + c.lower() for c in agg_feats.columns]\n",
    "\n",
    "# 合并 'src_subject_id' 列\n",
    "agg_feats = pd.concat((df[['src_subject_id']], agg_feats), axis=1)\n",
    "\n",
    "# 取消注释以保存特征到 CSV 文件\n",
    "# agg_feats.to_csv('./data/features/agg_feats.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>AGG_ad_ad_mean</th>\n",
       "      <th>AGG_ad_ad_min</th>\n",
       "      <th>AGG_ad_ad_max</th>\n",
       "      <th>AGG_ad_ca_mean</th>\n",
       "      <th>AGG_ad_ca_min</th>\n",
       "      <th>AGG_ad_ca_max</th>\n",
       "      <th>AGG_ad_cgc_mean</th>\n",
       "      <th>AGG_ad_cgc_min</th>\n",
       "      <th>AGG_ad_cgc_max</th>\n",
       "      <th>...</th>\n",
       "      <th>AGG_smm_vta_max</th>\n",
       "      <th>AGG_vs_vs_mean</th>\n",
       "      <th>AGG_vs_vs_min</th>\n",
       "      <th>AGG_vs_vs_max</th>\n",
       "      <th>AGG_vs_vta_mean</th>\n",
       "      <th>AGG_vs_vta_min</th>\n",
       "      <th>AGG_vs_vta_max</th>\n",
       "      <th>AGG_vta_vta_mean</th>\n",
       "      <th>AGG_vta_vta_min</th>\n",
       "      <th>AGG_vta_vta_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>0.471330</td>\n",
       "      <td>0.471330</td>\n",
       "      <td>0.471330</td>\n",
       "      <td>-0.076960</td>\n",
       "      <td>-0.076960</td>\n",
       "      <td>-0.076960</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042176</td>\n",
       "      <td>0.371960</td>\n",
       "      <td>0.371960</td>\n",
       "      <td>0.371960</td>\n",
       "      <td>-0.155134</td>\n",
       "      <td>-0.155134</td>\n",
       "      <td>-0.155134</td>\n",
       "      <td>0.233819</td>\n",
       "      <td>0.233819</td>\n",
       "      <td>0.233819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>0.116256</td>\n",
       "      <td>0.116256</td>\n",
       "      <td>0.116256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007564</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>-0.071943</td>\n",
       "      <td>-0.071943</td>\n",
       "      <td>-0.071943</td>\n",
       "      <td>0.173929</td>\n",
       "      <td>0.173929</td>\n",
       "      <td>0.173929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>0.400397</td>\n",
       "      <td>0.400397</td>\n",
       "      <td>0.400397</td>\n",
       "      <td>-0.076156</td>\n",
       "      <td>-0.076156</td>\n",
       "      <td>-0.076156</td>\n",
       "      <td>0.257184</td>\n",
       "      <td>0.257184</td>\n",
       "      <td>0.257184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>0.241918</td>\n",
       "      <td>0.241918</td>\n",
       "      <td>0.241918</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011466</td>\n",
       "      <td>0.334313</td>\n",
       "      <td>0.334313</td>\n",
       "      <td>0.334313</td>\n",
       "      <td>-0.092095</td>\n",
       "      <td>-0.092095</td>\n",
       "      <td>-0.092095</td>\n",
       "      <td>0.167689</td>\n",
       "      <td>0.167689</td>\n",
       "      <td>0.167689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>-0.142808</td>\n",
       "      <td>-0.142808</td>\n",
       "      <td>-0.142808</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074829</td>\n",
       "      <td>0.485745</td>\n",
       "      <td>0.485745</td>\n",
       "      <td>0.485745</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>0.230946</td>\n",
       "      <td>0.230946</td>\n",
       "      <td>0.230946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>0.315367</td>\n",
       "      <td>0.315367</td>\n",
       "      <td>0.315367</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>-0.054182</td>\n",
       "      <td>-0.054182</td>\n",
       "      <td>-0.054182</td>\n",
       "      <td>0.183470</td>\n",
       "      <td>0.183470</td>\n",
       "      <td>0.183470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>0.343187</td>\n",
       "      <td>0.343187</td>\n",
       "      <td>0.343187</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>0.142774</td>\n",
       "      <td>0.142774</td>\n",
       "      <td>0.142774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.484629</td>\n",
       "      <td>0.484629</td>\n",
       "      <td>0.484629</td>\n",
       "      <td>-0.126353</td>\n",
       "      <td>-0.126353</td>\n",
       "      <td>-0.126353</td>\n",
       "      <td>0.238454</td>\n",
       "      <td>0.238454</td>\n",
       "      <td>0.238454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>0.404070</td>\n",
       "      <td>0.404070</td>\n",
       "      <td>0.404070</td>\n",
       "      <td>-0.167735</td>\n",
       "      <td>-0.167735</td>\n",
       "      <td>-0.167735</td>\n",
       "      <td>0.300714</td>\n",
       "      <td>0.300714</td>\n",
       "      <td>0.300714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128084</td>\n",
       "      <td>0.462496</td>\n",
       "      <td>0.462496</td>\n",
       "      <td>0.462496</td>\n",
       "      <td>-0.126898</td>\n",
       "      <td>-0.126898</td>\n",
       "      <td>-0.126898</td>\n",
       "      <td>0.267290</td>\n",
       "      <td>0.267290</td>\n",
       "      <td>0.267290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>0.411105</td>\n",
       "      <td>0.411105</td>\n",
       "      <td>0.411105</td>\n",
       "      <td>-0.041473</td>\n",
       "      <td>-0.041473</td>\n",
       "      <td>-0.041473</td>\n",
       "      <td>0.209193</td>\n",
       "      <td>0.209193</td>\n",
       "      <td>0.209193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029845</td>\n",
       "      <td>0.351791</td>\n",
       "      <td>0.351791</td>\n",
       "      <td>0.351791</td>\n",
       "      <td>-0.153170</td>\n",
       "      <td>-0.153170</td>\n",
       "      <td>-0.153170</td>\n",
       "      <td>0.301422</td>\n",
       "      <td>0.301422</td>\n",
       "      <td>0.301422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>0.270290</td>\n",
       "      <td>0.270290</td>\n",
       "      <td>0.270290</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>0.147113</td>\n",
       "      <td>0.147113</td>\n",
       "      <td>0.147113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011148</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>0.157045</td>\n",
       "      <td>0.157045</td>\n",
       "      <td>0.157045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11180 rows × 274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id  AGG_ad_ad_mean  AGG_ad_ad_min  AGG_ad_ad_max  \\\n",
       "0      NDAR_INV003RTV85        0.471330       0.471330       0.471330   \n",
       "1      NDAR_INV005V6D2C        0.279435       0.279435       0.279435   \n",
       "2      NDAR_INV007W6H7B        0.294463       0.294463       0.294463   \n",
       "3      NDAR_INV00BD7VDC        0.241918       0.241918       0.241918   \n",
       "6      NDAR_INV00CY2MDM        0.395300       0.395300       0.395300   \n",
       "...                 ...             ...            ...            ...   \n",
       "22117  NDAR_INVZZLZCKAY        0.315367       0.315367       0.315367   \n",
       "22120  NDAR_INVZZNX6W2P        0.343187       0.343187       0.343187   \n",
       "22123  NDAR_INVZZPKBDAC        0.404070       0.404070       0.404070   \n",
       "22125  NDAR_INVZZZ2ALR6        0.411105       0.411105       0.411105   \n",
       "22127  NDAR_INVZZZNB0XC        0.270290       0.270290       0.270290   \n",
       "\n",
       "       AGG_ad_ca_mean  AGG_ad_ca_min  AGG_ad_ca_max  AGG_ad_cgc_mean  \\\n",
       "0           -0.076960      -0.076960      -0.076960         0.256267   \n",
       "1            0.063664       0.063664       0.063664         0.116256   \n",
       "2           -0.071834      -0.071834      -0.071834         0.209772   \n",
       "3           -0.090651      -0.090651      -0.090651         0.163942   \n",
       "6           -0.142808      -0.142808      -0.142808         0.180940   \n",
       "...               ...            ...            ...              ...   \n",
       "22117       -0.010926      -0.010926      -0.010926         0.176642   \n",
       "22120       -0.055901      -0.055901      -0.055901         0.142774   \n",
       "22123       -0.167735      -0.167735      -0.167735         0.300714   \n",
       "22125       -0.041473      -0.041473      -0.041473         0.209193   \n",
       "22127       -0.111270      -0.111270      -0.111270         0.147113   \n",
       "\n",
       "       AGG_ad_cgc_min  AGG_ad_cgc_max  ...  AGG_smm_vta_max  AGG_vs_vs_mean  \\\n",
       "0            0.256267        0.256267  ...        -0.042176        0.371960   \n",
       "1            0.116256        0.116256  ...        -0.007564        0.346746   \n",
       "2            0.209772        0.209772  ...         0.026875        0.400397   \n",
       "3            0.163942        0.163942  ...         0.011466        0.334313   \n",
       "6            0.180940        0.180940  ...         0.074829        0.485745   \n",
       "...               ...             ...  ...              ...             ...   \n",
       "22117        0.176642        0.176642  ...         0.025325        0.333367   \n",
       "22120        0.142774        0.142774  ...         0.018554        0.484629   \n",
       "22123        0.300714        0.300714  ...         0.128084        0.462496   \n",
       "22125        0.209193        0.209193  ...         0.029845        0.351791   \n",
       "22127        0.147113        0.147113  ...        -0.011148        0.381383   \n",
       "\n",
       "       AGG_vs_vs_min  AGG_vs_vs_max  AGG_vs_vta_mean  AGG_vs_vta_min  \\\n",
       "0           0.371960       0.371960        -0.155134       -0.155134   \n",
       "1           0.346746       0.346746        -0.071943       -0.071943   \n",
       "2           0.400397       0.400397        -0.076156       -0.076156   \n",
       "3           0.334313       0.334313        -0.092095       -0.092095   \n",
       "6           0.485745       0.485745        -0.125210       -0.125210   \n",
       "...              ...            ...              ...             ...   \n",
       "22117       0.333367       0.333367        -0.054182       -0.054182   \n",
       "22120       0.484629       0.484629        -0.126353       -0.126353   \n",
       "22123       0.462496       0.462496        -0.126898       -0.126898   \n",
       "22125       0.351791       0.351791        -0.153170       -0.153170   \n",
       "22127       0.381383       0.381383        -0.105656       -0.105656   \n",
       "\n",
       "       AGG_vs_vta_max  AGG_vta_vta_mean  AGG_vta_vta_min  AGG_vta_vta_max  \n",
       "0           -0.155134          0.233819         0.233819         0.233819  \n",
       "1           -0.071943          0.173929         0.173929         0.173929  \n",
       "2           -0.076156          0.257184         0.257184         0.257184  \n",
       "3           -0.092095          0.167689         0.167689         0.167689  \n",
       "6           -0.125210          0.230946         0.230946         0.230946  \n",
       "...               ...               ...              ...              ...  \n",
       "22117       -0.054182          0.183470         0.183470         0.183470  \n",
       "22120       -0.126353          0.238454         0.238454         0.238454  \n",
       "22123       -0.126898          0.267290         0.267290         0.267290  \n",
       "22125       -0.153170          0.301422         0.301422         0.301422  \n",
       "22127       -0.105656          0.157045         0.157045         0.157045  \n",
       "\n",
       "[11180 rows x 274 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#筛选所有列名结尾为_mean的列\n",
    "mean_cols = [c for c in agg_feats.columns if c.endswith('_mean')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = agg_feats[['src_subject_id']+ mean_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>AGG_ad_ad_mean</th>\n",
       "      <th>AGG_ad_ca_mean</th>\n",
       "      <th>AGG_ad_cgc_mean</th>\n",
       "      <th>AGG_ad_dla_mean</th>\n",
       "      <th>AGG_ad_dt_mean</th>\n",
       "      <th>AGG_ad_fo_mean</th>\n",
       "      <th>AGG_ad_n_mean</th>\n",
       "      <th>AGG_ad_rspltp_mean</th>\n",
       "      <th>AGG_ad_sa_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>AGG_smh_smh_mean</th>\n",
       "      <th>AGG_smh_smm_mean</th>\n",
       "      <th>AGG_smh_vs_mean</th>\n",
       "      <th>AGG_smh_vta_mean</th>\n",
       "      <th>AGG_smm_smm_mean</th>\n",
       "      <th>AGG_smm_vs_mean</th>\n",
       "      <th>AGG_smm_vta_mean</th>\n",
       "      <th>AGG_vs_vs_mean</th>\n",
       "      <th>AGG_vs_vta_mean</th>\n",
       "      <th>AGG_vta_vta_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>0.471330</td>\n",
       "      <td>-0.076960</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>-0.116451</td>\n",
       "      <td>-0.036302</td>\n",
       "      <td>-0.057183</td>\n",
       "      <td>-0.048132</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314437</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>-0.064416</td>\n",
       "      <td>-0.042587</td>\n",
       "      <td>0.610255</td>\n",
       "      <td>0.098679</td>\n",
       "      <td>-0.042176</td>\n",
       "      <td>0.371960</td>\n",
       "      <td>-0.155134</td>\n",
       "      <td>0.233819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>0.116256</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.024781</td>\n",
       "      <td>-0.023421</td>\n",
       "      <td>-0.016284</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.056241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>-0.061459</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>0.506645</td>\n",
       "      <td>0.075145</td>\n",
       "      <td>-0.007564</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>-0.071943</td>\n",
       "      <td>0.173929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>-0.035168</td>\n",
       "      <td>-0.138693</td>\n",
       "      <td>0.044412</td>\n",
       "      <td>-0.041321</td>\n",
       "      <td>-0.185939</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288158</td>\n",
       "      <td>0.088796</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.683473</td>\n",
       "      <td>0.070566</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>0.400397</td>\n",
       "      <td>-0.076156</td>\n",
       "      <td>0.257184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>0.241918</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>-0.044039</td>\n",
       "      <td>-0.022455</td>\n",
       "      <td>-0.005482</td>\n",
       "      <td>-0.124627</td>\n",
       "      <td>0.095889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333995</td>\n",
       "      <td>0.056474</td>\n",
       "      <td>-0.029856</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.477314</td>\n",
       "      <td>-0.038823</td>\n",
       "      <td>0.011466</td>\n",
       "      <td>0.334313</td>\n",
       "      <td>-0.092095</td>\n",
       "      <td>0.167689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>-0.142808</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>-0.049414</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>-0.144481</td>\n",
       "      <td>-0.024881</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>-0.036592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426006</td>\n",
       "      <td>0.380348</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.818149</td>\n",
       "      <td>0.159728</td>\n",
       "      <td>0.074829</td>\n",
       "      <td>0.485745</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>0.230946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>0.315367</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>-0.030765</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.062264</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>-0.043990</td>\n",
       "      <td>0.140031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211188</td>\n",
       "      <td>0.024467</td>\n",
       "      <td>-0.045539</td>\n",
       "      <td>0.006467</td>\n",
       "      <td>0.596059</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>-0.054182</td>\n",
       "      <td>0.183470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>0.343187</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>0.142774</td>\n",
       "      <td>-0.006279</td>\n",
       "      <td>-0.046064</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>-0.064605</td>\n",
       "      <td>-0.014693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>0.120344</td>\n",
       "      <td>-0.082002</td>\n",
       "      <td>-0.003782</td>\n",
       "      <td>0.715068</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.484629</td>\n",
       "      <td>-0.126353</td>\n",
       "      <td>0.238454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>0.404070</td>\n",
       "      <td>-0.167735</td>\n",
       "      <td>0.300714</td>\n",
       "      <td>-0.044867</td>\n",
       "      <td>-0.093816</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>-0.157262</td>\n",
       "      <td>0.192685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223616</td>\n",
       "      <td>-0.033873</td>\n",
       "      <td>-0.098269</td>\n",
       "      <td>-0.007025</td>\n",
       "      <td>0.840622</td>\n",
       "      <td>0.069790</td>\n",
       "      <td>0.128084</td>\n",
       "      <td>0.462496</td>\n",
       "      <td>-0.126898</td>\n",
       "      <td>0.267290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>0.411105</td>\n",
       "      <td>-0.041473</td>\n",
       "      <td>0.209193</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>-0.063504</td>\n",
       "      <td>-0.046049</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>-0.162120</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302695</td>\n",
       "      <td>0.071764</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.082954</td>\n",
       "      <td>0.723260</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>0.029845</td>\n",
       "      <td>0.351791</td>\n",
       "      <td>-0.153170</td>\n",
       "      <td>0.301422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>0.270290</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>0.147113</td>\n",
       "      <td>-0.005676</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.083171</td>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252571</td>\n",
       "      <td>0.072439</td>\n",
       "      <td>-0.040876</td>\n",
       "      <td>-0.033312</td>\n",
       "      <td>0.473995</td>\n",
       "      <td>-0.019973</td>\n",
       "      <td>-0.011148</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>0.157045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11180 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id  AGG_ad_ad_mean  AGG_ad_ca_mean  AGG_ad_cgc_mean  \\\n",
       "0      NDAR_INV003RTV85        0.471330       -0.076960         0.256267   \n",
       "1      NDAR_INV005V6D2C        0.279435        0.063664         0.116256   \n",
       "2      NDAR_INV007W6H7B        0.294463       -0.071834         0.209772   \n",
       "3      NDAR_INV00BD7VDC        0.241918       -0.090651         0.163942   \n",
       "6      NDAR_INV00CY2MDM        0.395300       -0.142808         0.180940   \n",
       "...                 ...             ...             ...              ...   \n",
       "22117  NDAR_INVZZLZCKAY        0.315367       -0.010926         0.176642   \n",
       "22120  NDAR_INVZZNX6W2P        0.343187       -0.055901         0.142774   \n",
       "22123  NDAR_INVZZPKBDAC        0.404070       -0.167735         0.300714   \n",
       "22125  NDAR_INVZZZ2ALR6        0.411105       -0.041473         0.209193   \n",
       "22127  NDAR_INVZZZNB0XC        0.270290       -0.111270         0.147113   \n",
       "\n",
       "       AGG_ad_dla_mean  AGG_ad_dt_mean  AGG_ad_fo_mean  AGG_ad_n_mean  \\\n",
       "0             0.022202       -0.116451       -0.036302      -0.057183   \n",
       "1            -0.000840       -0.024781       -0.023421      -0.016284   \n",
       "2            -0.035168       -0.138693        0.044412      -0.041321   \n",
       "3             0.012523       -0.044039       -0.022455      -0.005482   \n",
       "6            -0.049414       -0.073424       -0.144481      -0.024881   \n",
       "...                ...             ...             ...            ...   \n",
       "22117        -0.030765       -0.019208       -0.062264       0.021828   \n",
       "22120        -0.006279       -0.046064        0.013171       0.017289   \n",
       "22123        -0.044867       -0.093816        0.019305       0.049103   \n",
       "22125        -0.102791       -0.063504       -0.046049      -0.009828   \n",
       "22127        -0.005676       -0.093982       -0.083171      -0.018443   \n",
       "\n",
       "       AGG_ad_rspltp_mean  AGG_ad_sa_mean  ...  AGG_smh_smh_mean  \\\n",
       "0               -0.048132        0.006416  ...          0.314437   \n",
       "1                0.022696        0.056241  ...          0.302632   \n",
       "2               -0.185939        0.071985  ...          0.288158   \n",
       "3               -0.124627        0.095889  ...          0.333995   \n",
       "6               -0.012894       -0.036592  ...          0.426006   \n",
       "...                   ...             ...  ...               ...   \n",
       "22117           -0.043990        0.140031  ...          0.211188   \n",
       "22120           -0.064605       -0.014693  ...          0.379075   \n",
       "22123           -0.157262        0.192685  ...          0.223616   \n",
       "22125           -0.162120        0.026045  ...          0.302695   \n",
       "22127           -0.074797        0.052438  ...          0.252571   \n",
       "\n",
       "       AGG_smh_smm_mean  AGG_smh_vs_mean  AGG_smh_vta_mean  AGG_smm_smm_mean  \\\n",
       "0              0.056185        -0.064416         -0.042587          0.610255   \n",
       "1              0.062337        -0.061459          0.023958          0.506645   \n",
       "2              0.088796        -0.060259          0.002308          0.683473   \n",
       "3              0.056474        -0.029856          0.020649          0.477314   \n",
       "6              0.380348         0.033060          0.032599          0.818149   \n",
       "...                 ...              ...               ...               ...   \n",
       "22117          0.024467        -0.045539          0.006467          0.596059   \n",
       "22120          0.120344        -0.082002         -0.003782          0.715068   \n",
       "22123         -0.033873        -0.098269         -0.007025          0.840622   \n",
       "22125          0.071764         0.000854         -0.082954          0.723260   \n",
       "22127          0.072439        -0.040876         -0.033312          0.473995   \n",
       "\n",
       "       AGG_smm_vs_mean  AGG_smm_vta_mean  AGG_vs_vs_mean  AGG_vs_vta_mean  \\\n",
       "0             0.098679         -0.042176        0.371960        -0.155134   \n",
       "1             0.075145         -0.007564        0.346746        -0.071943   \n",
       "2             0.070566          0.026875        0.400397        -0.076156   \n",
       "3            -0.038823          0.011466        0.334313        -0.092095   \n",
       "6             0.159728          0.074829        0.485745        -0.125210   \n",
       "...                ...               ...             ...              ...   \n",
       "22117        -0.012037          0.025325        0.333367        -0.054182   \n",
       "22120         0.015764          0.018554        0.484629        -0.126353   \n",
       "22123         0.069790          0.128084        0.462496        -0.126898   \n",
       "22125         0.030236          0.029845        0.351791        -0.153170   \n",
       "22127        -0.019973         -0.011148        0.381383        -0.105656   \n",
       "\n",
       "       AGG_vta_vta_mean  \n",
       "0              0.233819  \n",
       "1              0.173929  \n",
       "2              0.257184  \n",
       "3              0.167689  \n",
       "6              0.230946  \n",
       "...                 ...  \n",
       "22117          0.183470  \n",
       "22120          0.238454  \n",
       "22123          0.267290  \n",
       "22125          0.301422  \n",
       "22127          0.157045  \n",
       "\n",
       "[11180 rows x 92 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor_1</th>\n",
       "      <th>Factor_2</th>\n",
       "      <th>Factor_3</th>\n",
       "      <th>Factor_4</th>\n",
       "      <th>Factor_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.225962</td>\n",
       "      <td>-0.987701</td>\n",
       "      <td>1.428785</td>\n",
       "      <td>0.196275</td>\n",
       "      <td>2.294005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.270909</td>\n",
       "      <td>-1.172184</td>\n",
       "      <td>1.699451</td>\n",
       "      <td>1.889652</td>\n",
       "      <td>2.205972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170160</td>\n",
       "      <td>0.610330</td>\n",
       "      <td>-0.446073</td>\n",
       "      <td>2.320928</td>\n",
       "      <td>1.705356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.410402</td>\n",
       "      <td>-0.158501</td>\n",
       "      <td>0.223055</td>\n",
       "      <td>3.264879</td>\n",
       "      <td>1.840801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.462913</td>\n",
       "      <td>-5.130525</td>\n",
       "      <td>1.558750</td>\n",
       "      <td>5.146125</td>\n",
       "      <td>-4.048456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>-6.752625</td>\n",
       "      <td>5.188299</td>\n",
       "      <td>0.805525</td>\n",
       "      <td>-0.362465</td>\n",
       "      <td>-2.794127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>0.047847</td>\n",
       "      <td>-2.206021</td>\n",
       "      <td>1.290669</td>\n",
       "      <td>3.946332</td>\n",
       "      <td>-0.416121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>-0.721727</td>\n",
       "      <td>-0.715168</td>\n",
       "      <td>1.275204</td>\n",
       "      <td>0.842071</td>\n",
       "      <td>2.017429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>-0.617627</td>\n",
       "      <td>-1.584424</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>1.639837</td>\n",
       "      <td>0.536771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>1.174780</td>\n",
       "      <td>-0.877488</td>\n",
       "      <td>1.137938</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.846687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
       "0      0.225962 -0.987701  1.428785  0.196275  2.294005\n",
       "1     -0.270909 -1.172184  1.699451  1.889652  2.205972\n",
       "2     -0.170160  0.610330 -0.446073  2.320928  1.705356\n",
       "3      0.410402 -0.158501  0.223055  3.264879  1.840801\n",
       "4      3.462913 -5.130525  1.558750  5.146125 -4.048456\n",
       "...         ...       ...       ...       ...       ...\n",
       "10647 -6.752625  5.188299  0.805525 -0.362465 -2.794127\n",
       "10648  0.047847 -2.206021  1.290669  3.946332 -0.416121\n",
       "10649 -0.721727 -0.715168  1.275204  0.842071  2.017429\n",
       "10650 -0.617627 -1.584424  0.179293  1.639837  0.536771\n",
       "10651  1.174780 -0.877488  1.137938  0.925061  0.846687\n",
       "\n",
       "[10652 rows x 5 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>cbcl_q01_p</th>\n",
       "      <th>cbcl_q03_p</th>\n",
       "      <th>cbcl_q04_p</th>\n",
       "      <th>cbcl_q05_p</th>\n",
       "      <th>cbcl_q06_p</th>\n",
       "      <th>cbcl_q07_p</th>\n",
       "      <th>cbcl_q08_p</th>\n",
       "      <th>cbcl_q09_p</th>\n",
       "      <th>...</th>\n",
       "      <th>cbcl_q102_p</th>\n",
       "      <th>cbcl_q103_p</th>\n",
       "      <th>cbcl_q104_p</th>\n",
       "      <th>cbcl_q106_p</th>\n",
       "      <th>cbcl_q107_p</th>\n",
       "      <th>cbcl_q108_p</th>\n",
       "      <th>cbcl_q109_p</th>\n",
       "      <th>cbcl_q110_p</th>\n",
       "      <th>cbcl_q111_p</th>\n",
       "      <th>cbcl_q112_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>11158</td>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>11159</td>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>11160</td>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>11161</td>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>11162</td>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    src_subject_id  cbcl_q01_p  cbcl_q03_p  cbcl_q04_p  \\\n",
       "0               1  NDAR_INV003RTV85           0           0           0   \n",
       "1               2  NDAR_INV005V6D2C           0           0           0   \n",
       "2               3  NDAR_INV007W6H7B           0           1           0   \n",
       "3               4  NDAR_INV00BD7VDC           0           1           1   \n",
       "4               5  NDAR_INV00CY2MDM           1           2           1   \n",
       "...           ...               ...         ...         ...         ...   \n",
       "10647       11158  NDAR_INVZZLZCKAY           0           2           1   \n",
       "10648       11159  NDAR_INVZZNX6W2P           0           1           0   \n",
       "10649       11160  NDAR_INVZZPKBDAC           0           0           0   \n",
       "10650       11161  NDAR_INVZZZ2ALR6           0           1           0   \n",
       "10651       11162  NDAR_INVZZZNB0XC           1           0           1   \n",
       "\n",
       "       cbcl_q05_p  cbcl_q06_p  cbcl_q07_p  cbcl_q08_p  cbcl_q09_p  ...  \\\n",
       "0               0           0           0           0           0  ...   \n",
       "1               0           0           0           0           0  ...   \n",
       "2               0           0           1           0           1  ...   \n",
       "3               0           0           1           1           1  ...   \n",
       "4               0           0           1           2           0  ...   \n",
       "...           ...         ...         ...         ...         ...  ...   \n",
       "10647           0           0           0           1           1  ...   \n",
       "10648           1           0           0           0           0  ...   \n",
       "10649           0           0           0           0           0  ...   \n",
       "10650           0           0           0           0           0  ...   \n",
       "10651           0           0           0           0           1  ...   \n",
       "\n",
       "       cbcl_q102_p  cbcl_q103_p  cbcl_q104_p  cbcl_q106_p  cbcl_q107_p  \\\n",
       "0                0            0            0            0            0   \n",
       "1                0            0            0            0            0   \n",
       "2                0            0            1            0            0   \n",
       "3                0            0            1            0            0   \n",
       "4                0            1            0            0            0   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10647            1            0            1            0            0   \n",
       "10648            0            1            0            0            0   \n",
       "10649            0            0            0            0            0   \n",
       "10650            0            0            0            0            0   \n",
       "10651            0            0            0            0            0   \n",
       "\n",
       "       cbcl_q108_p  cbcl_q109_p  cbcl_q110_p  cbcl_q111_p  cbcl_q112_p  \n",
       "0                1            0            0            0            0  \n",
       "1                0            0            0            0            0  \n",
       "2                1            0            0            0            1  \n",
       "3                0            0            0            0            1  \n",
       "4                0            0            0            0            0  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "10647            0            1            0            1            1  \n",
       "10648            0            1            0            0            0  \n",
       "10649            0            0            0            0            0  \n",
       "10650            0            0            0            0            0  \n",
       "10651            0            0            0            0            0  \n",
       "\n",
       "[10652 rows x 116 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = mean_df[mean_df['src_subject_id'].isin(scores[\"src_subject_id\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>AGG_ad_ad_mean</th>\n",
       "      <th>AGG_ad_ca_mean</th>\n",
       "      <th>AGG_ad_cgc_mean</th>\n",
       "      <th>AGG_ad_dla_mean</th>\n",
       "      <th>AGG_ad_dt_mean</th>\n",
       "      <th>AGG_ad_fo_mean</th>\n",
       "      <th>AGG_ad_n_mean</th>\n",
       "      <th>AGG_ad_rspltp_mean</th>\n",
       "      <th>AGG_ad_sa_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>AGG_smh_smh_mean</th>\n",
       "      <th>AGG_smh_smm_mean</th>\n",
       "      <th>AGG_smh_vs_mean</th>\n",
       "      <th>AGG_smh_vta_mean</th>\n",
       "      <th>AGG_smm_smm_mean</th>\n",
       "      <th>AGG_smm_vs_mean</th>\n",
       "      <th>AGG_smm_vta_mean</th>\n",
       "      <th>AGG_vs_vs_mean</th>\n",
       "      <th>AGG_vs_vta_mean</th>\n",
       "      <th>AGG_vta_vta_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>0.471330</td>\n",
       "      <td>-0.076960</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>-0.116451</td>\n",
       "      <td>-0.036302</td>\n",
       "      <td>-0.057183</td>\n",
       "      <td>-0.048132</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314437</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>-0.064416</td>\n",
       "      <td>-0.042587</td>\n",
       "      <td>0.610255</td>\n",
       "      <td>0.098679</td>\n",
       "      <td>-0.042176</td>\n",
       "      <td>0.371960</td>\n",
       "      <td>-0.155134</td>\n",
       "      <td>0.233819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>0.116256</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.024781</td>\n",
       "      <td>-0.023421</td>\n",
       "      <td>-0.016284</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.056241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>-0.061459</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>0.506645</td>\n",
       "      <td>0.075145</td>\n",
       "      <td>-0.007564</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>-0.071943</td>\n",
       "      <td>0.173929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>-0.035168</td>\n",
       "      <td>-0.138693</td>\n",
       "      <td>0.044412</td>\n",
       "      <td>-0.041321</td>\n",
       "      <td>-0.185939</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288158</td>\n",
       "      <td>0.088796</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.683473</td>\n",
       "      <td>0.070566</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>0.400397</td>\n",
       "      <td>-0.076156</td>\n",
       "      <td>0.257184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>0.241918</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>-0.044039</td>\n",
       "      <td>-0.022455</td>\n",
       "      <td>-0.005482</td>\n",
       "      <td>-0.124627</td>\n",
       "      <td>0.095889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333995</td>\n",
       "      <td>0.056474</td>\n",
       "      <td>-0.029856</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.477314</td>\n",
       "      <td>-0.038823</td>\n",
       "      <td>0.011466</td>\n",
       "      <td>0.334313</td>\n",
       "      <td>-0.092095</td>\n",
       "      <td>0.167689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>-0.142808</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>-0.049414</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>-0.144481</td>\n",
       "      <td>-0.024881</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>-0.036592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426006</td>\n",
       "      <td>0.380348</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.818149</td>\n",
       "      <td>0.159728</td>\n",
       "      <td>0.074829</td>\n",
       "      <td>0.485745</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>0.230946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>0.315367</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>-0.030765</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.062264</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>-0.043990</td>\n",
       "      <td>0.140031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211188</td>\n",
       "      <td>0.024467</td>\n",
       "      <td>-0.045539</td>\n",
       "      <td>0.006467</td>\n",
       "      <td>0.596059</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>-0.054182</td>\n",
       "      <td>0.183470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>0.343187</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>0.142774</td>\n",
       "      <td>-0.006279</td>\n",
       "      <td>-0.046064</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>-0.064605</td>\n",
       "      <td>-0.014693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>0.120344</td>\n",
       "      <td>-0.082002</td>\n",
       "      <td>-0.003782</td>\n",
       "      <td>0.715068</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.484629</td>\n",
       "      <td>-0.126353</td>\n",
       "      <td>0.238454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>0.404070</td>\n",
       "      <td>-0.167735</td>\n",
       "      <td>0.300714</td>\n",
       "      <td>-0.044867</td>\n",
       "      <td>-0.093816</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>-0.157262</td>\n",
       "      <td>0.192685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223616</td>\n",
       "      <td>-0.033873</td>\n",
       "      <td>-0.098269</td>\n",
       "      <td>-0.007025</td>\n",
       "      <td>0.840622</td>\n",
       "      <td>0.069790</td>\n",
       "      <td>0.128084</td>\n",
       "      <td>0.462496</td>\n",
       "      <td>-0.126898</td>\n",
       "      <td>0.267290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>0.411105</td>\n",
       "      <td>-0.041473</td>\n",
       "      <td>0.209193</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>-0.063504</td>\n",
       "      <td>-0.046049</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>-0.162120</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302695</td>\n",
       "      <td>0.071764</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.082954</td>\n",
       "      <td>0.723260</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>0.029845</td>\n",
       "      <td>0.351791</td>\n",
       "      <td>-0.153170</td>\n",
       "      <td>0.301422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>0.270290</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>0.147113</td>\n",
       "      <td>-0.005676</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.083171</td>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252571</td>\n",
       "      <td>0.072439</td>\n",
       "      <td>-0.040876</td>\n",
       "      <td>-0.033312</td>\n",
       "      <td>0.473995</td>\n",
       "      <td>-0.019973</td>\n",
       "      <td>-0.011148</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>0.157045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10651 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id  AGG_ad_ad_mean  AGG_ad_ca_mean  AGG_ad_cgc_mean  \\\n",
       "0      NDAR_INV003RTV85        0.471330       -0.076960         0.256267   \n",
       "1      NDAR_INV005V6D2C        0.279435        0.063664         0.116256   \n",
       "2      NDAR_INV007W6H7B        0.294463       -0.071834         0.209772   \n",
       "3      NDAR_INV00BD7VDC        0.241918       -0.090651         0.163942   \n",
       "6      NDAR_INV00CY2MDM        0.395300       -0.142808         0.180940   \n",
       "...                 ...             ...             ...              ...   \n",
       "22117  NDAR_INVZZLZCKAY        0.315367       -0.010926         0.176642   \n",
       "22120  NDAR_INVZZNX6W2P        0.343187       -0.055901         0.142774   \n",
       "22123  NDAR_INVZZPKBDAC        0.404070       -0.167735         0.300714   \n",
       "22125  NDAR_INVZZZ2ALR6        0.411105       -0.041473         0.209193   \n",
       "22127  NDAR_INVZZZNB0XC        0.270290       -0.111270         0.147113   \n",
       "\n",
       "       AGG_ad_dla_mean  AGG_ad_dt_mean  AGG_ad_fo_mean  AGG_ad_n_mean  \\\n",
       "0             0.022202       -0.116451       -0.036302      -0.057183   \n",
       "1            -0.000840       -0.024781       -0.023421      -0.016284   \n",
       "2            -0.035168       -0.138693        0.044412      -0.041321   \n",
       "3             0.012523       -0.044039       -0.022455      -0.005482   \n",
       "6            -0.049414       -0.073424       -0.144481      -0.024881   \n",
       "...                ...             ...             ...            ...   \n",
       "22117        -0.030765       -0.019208       -0.062264       0.021828   \n",
       "22120        -0.006279       -0.046064        0.013171       0.017289   \n",
       "22123        -0.044867       -0.093816        0.019305       0.049103   \n",
       "22125        -0.102791       -0.063504       -0.046049      -0.009828   \n",
       "22127        -0.005676       -0.093982       -0.083171      -0.018443   \n",
       "\n",
       "       AGG_ad_rspltp_mean  AGG_ad_sa_mean  ...  AGG_smh_smh_mean  \\\n",
       "0               -0.048132        0.006416  ...          0.314437   \n",
       "1                0.022696        0.056241  ...          0.302632   \n",
       "2               -0.185939        0.071985  ...          0.288158   \n",
       "3               -0.124627        0.095889  ...          0.333995   \n",
       "6               -0.012894       -0.036592  ...          0.426006   \n",
       "...                   ...             ...  ...               ...   \n",
       "22117           -0.043990        0.140031  ...          0.211188   \n",
       "22120           -0.064605       -0.014693  ...          0.379075   \n",
       "22123           -0.157262        0.192685  ...          0.223616   \n",
       "22125           -0.162120        0.026045  ...          0.302695   \n",
       "22127           -0.074797        0.052438  ...          0.252571   \n",
       "\n",
       "       AGG_smh_smm_mean  AGG_smh_vs_mean  AGG_smh_vta_mean  AGG_smm_smm_mean  \\\n",
       "0              0.056185        -0.064416         -0.042587          0.610255   \n",
       "1              0.062337        -0.061459          0.023958          0.506645   \n",
       "2              0.088796        -0.060259          0.002308          0.683473   \n",
       "3              0.056474        -0.029856          0.020649          0.477314   \n",
       "6              0.380348         0.033060          0.032599          0.818149   \n",
       "...                 ...              ...               ...               ...   \n",
       "22117          0.024467        -0.045539          0.006467          0.596059   \n",
       "22120          0.120344        -0.082002         -0.003782          0.715068   \n",
       "22123         -0.033873        -0.098269         -0.007025          0.840622   \n",
       "22125          0.071764         0.000854         -0.082954          0.723260   \n",
       "22127          0.072439        -0.040876         -0.033312          0.473995   \n",
       "\n",
       "       AGG_smm_vs_mean  AGG_smm_vta_mean  AGG_vs_vs_mean  AGG_vs_vta_mean  \\\n",
       "0             0.098679         -0.042176        0.371960        -0.155134   \n",
       "1             0.075145         -0.007564        0.346746        -0.071943   \n",
       "2             0.070566          0.026875        0.400397        -0.076156   \n",
       "3            -0.038823          0.011466        0.334313        -0.092095   \n",
       "6             0.159728          0.074829        0.485745        -0.125210   \n",
       "...                ...               ...             ...              ...   \n",
       "22117        -0.012037          0.025325        0.333367        -0.054182   \n",
       "22120         0.015764          0.018554        0.484629        -0.126353   \n",
       "22123         0.069790          0.128084        0.462496        -0.126898   \n",
       "22125         0.030236          0.029845        0.351791        -0.153170   \n",
       "22127        -0.019973         -0.011148        0.381383        -0.105656   \n",
       "\n",
       "       AGG_vta_vta_mean  \n",
       "0              0.233819  \n",
       "1              0.173929  \n",
       "2              0.257184  \n",
       "3              0.167689  \n",
       "6              0.230946  \n",
       "...                 ...  \n",
       "22117          0.183470  \n",
       "22120          0.238454  \n",
       "22123          0.267290  \n",
       "22125          0.301422  \n",
       "22127          0.157045  \n",
       "\n",
       "[10651 rows x 92 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = mean_df.drop(columns=['src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGG_ad_ad_mean</th>\n",
       "      <th>AGG_ad_ca_mean</th>\n",
       "      <th>AGG_ad_cgc_mean</th>\n",
       "      <th>AGG_ad_dla_mean</th>\n",
       "      <th>AGG_ad_dt_mean</th>\n",
       "      <th>AGG_ad_fo_mean</th>\n",
       "      <th>AGG_ad_n_mean</th>\n",
       "      <th>AGG_ad_rspltp_mean</th>\n",
       "      <th>AGG_ad_sa_mean</th>\n",
       "      <th>AGG_ad_smh_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>AGG_smh_smh_mean</th>\n",
       "      <th>AGG_smh_smm_mean</th>\n",
       "      <th>AGG_smh_vs_mean</th>\n",
       "      <th>AGG_smh_vta_mean</th>\n",
       "      <th>AGG_smm_smm_mean</th>\n",
       "      <th>AGG_smm_vs_mean</th>\n",
       "      <th>AGG_smm_vta_mean</th>\n",
       "      <th>AGG_vs_vs_mean</th>\n",
       "      <th>AGG_vs_vta_mean</th>\n",
       "      <th>AGG_vta_vta_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.471330</td>\n",
       "      <td>-0.076960</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>-0.116451</td>\n",
       "      <td>-0.036302</td>\n",
       "      <td>-0.057183</td>\n",
       "      <td>-0.048132</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>0.199033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314437</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>-0.064416</td>\n",
       "      <td>-0.042587</td>\n",
       "      <td>0.610255</td>\n",
       "      <td>0.098679</td>\n",
       "      <td>-0.042176</td>\n",
       "      <td>0.371960</td>\n",
       "      <td>-0.155134</td>\n",
       "      <td>0.233819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>0.116256</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.024781</td>\n",
       "      <td>-0.023421</td>\n",
       "      <td>-0.016284</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.056241</td>\n",
       "      <td>0.110631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>-0.061459</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>0.506645</td>\n",
       "      <td>0.075145</td>\n",
       "      <td>-0.007564</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>-0.071943</td>\n",
       "      <td>0.173929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.294463</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>-0.035168</td>\n",
       "      <td>-0.138693</td>\n",
       "      <td>0.044412</td>\n",
       "      <td>-0.041321</td>\n",
       "      <td>-0.185939</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>0.096365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288158</td>\n",
       "      <td>0.088796</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.683473</td>\n",
       "      <td>0.070566</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>0.400397</td>\n",
       "      <td>-0.076156</td>\n",
       "      <td>0.257184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.241918</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>-0.044039</td>\n",
       "      <td>-0.022455</td>\n",
       "      <td>-0.005482</td>\n",
       "      <td>-0.124627</td>\n",
       "      <td>0.095889</td>\n",
       "      <td>0.151396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333995</td>\n",
       "      <td>0.056474</td>\n",
       "      <td>-0.029856</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.477314</td>\n",
       "      <td>-0.038823</td>\n",
       "      <td>0.011466</td>\n",
       "      <td>0.334313</td>\n",
       "      <td>-0.092095</td>\n",
       "      <td>0.167689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.395300</td>\n",
       "      <td>-0.142808</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>-0.049414</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>-0.144481</td>\n",
       "      <td>-0.024881</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>-0.036592</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426006</td>\n",
       "      <td>0.380348</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.818149</td>\n",
       "      <td>0.159728</td>\n",
       "      <td>0.074829</td>\n",
       "      <td>0.485745</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>0.230946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>0.315367</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>-0.030765</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.062264</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>-0.043990</td>\n",
       "      <td>0.140031</td>\n",
       "      <td>0.099674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211188</td>\n",
       "      <td>0.024467</td>\n",
       "      <td>-0.045539</td>\n",
       "      <td>0.006467</td>\n",
       "      <td>0.596059</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>-0.054182</td>\n",
       "      <td>0.183470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>0.343187</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>0.142774</td>\n",
       "      <td>-0.006279</td>\n",
       "      <td>-0.046064</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>-0.064605</td>\n",
       "      <td>-0.014693</td>\n",
       "      <td>0.149963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>0.120344</td>\n",
       "      <td>-0.082002</td>\n",
       "      <td>-0.003782</td>\n",
       "      <td>0.715068</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.484629</td>\n",
       "      <td>-0.126353</td>\n",
       "      <td>0.238454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>0.404070</td>\n",
       "      <td>-0.167735</td>\n",
       "      <td>0.300714</td>\n",
       "      <td>-0.044867</td>\n",
       "      <td>-0.093816</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>-0.157262</td>\n",
       "      <td>0.192685</td>\n",
       "      <td>0.052396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223616</td>\n",
       "      <td>-0.033873</td>\n",
       "      <td>-0.098269</td>\n",
       "      <td>-0.007025</td>\n",
       "      <td>0.840622</td>\n",
       "      <td>0.069790</td>\n",
       "      <td>0.128084</td>\n",
       "      <td>0.462496</td>\n",
       "      <td>-0.126898</td>\n",
       "      <td>0.267290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>0.411105</td>\n",
       "      <td>-0.041473</td>\n",
       "      <td>0.209193</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>-0.063504</td>\n",
       "      <td>-0.046049</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>-0.162120</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>0.078251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302695</td>\n",
       "      <td>0.071764</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.082954</td>\n",
       "      <td>0.723260</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>0.029845</td>\n",
       "      <td>0.351791</td>\n",
       "      <td>-0.153170</td>\n",
       "      <td>0.301422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>0.270290</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>0.147113</td>\n",
       "      <td>-0.005676</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.083171</td>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>0.136133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252571</td>\n",
       "      <td>0.072439</td>\n",
       "      <td>-0.040876</td>\n",
       "      <td>-0.033312</td>\n",
       "      <td>0.473995</td>\n",
       "      <td>-0.019973</td>\n",
       "      <td>-0.011148</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>0.157045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10651 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AGG_ad_ad_mean  AGG_ad_ca_mean  AGG_ad_cgc_mean  AGG_ad_dla_mean  \\\n",
       "0            0.471330       -0.076960         0.256267         0.022202   \n",
       "1            0.279435        0.063664         0.116256        -0.000840   \n",
       "2            0.294463       -0.071834         0.209772        -0.035168   \n",
       "3            0.241918       -0.090651         0.163942         0.012523   \n",
       "6            0.395300       -0.142808         0.180940        -0.049414   \n",
       "...               ...             ...              ...              ...   \n",
       "22117        0.315367       -0.010926         0.176642        -0.030765   \n",
       "22120        0.343187       -0.055901         0.142774        -0.006279   \n",
       "22123        0.404070       -0.167735         0.300714        -0.044867   \n",
       "22125        0.411105       -0.041473         0.209193        -0.102791   \n",
       "22127        0.270290       -0.111270         0.147113        -0.005676   \n",
       "\n",
       "       AGG_ad_dt_mean  AGG_ad_fo_mean  AGG_ad_n_mean  AGG_ad_rspltp_mean  \\\n",
       "0           -0.116451       -0.036302      -0.057183           -0.048132   \n",
       "1           -0.024781       -0.023421      -0.016284            0.022696   \n",
       "2           -0.138693        0.044412      -0.041321           -0.185939   \n",
       "3           -0.044039       -0.022455      -0.005482           -0.124627   \n",
       "6           -0.073424       -0.144481      -0.024881           -0.012894   \n",
       "...               ...             ...            ...                 ...   \n",
       "22117       -0.019208       -0.062264       0.021828           -0.043990   \n",
       "22120       -0.046064        0.013171       0.017289           -0.064605   \n",
       "22123       -0.093816        0.019305       0.049103           -0.157262   \n",
       "22125       -0.063504       -0.046049      -0.009828           -0.162120   \n",
       "22127       -0.093982       -0.083171      -0.018443           -0.074797   \n",
       "\n",
       "       AGG_ad_sa_mean  AGG_ad_smh_mean  ...  AGG_smh_smh_mean  \\\n",
       "0            0.006416         0.199033  ...          0.314437   \n",
       "1            0.056241         0.110631  ...          0.302632   \n",
       "2            0.071985         0.096365  ...          0.288158   \n",
       "3            0.095889         0.151396  ...          0.333995   \n",
       "6           -0.036592         0.321739  ...          0.426006   \n",
       "...               ...              ...  ...               ...   \n",
       "22117        0.140031         0.099674  ...          0.211188   \n",
       "22120       -0.014693         0.149963  ...          0.379075   \n",
       "22123        0.192685         0.052396  ...          0.223616   \n",
       "22125        0.026045         0.078251  ...          0.302695   \n",
       "22127        0.052438         0.136133  ...          0.252571   \n",
       "\n",
       "       AGG_smh_smm_mean  AGG_smh_vs_mean  AGG_smh_vta_mean  AGG_smm_smm_mean  \\\n",
       "0              0.056185        -0.064416         -0.042587          0.610255   \n",
       "1              0.062337        -0.061459          0.023958          0.506645   \n",
       "2              0.088796        -0.060259          0.002308          0.683473   \n",
       "3              0.056474        -0.029856          0.020649          0.477314   \n",
       "6              0.380348         0.033060          0.032599          0.818149   \n",
       "...                 ...              ...               ...               ...   \n",
       "22117          0.024467        -0.045539          0.006467          0.596059   \n",
       "22120          0.120344        -0.082002         -0.003782          0.715068   \n",
       "22123         -0.033873        -0.098269         -0.007025          0.840622   \n",
       "22125          0.071764         0.000854         -0.082954          0.723260   \n",
       "22127          0.072439        -0.040876         -0.033312          0.473995   \n",
       "\n",
       "       AGG_smm_vs_mean  AGG_smm_vta_mean  AGG_vs_vs_mean  AGG_vs_vta_mean  \\\n",
       "0             0.098679         -0.042176        0.371960        -0.155134   \n",
       "1             0.075145         -0.007564        0.346746        -0.071943   \n",
       "2             0.070566          0.026875        0.400397        -0.076156   \n",
       "3            -0.038823          0.011466        0.334313        -0.092095   \n",
       "6             0.159728          0.074829        0.485745        -0.125210   \n",
       "...                ...               ...             ...              ...   \n",
       "22117        -0.012037          0.025325        0.333367        -0.054182   \n",
       "22120         0.015764          0.018554        0.484629        -0.126353   \n",
       "22123         0.069790          0.128084        0.462496        -0.126898   \n",
       "22125         0.030236          0.029845        0.351791        -0.153170   \n",
       "22127        -0.019973         -0.011148        0.381383        -0.105656   \n",
       "\n",
       "       AGG_vta_vta_mean  \n",
       "0              0.233819  \n",
       "1              0.173929  \n",
       "2              0.257184  \n",
       "3              0.167689  \n",
       "6              0.230946  \n",
       "...                 ...  \n",
       "22117          0.183470  \n",
       "22120          0.238454  \n",
       "22123          0.267290  \n",
       "22125          0.301422  \n",
       "22127          0.157045  \n",
       "\n",
       "[10651 rows x 91 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor_1</th>\n",
       "      <th>Factor_2</th>\n",
       "      <th>Factor_3</th>\n",
       "      <th>Factor_4</th>\n",
       "      <th>Factor_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.225962</td>\n",
       "      <td>-0.987701</td>\n",
       "      <td>1.428785</td>\n",
       "      <td>0.196275</td>\n",
       "      <td>2.294005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.270909</td>\n",
       "      <td>-1.172184</td>\n",
       "      <td>1.699451</td>\n",
       "      <td>1.889652</td>\n",
       "      <td>2.205972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170160</td>\n",
       "      <td>0.610330</td>\n",
       "      <td>-0.446073</td>\n",
       "      <td>2.320928</td>\n",
       "      <td>1.705356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.410402</td>\n",
       "      <td>-0.158501</td>\n",
       "      <td>0.223055</td>\n",
       "      <td>3.264879</td>\n",
       "      <td>1.840801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.462913</td>\n",
       "      <td>-5.130525</td>\n",
       "      <td>1.558750</td>\n",
       "      <td>5.146125</td>\n",
       "      <td>-4.048456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>-6.752625</td>\n",
       "      <td>5.188299</td>\n",
       "      <td>0.805525</td>\n",
       "      <td>-0.362465</td>\n",
       "      <td>-2.794127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>0.047847</td>\n",
       "      <td>-2.206021</td>\n",
       "      <td>1.290669</td>\n",
       "      <td>3.946332</td>\n",
       "      <td>-0.416121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>-0.721727</td>\n",
       "      <td>-0.715168</td>\n",
       "      <td>1.275204</td>\n",
       "      <td>0.842071</td>\n",
       "      <td>2.017429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>-0.617627</td>\n",
       "      <td>-1.584424</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>1.639837</td>\n",
       "      <td>0.536771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>1.174780</td>\n",
       "      <td>-0.877488</td>\n",
       "      <td>1.137938</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.846687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
       "0      0.225962 -0.987701  1.428785  0.196275  2.294005\n",
       "1     -0.270909 -1.172184  1.699451  1.889652  2.205972\n",
       "2     -0.170160  0.610330 -0.446073  2.320928  1.705356\n",
       "3      0.410402 -0.158501  0.223055  3.264879  1.840801\n",
       "4      3.462913 -5.130525  1.558750  5.146125 -4.048456\n",
       "...         ...       ...       ...       ...       ...\n",
       "10647 -6.752625  5.188299  0.805525 -0.362465 -2.794127\n",
       "10648  0.047847 -2.206021  1.290669  3.946332 -0.416121\n",
       "10649 -0.721727 -0.715168  1.275204  0.842071  2.017429\n",
       "10650 -0.617627 -1.584424  0.179293  1.639837  0.536771\n",
       "10651  1.174780 -0.877488  1.137938  0.925061  0.846687\n",
       "\n",
       "[10652 rows x 5 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>Factor_1</th>\n",
       "      <th>Factor_2</th>\n",
       "      <th>Factor_3</th>\n",
       "      <th>Factor_4</th>\n",
       "      <th>Factor_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>0.225962</td>\n",
       "      <td>-0.987701</td>\n",
       "      <td>1.428785</td>\n",
       "      <td>0.196275</td>\n",
       "      <td>2.294005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>-0.270909</td>\n",
       "      <td>-1.172184</td>\n",
       "      <td>1.699451</td>\n",
       "      <td>1.889652</td>\n",
       "      <td>2.205972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>-0.170160</td>\n",
       "      <td>0.610330</td>\n",
       "      <td>-0.446073</td>\n",
       "      <td>2.320928</td>\n",
       "      <td>1.705356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>0.410402</td>\n",
       "      <td>-0.158501</td>\n",
       "      <td>0.223055</td>\n",
       "      <td>3.264879</td>\n",
       "      <td>1.840801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>3.462913</td>\n",
       "      <td>-5.130525</td>\n",
       "      <td>1.558750</td>\n",
       "      <td>5.146125</td>\n",
       "      <td>-4.048456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>-6.752625</td>\n",
       "      <td>5.188299</td>\n",
       "      <td>0.805525</td>\n",
       "      <td>-0.362465</td>\n",
       "      <td>-2.794127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>-2.206021</td>\n",
       "      <td>1.290669</td>\n",
       "      <td>3.946332</td>\n",
       "      <td>-0.416121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>-0.721727</td>\n",
       "      <td>-0.715168</td>\n",
       "      <td>1.275204</td>\n",
       "      <td>0.842071</td>\n",
       "      <td>2.017429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>-0.617627</td>\n",
       "      <td>-1.584424</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>1.639837</td>\n",
       "      <td>0.536771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>1.174780</td>\n",
       "      <td>-0.877488</td>\n",
       "      <td>1.137938</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.846687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10652 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id  Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
       "0      NDAR_INV003RTV85  0.225962 -0.987701  1.428785  0.196275  2.294005\n",
       "1      NDAR_INV005V6D2C -0.270909 -1.172184  1.699451  1.889652  2.205972\n",
       "2      NDAR_INV007W6H7B -0.170160  0.610330 -0.446073  2.320928  1.705356\n",
       "3      NDAR_INV00BD7VDC  0.410402 -0.158501  0.223055  3.264879  1.840801\n",
       "4      NDAR_INV00CY2MDM  3.462913 -5.130525  1.558750  5.146125 -4.048456\n",
       "...                 ...       ...       ...       ...       ...       ...\n",
       "10647  NDAR_INVZZLZCKAY -6.752625  5.188299  0.805525 -0.362465 -2.794127\n",
       "10648  NDAR_INVZZNX6W2P  0.047847 -2.206021  1.290669  3.946332 -0.416121\n",
       "10649  NDAR_INVZZPKBDAC -0.721727 -0.715168  1.275204  0.842071  2.017429\n",
       "10650  NDAR_INVZZZ2ALR6 -0.617627 -1.584424  0.179293  1.639837  0.536771\n",
       "10651  NDAR_INVZZZNB0XC  1.174780 -0.877488  1.137938  0.925061  0.846687\n",
       "\n",
       "[10652 rows x 6 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将qns的第一列和latent_df合并\n",
    "test = pd.concat([qns.iloc[:,1], latent_df], axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>AGG_ad_ad_mean</th>\n",
       "      <th>AGG_ad_ca_mean</th>\n",
       "      <th>AGG_ad_cgc_mean</th>\n",
       "      <th>AGG_ad_dla_mean</th>\n",
       "      <th>AGG_ad_dt_mean</th>\n",
       "      <th>AGG_ad_fo_mean</th>\n",
       "      <th>AGG_ad_n_mean</th>\n",
       "      <th>AGG_ad_rspltp_mean</th>\n",
       "      <th>AGG_ad_sa_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>AGG_smh_smh_mean</th>\n",
       "      <th>AGG_smh_smm_mean</th>\n",
       "      <th>AGG_smh_vs_mean</th>\n",
       "      <th>AGG_smh_vta_mean</th>\n",
       "      <th>AGG_smm_smm_mean</th>\n",
       "      <th>AGG_smm_vs_mean</th>\n",
       "      <th>AGG_smm_vta_mean</th>\n",
       "      <th>AGG_vs_vs_mean</th>\n",
       "      <th>AGG_vs_vta_mean</th>\n",
       "      <th>AGG_vta_vta_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDAR_INV003RTV85</td>\n",
       "      <td>0.471330</td>\n",
       "      <td>-0.076960</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>-0.116451</td>\n",
       "      <td>-0.036302</td>\n",
       "      <td>-0.057183</td>\n",
       "      <td>-0.048132</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314437</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>-0.064416</td>\n",
       "      <td>-0.042587</td>\n",
       "      <td>0.610255</td>\n",
       "      <td>0.098679</td>\n",
       "      <td>-0.042176</td>\n",
       "      <td>0.371960</td>\n",
       "      <td>-0.155134</td>\n",
       "      <td>0.233819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDAR_INV005V6D2C</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>0.116256</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.024781</td>\n",
       "      <td>-0.023421</td>\n",
       "      <td>-0.016284</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.056241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>-0.061459</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>0.506645</td>\n",
       "      <td>0.075145</td>\n",
       "      <td>-0.007564</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>-0.071943</td>\n",
       "      <td>0.173929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDAR_INV007W6H7B</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>-0.035168</td>\n",
       "      <td>-0.138693</td>\n",
       "      <td>0.044412</td>\n",
       "      <td>-0.041321</td>\n",
       "      <td>-0.185939</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288158</td>\n",
       "      <td>0.088796</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.683473</td>\n",
       "      <td>0.070566</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>0.400397</td>\n",
       "      <td>-0.076156</td>\n",
       "      <td>0.257184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDAR_INV00BD7VDC</td>\n",
       "      <td>0.241918</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>-0.044039</td>\n",
       "      <td>-0.022455</td>\n",
       "      <td>-0.005482</td>\n",
       "      <td>-0.124627</td>\n",
       "      <td>0.095889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333995</td>\n",
       "      <td>0.056474</td>\n",
       "      <td>-0.029856</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.477314</td>\n",
       "      <td>-0.038823</td>\n",
       "      <td>0.011466</td>\n",
       "      <td>0.334313</td>\n",
       "      <td>-0.092095</td>\n",
       "      <td>0.167689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NDAR_INV00CY2MDM</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>-0.142808</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>-0.049414</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>-0.144481</td>\n",
       "      <td>-0.024881</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>-0.036592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426006</td>\n",
       "      <td>0.380348</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.818149</td>\n",
       "      <td>0.159728</td>\n",
       "      <td>0.074829</td>\n",
       "      <td>0.485745</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>0.230946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>NDAR_INVZZLZCKAY</td>\n",
       "      <td>0.315367</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>-0.030765</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.062264</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>-0.043990</td>\n",
       "      <td>0.140031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211188</td>\n",
       "      <td>0.024467</td>\n",
       "      <td>-0.045539</td>\n",
       "      <td>0.006467</td>\n",
       "      <td>0.596059</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>-0.054182</td>\n",
       "      <td>0.183470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>NDAR_INVZZNX6W2P</td>\n",
       "      <td>0.343187</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>0.142774</td>\n",
       "      <td>-0.006279</td>\n",
       "      <td>-0.046064</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>-0.064605</td>\n",
       "      <td>-0.014693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>0.120344</td>\n",
       "      <td>-0.082002</td>\n",
       "      <td>-0.003782</td>\n",
       "      <td>0.715068</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.484629</td>\n",
       "      <td>-0.126353</td>\n",
       "      <td>0.238454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>NDAR_INVZZPKBDAC</td>\n",
       "      <td>0.404070</td>\n",
       "      <td>-0.167735</td>\n",
       "      <td>0.300714</td>\n",
       "      <td>-0.044867</td>\n",
       "      <td>-0.093816</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>-0.157262</td>\n",
       "      <td>0.192685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223616</td>\n",
       "      <td>-0.033873</td>\n",
       "      <td>-0.098269</td>\n",
       "      <td>-0.007025</td>\n",
       "      <td>0.840622</td>\n",
       "      <td>0.069790</td>\n",
       "      <td>0.128084</td>\n",
       "      <td>0.462496</td>\n",
       "      <td>-0.126898</td>\n",
       "      <td>0.267290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>NDAR_INVZZZ2ALR6</td>\n",
       "      <td>0.411105</td>\n",
       "      <td>-0.041473</td>\n",
       "      <td>0.209193</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>-0.063504</td>\n",
       "      <td>-0.046049</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>-0.162120</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302695</td>\n",
       "      <td>0.071764</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.082954</td>\n",
       "      <td>0.723260</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>0.029845</td>\n",
       "      <td>0.351791</td>\n",
       "      <td>-0.153170</td>\n",
       "      <td>0.301422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>NDAR_INVZZZNB0XC</td>\n",
       "      <td>0.270290</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>0.147113</td>\n",
       "      <td>-0.005676</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.083171</td>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252571</td>\n",
       "      <td>0.072439</td>\n",
       "      <td>-0.040876</td>\n",
       "      <td>-0.033312</td>\n",
       "      <td>0.473995</td>\n",
       "      <td>-0.019973</td>\n",
       "      <td>-0.011148</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>0.157045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10651 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_subject_id  AGG_ad_ad_mean  AGG_ad_ca_mean  AGG_ad_cgc_mean  \\\n",
       "0      NDAR_INV003RTV85        0.471330       -0.076960         0.256267   \n",
       "1      NDAR_INV005V6D2C        0.279435        0.063664         0.116256   \n",
       "2      NDAR_INV007W6H7B        0.294463       -0.071834         0.209772   \n",
       "3      NDAR_INV00BD7VDC        0.241918       -0.090651         0.163942   \n",
       "6      NDAR_INV00CY2MDM        0.395300       -0.142808         0.180940   \n",
       "...                 ...             ...             ...              ...   \n",
       "22117  NDAR_INVZZLZCKAY        0.315367       -0.010926         0.176642   \n",
       "22120  NDAR_INVZZNX6W2P        0.343187       -0.055901         0.142774   \n",
       "22123  NDAR_INVZZPKBDAC        0.404070       -0.167735         0.300714   \n",
       "22125  NDAR_INVZZZ2ALR6        0.411105       -0.041473         0.209193   \n",
       "22127  NDAR_INVZZZNB0XC        0.270290       -0.111270         0.147113   \n",
       "\n",
       "       AGG_ad_dla_mean  AGG_ad_dt_mean  AGG_ad_fo_mean  AGG_ad_n_mean  \\\n",
       "0             0.022202       -0.116451       -0.036302      -0.057183   \n",
       "1            -0.000840       -0.024781       -0.023421      -0.016284   \n",
       "2            -0.035168       -0.138693        0.044412      -0.041321   \n",
       "3             0.012523       -0.044039       -0.022455      -0.005482   \n",
       "6            -0.049414       -0.073424       -0.144481      -0.024881   \n",
       "...                ...             ...             ...            ...   \n",
       "22117        -0.030765       -0.019208       -0.062264       0.021828   \n",
       "22120        -0.006279       -0.046064        0.013171       0.017289   \n",
       "22123        -0.044867       -0.093816        0.019305       0.049103   \n",
       "22125        -0.102791       -0.063504       -0.046049      -0.009828   \n",
       "22127        -0.005676       -0.093982       -0.083171      -0.018443   \n",
       "\n",
       "       AGG_ad_rspltp_mean  AGG_ad_sa_mean  ...  AGG_smh_smh_mean  \\\n",
       "0               -0.048132        0.006416  ...          0.314437   \n",
       "1                0.022696        0.056241  ...          0.302632   \n",
       "2               -0.185939        0.071985  ...          0.288158   \n",
       "3               -0.124627        0.095889  ...          0.333995   \n",
       "6               -0.012894       -0.036592  ...          0.426006   \n",
       "...                   ...             ...  ...               ...   \n",
       "22117           -0.043990        0.140031  ...          0.211188   \n",
       "22120           -0.064605       -0.014693  ...          0.379075   \n",
       "22123           -0.157262        0.192685  ...          0.223616   \n",
       "22125           -0.162120        0.026045  ...          0.302695   \n",
       "22127           -0.074797        0.052438  ...          0.252571   \n",
       "\n",
       "       AGG_smh_smm_mean  AGG_smh_vs_mean  AGG_smh_vta_mean  AGG_smm_smm_mean  \\\n",
       "0              0.056185        -0.064416         -0.042587          0.610255   \n",
       "1              0.062337        -0.061459          0.023958          0.506645   \n",
       "2              0.088796        -0.060259          0.002308          0.683473   \n",
       "3              0.056474        -0.029856          0.020649          0.477314   \n",
       "6              0.380348         0.033060          0.032599          0.818149   \n",
       "...                 ...              ...               ...               ...   \n",
       "22117          0.024467        -0.045539          0.006467          0.596059   \n",
       "22120          0.120344        -0.082002         -0.003782          0.715068   \n",
       "22123         -0.033873        -0.098269         -0.007025          0.840622   \n",
       "22125          0.071764         0.000854         -0.082954          0.723260   \n",
       "22127          0.072439        -0.040876         -0.033312          0.473995   \n",
       "\n",
       "       AGG_smm_vs_mean  AGG_smm_vta_mean  AGG_vs_vs_mean  AGG_vs_vta_mean  \\\n",
       "0             0.098679         -0.042176        0.371960        -0.155134   \n",
       "1             0.075145         -0.007564        0.346746        -0.071943   \n",
       "2             0.070566          0.026875        0.400397        -0.076156   \n",
       "3            -0.038823          0.011466        0.334313        -0.092095   \n",
       "6             0.159728          0.074829        0.485745        -0.125210   \n",
       "...                ...               ...             ...              ...   \n",
       "22117        -0.012037          0.025325        0.333367        -0.054182   \n",
       "22120         0.015764          0.018554        0.484629        -0.126353   \n",
       "22123         0.069790          0.128084        0.462496        -0.126898   \n",
       "22125         0.030236          0.029845        0.351791        -0.153170   \n",
       "22127        -0.019973         -0.011148        0.381383        -0.105656   \n",
       "\n",
       "       AGG_vta_vta_mean  \n",
       "0              0.233819  \n",
       "1              0.173929  \n",
       "2              0.257184  \n",
       "3              0.167689  \n",
       "6              0.230946  \n",
       "...                 ...  \n",
       "22117          0.183470  \n",
       "22120          0.238454  \n",
       "22123          0.267290  \n",
       "22125          0.301422  \n",
       "22127          0.157045  \n",
       "\n",
       "[10651 rows x 92 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df = mean_df[mean_df['src_subject_id'].isin(test[\"src_subject_id\"])]\n",
    "mean_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "mena_df = mean_df.drop(columns=['src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['src_subject_id'].isin(mean_df[\"src_subject_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns=['src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor_1</th>\n",
       "      <th>Factor_2</th>\n",
       "      <th>Factor_3</th>\n",
       "      <th>Factor_4</th>\n",
       "      <th>Factor_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.225962</td>\n",
       "      <td>-0.987701</td>\n",
       "      <td>1.428785</td>\n",
       "      <td>0.196275</td>\n",
       "      <td>2.294005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.270909</td>\n",
       "      <td>-1.172184</td>\n",
       "      <td>1.699451</td>\n",
       "      <td>1.889652</td>\n",
       "      <td>2.205972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170160</td>\n",
       "      <td>0.610330</td>\n",
       "      <td>-0.446073</td>\n",
       "      <td>2.320928</td>\n",
       "      <td>1.705356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.410402</td>\n",
       "      <td>-0.158501</td>\n",
       "      <td>0.223055</td>\n",
       "      <td>3.264879</td>\n",
       "      <td>1.840801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.462913</td>\n",
       "      <td>-5.130525</td>\n",
       "      <td>1.558750</td>\n",
       "      <td>5.146125</td>\n",
       "      <td>-4.048456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10647</th>\n",
       "      <td>-6.752625</td>\n",
       "      <td>5.188299</td>\n",
       "      <td>0.805525</td>\n",
       "      <td>-0.362465</td>\n",
       "      <td>-2.794127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>0.047847</td>\n",
       "      <td>-2.206021</td>\n",
       "      <td>1.290669</td>\n",
       "      <td>3.946332</td>\n",
       "      <td>-0.416121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10649</th>\n",
       "      <td>-0.721727</td>\n",
       "      <td>-0.715168</td>\n",
       "      <td>1.275204</td>\n",
       "      <td>0.842071</td>\n",
       "      <td>2.017429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10650</th>\n",
       "      <td>-0.617627</td>\n",
       "      <td>-1.584424</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>1.639837</td>\n",
       "      <td>0.536771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>1.174780</td>\n",
       "      <td>-0.877488</td>\n",
       "      <td>1.137938</td>\n",
       "      <td>0.925061</td>\n",
       "      <td>0.846687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10651 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Factor_1  Factor_2  Factor_3  Factor_4  Factor_5\n",
       "0      0.225962 -0.987701  1.428785  0.196275  2.294005\n",
       "1     -0.270909 -1.172184  1.699451  1.889652  2.205972\n",
       "2     -0.170160  0.610330 -0.446073  2.320928  1.705356\n",
       "3      0.410402 -0.158501  0.223055  3.264879  1.840801\n",
       "4      3.462913 -5.130525  1.558750  5.146125 -4.048456\n",
       "...         ...       ...       ...       ...       ...\n",
       "10647 -6.752625  5.188299  0.805525 -0.362465 -2.794127\n",
       "10648  0.047847 -2.206021  1.290669  3.946332 -0.416121\n",
       "10649 -0.721727 -0.715168  1.275204  0.842071  2.017429\n",
       "10650 -0.617627 -1.584424  0.179293  1.639837  0.536771\n",
       "10651  1.174780 -0.877488  1.137938  0.925061  0.846687\n",
       "\n",
       "[10651 rows x 5 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = mean_df.drop(columns=['src_subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGG_ad_ad_mean</th>\n",
       "      <th>AGG_ad_ca_mean</th>\n",
       "      <th>AGG_ad_cgc_mean</th>\n",
       "      <th>AGG_ad_dla_mean</th>\n",
       "      <th>AGG_ad_dt_mean</th>\n",
       "      <th>AGG_ad_fo_mean</th>\n",
       "      <th>AGG_ad_n_mean</th>\n",
       "      <th>AGG_ad_rspltp_mean</th>\n",
       "      <th>AGG_ad_sa_mean</th>\n",
       "      <th>AGG_ad_smh_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>AGG_smh_smh_mean</th>\n",
       "      <th>AGG_smh_smm_mean</th>\n",
       "      <th>AGG_smh_vs_mean</th>\n",
       "      <th>AGG_smh_vta_mean</th>\n",
       "      <th>AGG_smm_smm_mean</th>\n",
       "      <th>AGG_smm_vs_mean</th>\n",
       "      <th>AGG_smm_vta_mean</th>\n",
       "      <th>AGG_vs_vs_mean</th>\n",
       "      <th>AGG_vs_vta_mean</th>\n",
       "      <th>AGG_vta_vta_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.471330</td>\n",
       "      <td>-0.076960</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>-0.116451</td>\n",
       "      <td>-0.036302</td>\n",
       "      <td>-0.057183</td>\n",
       "      <td>-0.048132</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>0.199033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314437</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>-0.064416</td>\n",
       "      <td>-0.042587</td>\n",
       "      <td>0.610255</td>\n",
       "      <td>0.098679</td>\n",
       "      <td>-0.042176</td>\n",
       "      <td>0.371960</td>\n",
       "      <td>-0.155134</td>\n",
       "      <td>0.233819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.063664</td>\n",
       "      <td>0.116256</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.024781</td>\n",
       "      <td>-0.023421</td>\n",
       "      <td>-0.016284</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.056241</td>\n",
       "      <td>0.110631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>-0.061459</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>0.506645</td>\n",
       "      <td>0.075145</td>\n",
       "      <td>-0.007564</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>-0.071943</td>\n",
       "      <td>0.173929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.294463</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>-0.035168</td>\n",
       "      <td>-0.138693</td>\n",
       "      <td>0.044412</td>\n",
       "      <td>-0.041321</td>\n",
       "      <td>-0.185939</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>0.096365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288158</td>\n",
       "      <td>0.088796</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.683473</td>\n",
       "      <td>0.070566</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>0.400397</td>\n",
       "      <td>-0.076156</td>\n",
       "      <td>0.257184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.241918</td>\n",
       "      <td>-0.090651</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>-0.044039</td>\n",
       "      <td>-0.022455</td>\n",
       "      <td>-0.005482</td>\n",
       "      <td>-0.124627</td>\n",
       "      <td>0.095889</td>\n",
       "      <td>0.151396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333995</td>\n",
       "      <td>0.056474</td>\n",
       "      <td>-0.029856</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.477314</td>\n",
       "      <td>-0.038823</td>\n",
       "      <td>0.011466</td>\n",
       "      <td>0.334313</td>\n",
       "      <td>-0.092095</td>\n",
       "      <td>0.167689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.395300</td>\n",
       "      <td>-0.142808</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>-0.049414</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>-0.144481</td>\n",
       "      <td>-0.024881</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>-0.036592</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426006</td>\n",
       "      <td>0.380348</td>\n",
       "      <td>0.033060</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.818149</td>\n",
       "      <td>0.159728</td>\n",
       "      <td>0.074829</td>\n",
       "      <td>0.485745</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>0.230946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22117</th>\n",
       "      <td>0.315367</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>-0.030765</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>-0.062264</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>-0.043990</td>\n",
       "      <td>0.140031</td>\n",
       "      <td>0.099674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211188</td>\n",
       "      <td>0.024467</td>\n",
       "      <td>-0.045539</td>\n",
       "      <td>0.006467</td>\n",
       "      <td>0.596059</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>0.025325</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>-0.054182</td>\n",
       "      <td>0.183470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>0.343187</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>0.142774</td>\n",
       "      <td>-0.006279</td>\n",
       "      <td>-0.046064</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>-0.064605</td>\n",
       "      <td>-0.014693</td>\n",
       "      <td>0.149963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>0.120344</td>\n",
       "      <td>-0.082002</td>\n",
       "      <td>-0.003782</td>\n",
       "      <td>0.715068</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.484629</td>\n",
       "      <td>-0.126353</td>\n",
       "      <td>0.238454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22123</th>\n",
       "      <td>0.404070</td>\n",
       "      <td>-0.167735</td>\n",
       "      <td>0.300714</td>\n",
       "      <td>-0.044867</td>\n",
       "      <td>-0.093816</td>\n",
       "      <td>0.019305</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>-0.157262</td>\n",
       "      <td>0.192685</td>\n",
       "      <td>0.052396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223616</td>\n",
       "      <td>-0.033873</td>\n",
       "      <td>-0.098269</td>\n",
       "      <td>-0.007025</td>\n",
       "      <td>0.840622</td>\n",
       "      <td>0.069790</td>\n",
       "      <td>0.128084</td>\n",
       "      <td>0.462496</td>\n",
       "      <td>-0.126898</td>\n",
       "      <td>0.267290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22125</th>\n",
       "      <td>0.411105</td>\n",
       "      <td>-0.041473</td>\n",
       "      <td>0.209193</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>-0.063504</td>\n",
       "      <td>-0.046049</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>-0.162120</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>0.078251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302695</td>\n",
       "      <td>0.071764</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.082954</td>\n",
       "      <td>0.723260</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>0.029845</td>\n",
       "      <td>0.351791</td>\n",
       "      <td>-0.153170</td>\n",
       "      <td>0.301422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22127</th>\n",
       "      <td>0.270290</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>0.147113</td>\n",
       "      <td>-0.005676</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.083171</td>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>0.136133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252571</td>\n",
       "      <td>0.072439</td>\n",
       "      <td>-0.040876</td>\n",
       "      <td>-0.033312</td>\n",
       "      <td>0.473995</td>\n",
       "      <td>-0.019973</td>\n",
       "      <td>-0.011148</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>-0.105656</td>\n",
       "      <td>0.157045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10651 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AGG_ad_ad_mean  AGG_ad_ca_mean  AGG_ad_cgc_mean  AGG_ad_dla_mean  \\\n",
       "0            0.471330       -0.076960         0.256267         0.022202   \n",
       "1            0.279435        0.063664         0.116256        -0.000840   \n",
       "2            0.294463       -0.071834         0.209772        -0.035168   \n",
       "3            0.241918       -0.090651         0.163942         0.012523   \n",
       "6            0.395300       -0.142808         0.180940        -0.049414   \n",
       "...               ...             ...              ...              ...   \n",
       "22117        0.315367       -0.010926         0.176642        -0.030765   \n",
       "22120        0.343187       -0.055901         0.142774        -0.006279   \n",
       "22123        0.404070       -0.167735         0.300714        -0.044867   \n",
       "22125        0.411105       -0.041473         0.209193        -0.102791   \n",
       "22127        0.270290       -0.111270         0.147113        -0.005676   \n",
       "\n",
       "       AGG_ad_dt_mean  AGG_ad_fo_mean  AGG_ad_n_mean  AGG_ad_rspltp_mean  \\\n",
       "0           -0.116451       -0.036302      -0.057183           -0.048132   \n",
       "1           -0.024781       -0.023421      -0.016284            0.022696   \n",
       "2           -0.138693        0.044412      -0.041321           -0.185939   \n",
       "3           -0.044039       -0.022455      -0.005482           -0.124627   \n",
       "6           -0.073424       -0.144481      -0.024881           -0.012894   \n",
       "...               ...             ...            ...                 ...   \n",
       "22117       -0.019208       -0.062264       0.021828           -0.043990   \n",
       "22120       -0.046064        0.013171       0.017289           -0.064605   \n",
       "22123       -0.093816        0.019305       0.049103           -0.157262   \n",
       "22125       -0.063504       -0.046049      -0.009828           -0.162120   \n",
       "22127       -0.093982       -0.083171      -0.018443           -0.074797   \n",
       "\n",
       "       AGG_ad_sa_mean  AGG_ad_smh_mean  ...  AGG_smh_smh_mean  \\\n",
       "0            0.006416         0.199033  ...          0.314437   \n",
       "1            0.056241         0.110631  ...          0.302632   \n",
       "2            0.071985         0.096365  ...          0.288158   \n",
       "3            0.095889         0.151396  ...          0.333995   \n",
       "6           -0.036592         0.321739  ...          0.426006   \n",
       "...               ...              ...  ...               ...   \n",
       "22117        0.140031         0.099674  ...          0.211188   \n",
       "22120       -0.014693         0.149963  ...          0.379075   \n",
       "22123        0.192685         0.052396  ...          0.223616   \n",
       "22125        0.026045         0.078251  ...          0.302695   \n",
       "22127        0.052438         0.136133  ...          0.252571   \n",
       "\n",
       "       AGG_smh_smm_mean  AGG_smh_vs_mean  AGG_smh_vta_mean  AGG_smm_smm_mean  \\\n",
       "0              0.056185        -0.064416         -0.042587          0.610255   \n",
       "1              0.062337        -0.061459          0.023958          0.506645   \n",
       "2              0.088796        -0.060259          0.002308          0.683473   \n",
       "3              0.056474        -0.029856          0.020649          0.477314   \n",
       "6              0.380348         0.033060          0.032599          0.818149   \n",
       "...                 ...              ...               ...               ...   \n",
       "22117          0.024467        -0.045539          0.006467          0.596059   \n",
       "22120          0.120344        -0.082002         -0.003782          0.715068   \n",
       "22123         -0.033873        -0.098269         -0.007025          0.840622   \n",
       "22125          0.071764         0.000854         -0.082954          0.723260   \n",
       "22127          0.072439        -0.040876         -0.033312          0.473995   \n",
       "\n",
       "       AGG_smm_vs_mean  AGG_smm_vta_mean  AGG_vs_vs_mean  AGG_vs_vta_mean  \\\n",
       "0             0.098679         -0.042176        0.371960        -0.155134   \n",
       "1             0.075145         -0.007564        0.346746        -0.071943   \n",
       "2             0.070566          0.026875        0.400397        -0.076156   \n",
       "3            -0.038823          0.011466        0.334313        -0.092095   \n",
       "6             0.159728          0.074829        0.485745        -0.125210   \n",
       "...                ...               ...             ...              ...   \n",
       "22117        -0.012037          0.025325        0.333367        -0.054182   \n",
       "22120         0.015764          0.018554        0.484629        -0.126353   \n",
       "22123         0.069790          0.128084        0.462496        -0.126898   \n",
       "22125         0.030236          0.029845        0.351791        -0.153170   \n",
       "22127        -0.019973         -0.011148        0.381383        -0.105656   \n",
       "\n",
       "       AGG_vta_vta_mean  \n",
       "0              0.233819  \n",
       "1              0.173929  \n",
       "2              0.257184  \n",
       "3              0.167689  \n",
       "6              0.230946  \n",
       "...                 ...  \n",
       "22117          0.183470  \n",
       "22120          0.238454  \n",
       "22123          0.267290  \n",
       "22125          0.301422  \n",
       "22127          0.157045  \n",
       "\n",
       "[10651 rows x 91 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\mne\\Lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均方误差 (MSE): 4.392013142619934\n",
      "R^2 评分: 0.0075454737798142935\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "#import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mean_df, test.iloc[:,[0]], test_size=0.2, random_state=42)\n",
    "\n",
    "# 使用支持向量回归\n",
    "regressor = svm.SVR()  # 确保使用 SVR 而不是 SVC\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = regressor.predict(X_test)\n",
    "# accuracy \n",
    "# 评估模型性能\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"均方误差 (MSE):\", mse)\n",
    "print(\"R^2 评分:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
